package:
  name: k2
  version: "{{ environ.get('K2_BUILD_VERSION') }}"

source:
  path: "{{ environ.get('K2_ROOT_DIR') }}"

build:
  number: 0
  string: cuda{{ environ.get('K2_CUDA_VERSION') }}_py{{ environ.get('K2_PYTHON_VERSION') }}_torch{{ environ.get('K2_TORCH_VERSION') }}
  script_env:
    - K2_IS_GITHUB_ACTIONS
    - K2_CUDA_VERSION
    - K2_TORCH_VERSION
    - K2_PYTHON_VERSION
    - K2_BUILD_TYPE
    - K2_BUILD_VERSION
    - K2_IS_FOR_CONDA

requirements:
  host:
    - cmake=3.18
    - python
    - pytorch={{ environ.get('K2_TORCH_VERSION') }}
    - pytorch-cuda={{ environ.get('K2_CUDA_VERSION') }}
    - gcc_linux-64=7
  run:
    - python
    - pytorch={{ environ.get('K2_TORCH_VERSION') }}
    - pytorch-cuda={{ environ.get('K2_CUDA_VERSION') }}

about:
  home: https://github.com/k2-fsa/k2
  doc_url: https://k2.readthedocs.io/en/latest/
  license: Apache V2
  license_file: LICENSE
  summary: FSA/FST algorithms, differentiable, with PyTorch compatibility
  description: |
    The vision of k2 is to be able to seamlessly integrate Finite State Automaton
    (FSA) and Finite State Transducer (FST) algorithms into autograd-based machine
    learning toolkits like PyTorch and TensorFlow.  For speech recognition
    applications, this should make it easy to interpolate and combine various
    training objectives such as cross-entropy, CTC and MMI and to jointly optimize a
    speech recognition system with multiple decoding passes including lattice
    rescoring and confidence estimation.  We hope k2 will have many other
    applications as well.
