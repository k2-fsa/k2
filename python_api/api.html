<!-- see https://stackoverflow.com/questions/2454577/sphinx-restructuredtext-show-hide-code-snippets -->
<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>k2 &mdash; k2 1.24.4 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="version" href="version.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> k2
          </a>
              <div class="version">
                1.24.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../core_concepts/index.html">Core concepts in k2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_tutorials/index.html">Python tutorials</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Python API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="version.html">version</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">k2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#add-epsilon-self-loops">add_epsilon_self_loops</a></li>
<li class="toctree-l3"><a class="reference internal" href="#arc-sort">arc_sort</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cat">cat</a></li>
<li class="toctree-l3"><a class="reference internal" href="#closure">closure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compose">compose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compose-arc-maps">compose_arc_maps</a></li>
<li class="toctree-l3"><a class="reference internal" href="#connect">connect</a></li>
<li class="toctree-l3"><a class="reference internal" href="#convert-dense-to-fsa-vec">convert_dense_to_fsa_vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-fsa-vec">create_fsa_vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-sparse">create_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ctc-graph">ctc_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ctc-loss">ctc_loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ctc-topo">ctc_topo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#determinize">determinize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#do-rnnt-pruning">do_rnnt_pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#expand-ragged-attributes">expand_ragged_attributes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-aux-labels">get_aux_labels</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-best-matching-stats">get_best_matching_stats</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-lattice">get_lattice</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-rnnt-logprobs">get_rnnt_logprobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-rnnt-logprobs-joint">get_rnnt_logprobs_joint</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-rnnt-logprobs-pruned">get_rnnt_logprobs_pruned</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-rnnt-logprobs-smoothed">get_rnnt_logprobs_smoothed</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-rnnt-prune-ranges">get_rnnt_prune_ranges</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-rnnt-prune-ranges-deprecated">get_rnnt_prune_ranges_deprecated</a></li>
<li class="toctree-l3"><a class="reference internal" href="#index-add">index_add</a></li>
<li class="toctree-l3"><a class="reference internal" href="#index-fsa">index_fsa</a></li>
<li class="toctree-l3"><a class="reference internal" href="#index-select">index_select</a></li>
<li class="toctree-l3"><a class="reference internal" href="#intersect">intersect</a></li>
<li class="toctree-l3"><a class="reference internal" href="#intersect-dense">intersect_dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="#intersect-dense-pruned">intersect_dense_pruned</a></li>
<li class="toctree-l3"><a class="reference internal" href="#intersect-device">intersect_device</a></li>
<li class="toctree-l3"><a class="reference internal" href="#invert">invert</a></li>
<li class="toctree-l3"><a class="reference internal" href="#is-rand-equivalent">is_rand_equivalent</a></li>
<li class="toctree-l3"><a class="reference internal" href="#joint-mutual-information-recursion">joint_mutual_information_recursion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#levenshtein-alignment">levenshtein_alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#levenshtein-graph">levenshtein_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-fsa">linear_fsa</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-fsa-with-self-loops">linear_fsa_with_self_loops</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-fst">linear_fst</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-fst-with-self-loops">linear_fst_with_self_loops</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mutual-information-recursion">mutual_information_recursion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mwer-loss">mwer_loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#one-best-decoding">one_best_decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#properties-to-str">properties_to_str</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prune-on-arc-post">prune_on_arc_post</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pruned-ranges-to-lattice">pruned_ranges_to_lattice</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-fsa">random_fsa</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-fsa-vec">random_fsa_vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-paths">random_paths</a></li>
<li class="toctree-l3"><a class="reference internal" href="#remove-epsilon">remove_epsilon</a></li>
<li class="toctree-l3"><a class="reference internal" href="#remove-epsilon-and-add-self-loops">remove_epsilon_and_add_self_loops</a></li>
<li class="toctree-l3"><a class="reference internal" href="#remove-epsilon-self-loops">remove_epsilon_self_loops</a></li>
<li class="toctree-l3"><a class="reference internal" href="#replace-fsa">replace_fsa</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reverse">reverse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnnt-loss">rnnt_loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnnt-loss-pruned">rnnt_loss_pruned</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnnt-loss-simple">rnnt_loss_simple</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnnt-loss-smoothed">rnnt_loss_smoothed</a></li>
<li class="toctree-l3"><a class="reference internal" href="#shortest-path">shortest_path</a></li>
<li class="toctree-l3"><a class="reference internal" href="#simple-ragged-index-select">simple_ragged_index_select</a></li>
<li class="toctree-l3"><a class="reference internal" href="#swoosh-l">swoosh_l</a></li>
<li class="toctree-l3"><a class="reference internal" href="#swoosh-l-forward">swoosh_l_forward</a></li>
<li class="toctree-l3"><a class="reference internal" href="#swoosh-l-forward-and-deriv">swoosh_l_forward_and_deriv</a></li>
<li class="toctree-l3"><a class="reference internal" href="#swoosh-r">swoosh_r</a></li>
<li class="toctree-l3"><a class="reference internal" href="#swoosh-r-forward">swoosh_r_forward</a></li>
<li class="toctree-l3"><a class="reference internal" href="#swoosh-r-forward-and-deriv">swoosh_r_forward_and_deriv</a></li>
<li class="toctree-l3"><a class="reference internal" href="#to-dot">to_dot</a></li>
<li class="toctree-l3"><a class="reference internal" href="#to-str">to_str</a></li>
<li class="toctree-l3"><a class="reference internal" href="#to-str-simple">to_str_simple</a></li>
<li class="toctree-l3"><a class="reference internal" href="#to-tensor">to_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#top-sort">top_sort</a></li>
<li class="toctree-l3"><a class="reference internal" href="#trivial-graph">trivial_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#union">union</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ctcloss">CtcLoss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#forward">forward</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#decodestateinfo">DecodeStateInfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#densefsavec">DenseFsaVec</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#init">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#from-dense-fsa-vec">_from_dense_fsa_vec</a></li>
<li class="toctree-l4"><a class="reference internal" href="#to">to</a></li>
<li class="toctree-l4"><a class="reference internal" href="#device">device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#duration">duration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#determinizeweightpushingtype">DeterminizeWeightPushingType</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#name">name</a></li>
<li class="toctree-l4"><a class="reference internal" href="#value">value</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fsa">Fsa</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#getattr">__getattr__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getitem">__getitem__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setattr">__setattr__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#str">__str__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-arc-post">_get_arc_post</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-backward-scores">_get_backward_scores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-entering-arcs">_get_entering_arcs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-forward-scores">_get_forward_scores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-tot-scores">_get_tot_scores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#invalidate-cache">_invalidate_cache_</a></li>
<li class="toctree-l4"><a class="reference internal" href="#as-dict">as_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convert-attr-to-ragged">convert_attr_to_ragged</a></li>
<li class="toctree-l4"><a class="reference internal" href="#draw">draw</a></li>
<li class="toctree-l4"><a class="reference internal" href="#from-openfst">from_openfst</a></li>
<li class="toctree-l4"><a class="reference internal" href="#from-str">from_str</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">get_arc_post</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">get_backward_scores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-filler">get_filler</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">get_forward_scores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">get_tot_scores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id10">invert</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">invert_</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rename-tensor-attribute">rename_tensor_attribute</a></li>
<li class="toctree-l4"><a class="reference internal" href="#requires-grad">requires_grad_</a></li>
<li class="toctree-l4"><a class="reference internal" href="#set-scores-stochastic">set_scores_stochastic</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">to</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id13">device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#grad">grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#num-arcs">num_arcs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#properties">properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="#properties-str">properties_str</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id14">requires_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#shape">shape</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#mwerloss">MWERLoss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id15">forward</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#nbest">Nbest</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#from-lattice">from_lattice</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id16">intersect</a></li>
<li class="toctree-l4"><a class="reference internal" href="#top-k">top_k</a></li>
<li class="toctree-l4"><a class="reference internal" href="#total-scores">total_scores</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#onlinedenseintersecter">OnlineDenseIntersecter</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id17">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#decode">decode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#num-streams">num_streams</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#raggedshape">RaggedShape</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#eq">__eq__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id18">__getitem__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id19">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ne">__ne__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#repr">__repr__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">__str__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id21">compose</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-layer">get_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#index">index</a></li>
<li class="toctree-l4"><a class="reference internal" href="#max-size">max_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numel">numel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#regular-ragged-shape">regular_ragged_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="#remove-axis">remove_axis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#row-ids">row_ids</a></li>
<li class="toctree-l4"><a class="reference internal" href="#row-splits">row_splits</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id22">to</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tot-size">tot_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tot-sizes">tot_sizes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id23">device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dim0">dim0</a></li>
<li class="toctree-l4"><a class="reference internal" href="#num-axes">num_axes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#raggedtensor">RaggedTensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id24">__eq__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id25">__getitem__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#getstate">__getstate__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id26">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id27">__ne__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id28">__repr__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setstate">__setstate__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id29">__str__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#add">add</a></li>
<li class="toctree-l4"><a class="reference internal" href="#arange">arange</a></li>
<li class="toctree-l4"><a class="reference internal" href="#argmax">argmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id30">cat</a></li>
<li class="toctree-l4"><a class="reference internal" href="#clone">clone</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id31">index</a></li>
<li class="toctree-l4"><a class="reference internal" href="#logsumexp">logsumexp</a></li>
<li class="toctree-l4"><a class="reference internal" href="#max">max</a></li>
<li class="toctree-l4"><a class="reference internal" href="#min">min</a></li>
<li class="toctree-l4"><a class="reference internal" href="#normalize">normalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id32">numel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pad">pad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id33">remove_axis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#remove-values-eq">remove_values_eq</a></li>
<li class="toctree-l4"><a class="reference internal" href="#remove-values-leq">remove_values_leq</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id34">requires_grad_</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sort">sort_</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sum">sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id35">to</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id36">to_str_simple</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tolist">tolist</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id37">tot_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unique">unique</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id38">device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id39">dim0</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dtype">dtype</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id40">grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#is-cuda">is_cuda</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id41">num_axes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id42">requires_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id43">shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="#values">values</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#rnntdecodingconfig">RnntDecodingConfig</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id44">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#beam">beam</a></li>
<li class="toctree-l4"><a class="reference internal" href="#decoder-history-len">decoder_history_len</a></li>
<li class="toctree-l4"><a class="reference internal" href="#max-contexts">max_contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#max-states">max_states</a></li>
<li class="toctree-l4"><a class="reference internal" href="#vocab-size">vocab_size</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#rnntdecodingstream">RnntDecodingStream</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id45">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id46">__str__</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#rnntdecodingstreams">RnntDecodingStreams</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id47">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id48">__str__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#advance">advance</a></li>
<li class="toctree-l4"><a class="reference internal" href="#format-output">format_output</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-contexts">get_contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#terminate-and-flush-to-streams">terminate_and_flush_to_streams</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#symboltable">SymbolTable</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id49">add</a></li>
<li class="toctree-l4"><a class="reference internal" href="#from-file">from_file</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id50">from_str</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get">get</a></li>
<li class="toctree-l4"><a class="reference internal" href="#merge">merge</a></li>
<li class="toctree-l4"><a class="reference internal" href="#to-file">to_file</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ids">ids</a></li>
<li class="toctree-l4"><a class="reference internal" href="#symbols">symbols</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#k2-ragged">k2.ragged</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id51">cat</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-ragged-shape2">create_ragged_shape2</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-ragged-tensor">create_ragged_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id52">index</a></li>
<li class="toctree-l3"><a class="reference internal" href="#index-and-sum">index_and_sum</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-ragged-shape">random_ragged_shape</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id53">regular_ragged_shape</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id54">RaggedShape</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id55">__eq__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id56">__getitem__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id57">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id58">__ne__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id59">__repr__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id60">__str__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id61">compose</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id62">get_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id63">index</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id64">max_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id65">numel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id66">regular_ragged_shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id67">remove_axis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id68">row_ids</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id69">row_splits</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id70">to</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id71">tot_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id72">tot_sizes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id73">device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id74">dim0</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id75">num_axes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id76">RaggedTensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id77">__eq__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id78">__getitem__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id79">__getstate__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id80">__init__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id81">__ne__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id82">__repr__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id83">__setstate__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id84">__str__</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id85">add</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id86">arange</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id87">argmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id88">cat</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id89">clone</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id90">index</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id91">logsumexp</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id93">max</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id94">min</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id95">normalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id96">numel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id97">pad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id98">remove_axis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id99">remove_values_eq</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id100">remove_values_leq</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id101">requires_grad_</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id102">sort_</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id103">sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id104">to</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id105">to_str_simple</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id106">tolist</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id107">tot_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id108">unique</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id109">device</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id110">dim0</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id111">dtype</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id112">grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id113">is_cuda</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id114">num_axes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id115">requires_grad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id116">shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id117">values</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">k2</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Python API reference</a> &raquo;</li>
      <li>k2</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/k2-fsa/k2/blob/master/docs/source/python_api/api.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="k2">
<h1>k2<a class="headerlink" href="#k2" title="Permalink to this headline"></a></h1>
<section id="add-epsilon-self-loops">
<h2>add_epsilon_self_loops<a class="headerlink" href="#add-epsilon-self-loops" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.add_epsilon_self_loops">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">add_epsilon_self_loops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ret_arc_map</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L623-L654"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.add_epsilon_self_loops" title="Permalink to this definition"></a></dt>
<dd><p>Add epsilon self-loops to an Fsa or FsaVec.</p>
<p>This is required when composing using a composition method that does not
treat epsilons specially, if the other FSA has epsilons in it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA. It can be either a single FSA or an FsaVec.</p></li>
<li><p><strong>ret_arc_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If False, return the resulting Fsa.
If True, return an extra arc map.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If ret_arc_map is False, return an instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code> that has an
epsilon self-loop on every non-final state.
If ret_arc_map is True, it returns an extra arc_map. arc_map[i] is the
arc index in the input <cite>fsa</cite> that corresponds to the i-th arc in the
resulting Fsa. arc_map[i] is -1 if the i-th arc in the resulting Fsa
has no counterpart in the input <cite>fsa</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="arc-sort">
<h2>arc_sort<a class="headerlink" href="#arc-sort" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.arc_sort">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">arc_sort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ret_arc_map</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L532-L595"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.arc_sort" title="Permalink to this definition"></a></dt>
<dd><p>Sort arcs of every state.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Arcs are sorted by labels first, and then by dest states.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>If the input <cite>fsa</cite> is already arc sorted, we return it directly.
Otherwise, a new sorted fsa is returned.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA.</p></li>
<li><p><strong>ret_arc_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to return an extra arc_map (a 1-D tensor with dtype being
torch.int32). arc_map[i] is the arc index in the input <cite>fsa</cite> that
corresponds to the i-th arc in the output Fsa.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If ret_arc_map is False, return the sorted FSA. It is the same as the
input <cite>fsa</cite> if the input <cite>fsa</cite> is arc sorted. Otherwise, a new sorted
fsa is returned and the input <cite>fsa</cite> is NOT modified.
If ret_arc_map is True, an extra arc map is also returned.</p>
</dd>
</dl>
<p><strong>Example: Sort a single FSA</strong></p>
<blockquote>
<div><div class="literal-block-wrapper docutils container" id="id119">
<div class="code-block-caption"><span class="caption-number">Listing 15 </span><span class="caption-text">Sort a single FSA</span><a class="headerlink" href="#id119" title="Permalink to this code"></a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/usr/bin/env python3</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="linenos"> 6</span><span class="s1">0 1 1 4 0.1</span>
<span class="linenos"> 7</span><span class="s1">0 1 3 5 0.2</span>
<span class="linenos"> 8</span><span class="s1">0 1 2 3 0.3</span>
<span class="linenos"> 9</span><span class="s1">0 2 5 2 0.4</span>
<span class="linenos">10</span><span class="s1">0 2 4 1 0.5</span>
<span class="linenos">11</span><span class="s1">1 2 2 3 0.6</span>
<span class="linenos">12</span><span class="s1">1 2 3 1 0.7</span>
<span class="linenos">13</span><span class="s1">1 2 1 2 0.8</span>
<span class="linenos">14</span><span class="s1">2 3 -1 -1 0.9</span>
<span class="linenos">15</span><span class="s1">3</span>
<span class="linenos">16</span><span class="s1">&#39;&#39;&#39;</span>
<span class="linenos">17</span><span class="n">fsa</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">Fsa</span><span class="o">.</span><span class="n">from_str</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">acceptor</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="linenos">18</span><span class="n">fsa</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;arc_sort_single_before.svg&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Before k2.arc_sort&#39;</span><span class="p">)</span>
<span class="linenos">19</span><span class="n">sorted_fsa</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">arc_sort</span><span class="p">(</span><span class="n">fsa</span><span class="p">)</span>
<span class="linenos">20</span><span class="n">sorted_fsa</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;arc_sort_single_after.svg&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;After k2.arc_sort&#39;</span><span class="p">)</span>
<span class="linenos">21</span>
<span class="linenos">22</span><span class="c1"># If you want to sort by aux_labels, you can use</span>
<span class="linenos">23</span><span class="n">inverted_fsa</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">invert</span><span class="p">(</span><span class="n">fsa</span><span class="p">)</span>
<span class="linenos">24</span><span class="n">sorted_fsa_2</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">arc_sort</span><span class="p">(</span><span class="n">inverted_fsa</span><span class="p">)</span>
<span class="linenos">25</span><span class="n">sorted_fsa_2</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">invert</span><span class="p">(</span><span class="n">sorted_fsa_2</span><span class="p">)</span>
<span class="linenos">26</span><span class="n">sorted_fsa_2</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;arc_sort_single_after_aux_labels.svg&#39;</span><span class="p">,</span>
<span class="linenos">27</span>                  <span class="n">title</span><span class="o">=</span><span class="s1">&#39;After k2.arc_sort by aux_labels&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-center" style="width: 600px">
<img alt="Fsa before k2.arc_sort" src="../_images/arc_sort_single_before.svg" /></figure>
<figure class="align-center" style="width: 600px">
<img alt="Fsa before k2.arc_sort" src="../_images/arc_sort_single_after.svg" /></figure>
<figure class="align-center" style="width: 600px">
<img alt="Fsa before k2.arc_sort" src="../_images/arc_sort_single_after_aux_labels.svg" /></figure>
</div></blockquote>
</dd></dl>

</section>
<section id="cat">
<h2>cat<a class="headerlink" href="#cat" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.cat">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">cat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">srcs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/ops.py#L198-L245"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.cat" title="Permalink to this definition"></a></dt>
<dd><p>Concatenate a list of FsaVec into a single FsaVec.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Only common tensor attributes are kept in the output FsaVec.
For non-tensor attributes, only one copy is kept in the output
FsaVec. We choose the first copy of the FsaVec that has the
lowest index in <cite>srcs</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>srcs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>]) – A list of FsaVec. Each element MUST be an FsaVec.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return a single FsaVec concatenated from the input FsaVecs.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="closure">
<h2>closure<a class="headerlink" href="#closure" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.closure">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">closure</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L828-L883"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.closure" title="Permalink to this definition"></a></dt>
<dd><p>Compute the Kleene closure of the input FSA.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA. It has to be a single FSA. That is,
len(fsa.shape) == 2.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The resulting FSA which is the Kleene closure of the input FSA.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="compose">
<h2>compose<a class="headerlink" href="#compose" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.compose">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">compose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a_fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b_fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">treat_epsilons_specially</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L416-L499"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.compose" title="Permalink to this definition"></a></dt>
<dd><p>Compute the composition of two FSAs.</p>
<p>When <cite>treat_epsilons_specially</cite> is True, this function works only on CPU.
When <cite>treat_epsilons_specially</cite> is False and both <cite>a_fsa</cite> and <cite>b_fsa</cite>
are on GPU, then this function works on GPU; in this case, the two
input FSAs do not need to be arc sorted.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>a_fsa.aux_labels</cite> is required to be defined and it can be either
a <cite>torch.Tensor</cite> or a ragged tensor of type <cite>k2.RaggedTensor</cite>.
If it is a ragged tensor, then it requires that a_fsa.requires_grad is
False.</p>
<p>For both FSAs, the <cite>aux_labels</cite> attribute is interpreted as output labels,
(olabels), and the composition involves matching the olabels of a_fsa with
the ilabels of b_fsa.  This is implemented by intersecting the inverse of
a_fsa (a_fsa_inv) with b_fsa, then replacing the ilabels of the result
with the original ilabels on a_fsa which are now the aux_labels of
a_fsa_inv.  If <cite>b_fsa.aux_labels</cite> is not defined, <cite>b_fsa</cite> is treated as an
acceptor (as in OpenFST), i.e. its olabels and ilabels are assumed to be
the same.</p>
</div>
<p>Refer to <a class="reference internal" href="#k2.intersect" title="k2.intersect"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.intersect()</span></code></a> for how we assign the attributes of the
output FSA.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a_fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The first input FSA. It can be either a single FSA or an FsaVec.</p></li>
<li><p><strong>b_fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The second input FSA. it can be either a single FSA or an FsaVec.</p></li>
<li><p><strong>treat_epsilons_specially</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True, epsilons will be treated as epsilon, meaning epsilon arcs can
match with an implicit epsilon self-loop.
If False, epsilons will be treated as real, normal symbols (to have
them treated as epsilons in this case you may have to add epsilon
self-loops to whichever of the inputs is naturally epsilon-free).</p></li>
<li><p><strong>inner_labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – If specified (and if a_fsa has <cite>aux_labels</cite>), the labels that we matched
on, which would normally be discarded, will instead be copied to
this attribute name.</p></li>
</ul>
</dd>
</dl>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><cite>b_fsa</cite> has to be arc sorted if the function runs on CPU.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result of composing a_fsa and b_fsa. <cite>len(out_fsa.shape)</cite> is 2
if and only if the two input FSAs are single FSAs;
otherwise, <cite>len(out_fsa.shape)</cite> is 3.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="compose-arc-maps">
<h2>compose_arc_maps<a class="headerlink" href="#compose-arc-maps" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.compose_arc_maps">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">compose_arc_maps</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step1_arc_map</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step2_arc_map</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/ops.py#L248-L276"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.compose_arc_maps" title="Permalink to this definition"></a></dt>
<dd><p>Compose arc maps from two Fsa operations.</p>
<p>It implements:</p>
<blockquote>
<div><ul class="simple">
<li><p>ans_arc_map[i] = step1_arc_map[step2_arc_map[i]] if
step2_arc_map[i] is not -1</p></li>
<li><p>ans_arc_map[i] = -1 if step2_arc_map[i] is -1</p></li>
</ul>
</div></blockquote>
<p>for i in 0 to <cite>step2_arc_map.numel() - 1</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>step1_arc_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A 1-D tensor with dtype torch.int32 from the first Fsa operation.</p></li>
<li><p><strong>step2_arc_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A 1-D tensor with dtype torch.int32 from the second Fsa operation.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return a 1-D tensor with dtype torch.int32. It has the same number
of elements as step2_arc_map. That is,
ans_arc_map.shape == step2_arc_map.shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="connect">
<h2>connect<a class="headerlink" href="#connect" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.connect">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">connect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L502-L529"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.connect" title="Permalink to this definition"></a></dt>
<dd><p>Connect an FSA.</p>
<p>Removes states that are neither accessible nor co-accessible.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A state is not accessible if it is not reachable from the start state.
A state is not co-accessible if it cannot reach the final state.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>If the input FSA is already connected, it is returned directly.
Otherwise, a new connected FSA is returned.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA to be connected.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An FSA that is connected.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="convert-dense-to-fsa-vec">
<h2>convert_dense_to_fsa_vec<a class="headerlink" href="#convert-dense-to-fsa-vec" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.convert_dense_to_fsa_vec">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">convert_dense_to_fsa_vec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dense_fsa_vec</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/dense_fsa_vec.py#L231-L246"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.convert_dense_to_fsa_vec" title="Permalink to this definition"></a></dt>
<dd><p>Convert a DenseFsaVec to an FsaVec.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Intended for use in testing/debug mode only. This operation is NOT
differentiable.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dense_fsa_vec</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code>) – DenseFsaVec to convert.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The converted FsaVec .</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="create-fsa-vec">
<h2>create_fsa_vec<a class="headerlink" href="#create-fsa-vec" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.create_fsa_vec">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">create_fsa_vec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsas</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L250-L306"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.create_fsa_vec" title="Permalink to this definition"></a></dt>
<dd><p>Create an FsaVec from a list of FSAs</p>
<p>We use the following rules to set the attributes of the output FsaVec:</p>
<ul class="simple">
<li><p>For tensor attributes, we assume that all input FSAs have the same
attribute name and the values are concatenated.</p></li>
<li><p>For non-tensor attributes, if any two of the input FSAs have the same
attribute name, then we assume that their attribute values are equal and
the output FSA will inherit the attribute.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsas</strong> – A list of <cite>Fsa</cite>. Each element must be a single FSA.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code> that represents a FsaVec.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="create-sparse">
<h2>create_sparse<a class="headerlink" href="#create-sparse" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.create_sparse">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">create_sparse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_col_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L361-L420"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.create_sparse" title="Permalink to this definition"></a></dt>
<dd><p>This is a utility function that creates a (torch) sparse matrix likely
intended to represent posteriors.  The likely usage is something like
(for example):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">post</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">create_sparse</span><span class="p">(</span><span class="n">fsa</span><span class="o">.</span><span class="n">seqframe</span><span class="p">,</span> <span class="n">fsa</span><span class="o">.</span><span class="n">phones</span><span class="p">,</span>
                        <span class="n">fsa</span><span class="o">.</span><span class="n">get_arc_post</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">(),</span>
                        <span class="n">min_col_index</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>(assuming <cite>seqframe</cite> and <cite>phones</cite> were integer-valued attributes of <cite>fsa</cite>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rows</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Row indexes of the sparse matrix (a torch.Tensor), which must have
values &gt;= 0; likely <cite>fsa.seqframe</cite>.   Must have row_indexes.dim == 1.
Will be converted to <cite>dtype=torch.long</cite></p></li>
<li><p><strong>cols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Column indexes of the sparse matrix, with the same shape as <cite>rows</cite>.
Will be converted to <cite>dtype=torch.long</cite></p></li>
<li><p><strong>values</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Values of the sparse matrix, likely of dtype float or double, with
the same shape as <cite>rows</cite> and <cite>cols</cite>.</p></li>
<li><p><strong>size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]) – Optional. If not None, it is assumed to be a tuple containing
<cite>(num_frames, highest_phone_plus_one)</cite></p></li>
<li><p><strong>min_col_index</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – If provided, before the sparse tensor is constructed we will filter out
elements with <cite>cols[i] &lt; min_col_index</cite>.  Will likely be 0 or 1, if
set.  This is necessary if <cite>col_indexes</cite> may have values less than 0,
or if you want to filter out 0 values (e.g. as representing blanks).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a torch.Tensor that is sparse with coo (coordinate) format,
i.e. <cite>layout=torch.sparse_coo</cite> (which is actually the only sparse format
that torch currently supports).</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="ctc-graph">
<h2>ctc_graph<a class="headerlink" href="#ctc-graph" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ctc_graph">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">ctc_graph</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modified</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L1160-L1249"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.ctc_graph" title="Permalink to this definition"></a></dt>
<dd><p>Construct ctc graphs from symbols.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The scores of arcs in the returned FSA are all 0.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>symbols</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>]) – <p>It can be one of the following types:</p>
<blockquote>
<div><ul>
<li><p>A list of list-of-integers, e..g, <cite>[ [1, 2], [1, 2, 3] ]</cite></p></li>
<li><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>.
Must have <cite>num_axes == 2</cite>.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>standard</strong> – Option to specify the type of CTC topology: “standard” or “simplified”,
where the “standard” one makes the blank mandatory between a pair of
identical symbols. Default True.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Optional. It can be either a string (e.g., ‘cpu’, ‘cuda:0’) or a
torch.device.
By default, the returned FSA is on CPU.
If <cite>symbols</cite> is an instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>, the returned
FSA will on the same device as <cite>k2.RaggedTensor</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An FsaVec containing the returned ctc graphs, with “Dim0()” the same as
“len(symbols)”(List[List[int]]) or “dim0”(k2.RaggedTensor)</p>
</dd>
</dl>
<p><strong>Example 1</strong></p>
<blockquote>
<div><div class="literal-block-wrapper docutils container" id="id120">
<div class="code-block-caption"><span class="caption-number">Listing 16 </span><span class="caption-text">Usage of k2.ctc_graph</span><a class="headerlink" href="#id120" title="Permalink to this code"></a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/usr/bin/env python3</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">isym</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">SymbolTable</span><span class="o">.</span><span class="n">from_str</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="linenos"> 6</span><span class="s1">blk 0</span>
<span class="linenos"> 7</span><span class="s1">a 1</span>
<span class="linenos"> 8</span><span class="s1">b 2</span>
<span class="linenos"> 9</span><span class="s1">c 3</span>
<span class="linenos">10</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="n">osym</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">SymbolTable</span><span class="o">.</span><span class="n">from_str</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="linenos">13</span><span class="s1">a 1</span>
<span class="linenos">14</span><span class="s1">b 2</span>
<span class="linenos">15</span><span class="s1">c 3</span>
<span class="linenos">16</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="n">fsa</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">ctc_graph</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">modified</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="linenos">19</span><span class="n">fsa_modified</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">ctc_graph</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">modified</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">20</span>
<span class="linenos">21</span><span class="n">fsa</span><span class="o">.</span><span class="n">labels_sym</span> <span class="o">=</span> <span class="n">isym</span>
<span class="linenos">22</span><span class="n">fsa</span><span class="o">.</span><span class="n">aux_labels_sym</span> <span class="o">=</span> <span class="n">osym</span>
<span class="linenos">23</span>
<span class="linenos">24</span><span class="n">fsa_modified</span><span class="o">.</span><span class="n">labels_sym</span> <span class="o">=</span> <span class="n">isym</span>
<span class="linenos">25</span><span class="n">fsa_modified</span><span class="o">.</span><span class="n">aux_labels_sym</span> <span class="o">=</span> <span class="n">osym</span>
<span class="linenos">26</span>
<span class="linenos">27</span><span class="c1"># fsa is an FsaVec, so we use fsa[0] to visualize the first Fsa</span>
<span class="linenos">28</span><span class="n">fsa</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;ctc_graph.svg&#39;</span><span class="p">,</span>
<span class="linenos">29</span>            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;CTC graph for the string &quot;abbc&quot; (modified=False)&#39;</span><span class="p">)</span>
<span class="linenos">30</span><span class="n">fsa_modified</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;modified_ctc_graph.svg&#39;</span><span class="p">,</span>
<span class="linenos">31</span>                     <span class="n">title</span><span class="o">=</span><span class="s1">&#39;CTC graph for the string &quot;abbc&quot; (modified=True)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-center" id="id121" style="width: 600px">
<img alt="CTC graph (modified=False)" src="../_images/ctc_graph.svg" /><figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">Note: There is a mandatory blank between state 3 and 5.</span><a class="headerlink" href="#id121" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id122" style="width: 600px">
<img alt="CTC graph (modified=True)" src="../_images/modified_ctc_graph.svg" /><figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">Note: There is <strong>no</strong> mandatory blank between state 3 and 5.</span><a class="headerlink" href="#id122" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</div></blockquote>
<p><strong>Example 2 Construct a CTC graph using composition</strong></p>
<blockquote>
<div><div class="literal-block-wrapper docutils container" id="id123">
<div class="code-block-caption"><span class="caption-number">Listing 17 </span><span class="caption-text">Construct a CTC graph using composition</span><a class="headerlink" href="#id123" title="Permalink to this code"></a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/usr/bin/env python3</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c1"># Construct a CTC graph by intersection</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="n">isym</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">SymbolTable</span><span class="o">.</span><span class="n">from_str</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="linenos"> 8</span><span class="s1">blk 0</span>
<span class="linenos"> 9</span><span class="s1">a 1</span>
<span class="linenos">10</span><span class="s1">b 2</span>
<span class="linenos">11</span><span class="s1">c 3</span>
<span class="linenos">12</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
<span class="linenos">13</span>
<span class="linenos">14</span><span class="n">osym</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">SymbolTable</span><span class="o">.</span><span class="n">from_str</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="linenos">15</span><span class="s1">a 1</span>
<span class="linenos">16</span><span class="s1">b 2</span>
<span class="linenos">17</span><span class="s1">c 3</span>
<span class="linenos">18</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
<span class="linenos">19</span>
<span class="linenos">20</span><span class="n">linear_fsa</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">linear_fsa</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="linenos">21</span><span class="n">linear_fsa</span><span class="o">.</span><span class="n">labels_sym</span> <span class="o">=</span> <span class="n">isym</span>
<span class="linenos">22</span>
<span class="linenos">23</span><span class="n">ctc_topo</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">ctc_topo</span><span class="p">(</span><span class="n">max_token</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">modified</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="linenos">24</span><span class="n">ctc_topo_modified</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">ctc_topo</span><span class="p">(</span><span class="n">max_token</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">modified</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">25</span>
<span class="linenos">26</span><span class="n">ctc_topo</span><span class="o">.</span><span class="n">labels_sym</span> <span class="o">=</span> <span class="n">isym</span>
<span class="linenos">27</span><span class="n">ctc_topo</span><span class="o">.</span><span class="n">aux_labels_sym</span> <span class="o">=</span> <span class="n">osym</span>
<span class="linenos">28</span>
<span class="linenos">29</span><span class="n">ctc_topo_modified</span><span class="o">.</span><span class="n">labels_sym</span> <span class="o">=</span> <span class="n">isym</span>
<span class="linenos">30</span><span class="n">ctc_topo_modified</span><span class="o">.</span><span class="n">aux_labels_sym</span> <span class="o">=</span> <span class="n">osym</span>
<span class="linenos">31</span>
<span class="linenos">32</span><span class="n">ctc_graph</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="n">ctc_topo</span><span class="p">,</span> <span class="n">linear_fsa</span><span class="p">)</span>
<span class="linenos">33</span><span class="n">ctc_graph_modified</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="n">ctc_topo_modified</span><span class="p">,</span> <span class="n">linear_fsa</span><span class="p">)</span>
<span class="linenos">34</span>
<span class="linenos">35</span><span class="n">linear_fsa</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;linear_fsa.svg&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Linear FSA of the string &quot;abbc&quot;&#39;</span><span class="p">)</span>
<span class="linenos">36</span><span class="n">ctc_topo</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;ctc_topo.svg&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;CTC topology&#39;</span><span class="p">)</span>
<span class="linenos">37</span><span class="n">ctc_topo_modified</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;ctc_topo_modified.svg&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Modified CTC topology&#39;</span><span class="p">)</span>
<span class="linenos">38</span>
<span class="linenos">39</span><span class="n">ctc_graph</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;ctc_topo_compose_linear_fsa.svg&#39;</span><span class="p">,</span>
<span class="linenos">40</span>               <span class="n">title</span><span class="o">=</span><span class="s1">&#39;k2.compose(ctc_topo, linear_fsa)&#39;</span><span class="p">)</span>
<span class="linenos">41</span>
<span class="linenos">42</span><span class="n">ctc_graph_modified</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;ctc_topo_modified_compose_linear_fsa.svg&#39;</span><span class="p">,</span>
<span class="linenos">43</span>                        <span class="n">title</span><span class="o">=</span><span class="s1">&#39;k2.compose(ctc_topo_modified, linear_fsa)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-center" style="width: 600px">
<img alt="Linear FSA" src="../_images/linear_fsa.svg" /></figure>
<figure class="align-center" style="width: 600px">
<img alt="CTC topology (modified=False)" src="../_images/ctc_topo.svg" /></figure>
<figure class="align-center" style="width: 600px">
<img alt="k2.compose(ctc_topo, linear_fsa)" src="../_images/ctc_topo_compose_linear_fsa.svg" /></figure>
<figure class="align-center" style="width: 600px">
<img alt="CTC topology (modified=True)" src="../_images/ctc_topo_modified.svg" /></figure>
<figure class="align-center" style="width: 600px">
<img alt="k2.compose(ctc_topo_modified, linear_fsa)" src="../_images/ctc_topo_modified_compose_linear_fsa.svg" /></figure>
</div></blockquote>
</dd></dl>

</section>
<section id="ctc-loss">
<h2>ctc_loss<a class="headerlink" href="#ctc-loss" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ctc_loss">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">ctc_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decoding_graph</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_fsa_vec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_beam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delay_penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sum'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_lengths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/ctc_loss.py#L154-L208"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.ctc_loss" title="Permalink to this definition"></a></dt>
<dd><p>Compute the CTC loss given a decoding graph and a dense fsa vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decoding_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec. It can be the composition result of a ctc topology
and a transcript.</p></li>
<li><p><strong>dense_fsa_vec</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code>) – It represents the neural network output. Refer to the help information
in <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.DenseFsaVec</span></code>.</p></li>
<li><p><strong>output_beam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Beam to prune output, similar to lattice-beam in Kaldi.  Relative
to best path of output.</p></li>
<li><p><strong>delay_penalty</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – A constant to penalize symbol delay, which is used to make symbol
emit earlier for streaming models. It is almost the same as the
<cite>delay_penalty</cite> in our <cite>rnnt_loss</cite>, See
<a class="reference external" href="https://github.com/k2-fsa/k2/issues/955">https://github.com/k2-fsa/k2/issues/955</a> and
<a class="reference external" href="https://arxiv.org/pdf/2211.00490.pdf">https://arxiv.org/pdf/2211.00490.pdf</a> for more details.</p></li>
<li><p><strong>reduction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[‘none’, ‘mean’, ‘sum’]) – Specifies the reduction to apply to the output: ‘none’ | ‘mean’ | ‘sum’.
‘none’: no reduction will be applied, ‘mean’: the output losses will be
divided by the target lengths and then the mean over the batch is taken.
‘sum’: sum the output losses over batches.</p></li>
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use double precision floating point in computing
the total scores. False to use single precision.</p></li>
<li><p><strong>target_lengths</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – Used only when <cite>reduction</cite> is <cite>mean</cite>. It is a 1-D tensor of batch
size representing lengths of the targets, e.g., number of phones or
number of word pieces in a sentence.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If <cite>reduction</cite> is <cite>none</cite>, return a 1-D tensor with size equal to batch
size. If <cite>reduction</cite> is <cite>mean</cite> or <cite>sum</cite>, return a scalar.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="ctc-topo">
<h2>ctc_topo<a class="headerlink" href="#ctc-topo" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ctc_topo">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">ctc_topo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_token</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modified</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L1252-L1306"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.ctc_topo" title="Permalink to this definition"></a></dt>
<dd><p>Create a CTC topology.</p>
<p>A token which appears once on the right side (i.e. olabels) may
appear multiple times on the left side (ilabels), possibly with
epsilons in between.
When 0 appears on the left side, it represents the blank symbol;
when it appears on the right side, it indicates an epsilon. That
is, 0 has two meanings here.</p>
<p>A standard CTC topology is the conventional one, where there
is a mandatory blank between two repeated neighboring symbols.
A non-standard, i.e., modified CTC topology, imposes no such constraint.</p>
<p>See <a class="reference external" href="https://github.com/k2-fsa/k2/issues/746#issuecomment-856421616">https://github.com/k2-fsa/k2/issues/746#issuecomment-856421616</a>
and <a class="reference external" href="https://github.com/k2-fsa/snowfall/pull/209">https://github.com/k2-fsa/snowfall/pull/209</a>
for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_token</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The maximum token ID (inclusive). We assume that token IDs
are contiguous (from 1 to <cite>max_token</cite>). 0 represents blank.</p></li>
<li><p><strong>modified</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If False, create a standard CTC topology. Otherwise, create a
modified CTC topology.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Optional. It can be either a string (e.g., ‘cpu’,
‘cuda:0’) or a torch.device.
If it is None, then the returned FSA is on CPU.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return either a standard or a modified CTC topology as an FSA
depending on whether <cite>standard</cite> is True or False.</p>
</dd>
</dl>
<p><strong>Example</strong></p>
<blockquote>
<div><div class="literal-block-wrapper docutils container" id="id124">
<div class="code-block-caption"><span class="caption-number">Listing 18 </span><span class="caption-text">Usage of k2.ctc_topo</span><a class="headerlink" href="#id124" title="Permalink to this code"></a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/usr/bin/env python3</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">isym</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">SymbolTable</span><span class="o">.</span><span class="n">from_str</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="linenos"> 6</span><span class="s1">blk 0</span>
<span class="linenos"> 7</span><span class="s1">a 1</span>
<span class="linenos"> 8</span><span class="s1">b 2</span>
<span class="linenos"> 9</span><span class="s1">c 3</span>
<span class="linenos">10</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="n">osym</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">SymbolTable</span><span class="o">.</span><span class="n">from_str</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="linenos">13</span><span class="s1">a 1</span>
<span class="linenos">14</span><span class="s1">b 2</span>
<span class="linenos">15</span><span class="s1">c 3</span>
<span class="linenos">16</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="n">fsa</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">ctc_topo</span><span class="p">(</span><span class="n">max_token</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">modified</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="linenos">19</span><span class="n">fsa_modified</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">ctc_topo</span><span class="p">(</span><span class="n">max_token</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">modified</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">20</span>
<span class="linenos">21</span><span class="n">fsa</span><span class="o">.</span><span class="n">labels_sym</span> <span class="o">=</span> <span class="n">isym</span>
<span class="linenos">22</span><span class="n">fsa</span><span class="o">.</span><span class="n">aux_labels_sym</span> <span class="o">=</span> <span class="n">osym</span>
<span class="linenos">23</span>
<span class="linenos">24</span><span class="n">fsa_modified</span><span class="o">.</span><span class="n">labels_sym</span> <span class="o">=</span> <span class="n">isym</span>
<span class="linenos">25</span><span class="n">fsa_modified</span><span class="o">.</span><span class="n">aux_labels_sym</span> <span class="o">=</span> <span class="n">osym</span>
<span class="linenos">26</span>
<span class="linenos">27</span><span class="n">fsa</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;ctc_topo.svg&#39;</span><span class="p">,</span>
<span class="linenos">28</span>         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;CTC topology with max_token=3 (modified=False)&#39;</span><span class="p">)</span>
<span class="linenos">29</span><span class="n">fsa_modified</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;modified_ctc_topo.svg&#39;</span><span class="p">,</span>
<span class="linenos">30</span>                  <span class="n">title</span><span class="o">=</span><span class="s1">&#39;CTC topology with max_token=3 (modified=True)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-center" style="width: 600px">
<img alt="CTC topology" src="../_images/ctc_topo1.svg" /></figure>
<figure class="align-center" style="width: 600px">
<img alt="CTC topology" src="../_images/modified_ctc_topo.svg" /></figure>
</div></blockquote>
</dd></dl>

</section>
<section id="determinize">
<h2>determinize<a class="headerlink" href="#determinize" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.determinize">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">determinize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_pushing_type=&lt;DeterminizeWeightPushingType.kNoWeightPushing:</span> <span class="pre">2&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L777-L825"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.determinize" title="Permalink to this definition"></a></dt>
<dd><p>Determinize the input Fsa.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<ul class="simple">
<li><p>It only works on for CPU.</p></li>
<li><p>Any weight_pushing_type value other than kNoWeightPushing causes
the ‘arc_derivs’ to not accurately reflect the real derivatives,
although this will not matter as long as the derivatives ultimately
derive from FSA operations such as getting total scores or
arc posteriors, which are insensitive to pushing.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA. It can be either a single FSA or an FsaVec.
Must be connected. It’s also expected to be epsilon-free,
but this is not checked; in any case,
epsilon will be treated as a normal symbol.</p></li>
<li><p><strong>weight_pushing_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DeterminizeWeightPushingType</span></code>) – <p>An enum value that determines what kind of weight pushing is desired,
default kNoWeightPushing.</p>
<blockquote>
<div><dl class="simple">
<dt>kTropicalWeightPushing:</dt><dd><p>use tropical semiring (actually, max on scores) for weight pushing.</p>
</dd>
<dt>kLogWeightPushing:</dt><dd><p>use log semiring (actually, log-sum on score) for weight pushing</p>
</dd>
<dt>kNoWeightPushing:</dt><dd><p>do no weight pushing; this will cause some delay in scores being
emitted, and the weights created in this way will correspond
exactly to those that would be produced by the arc_derivs.</p>
</dd>
</dl>
</div></blockquote>
<p>For decoding graph creation, we recommend kLogSumWeightPushing.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The resulting Fsa, it’s equivalent to the input <cite>fsa</cite> under
tropical semiring but will be deterministic.
It will be the same as the input <cite>fsa</cite> if the input
<cite>fsa</cite> has property kFsaPropertiesArcSortedAndDeterministic.
Otherwise, a new deterministic fsa is returned and the
input <cite>fsa</cite> is NOT modified.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="do-rnnt-pruning">
<h2>do_rnnt_pruning<a class="headerlink" href="#do-rnnt-pruning" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.do_rnnt_pruning">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">do_rnnt_pruning</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">am</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranges</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L913-L954"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.do_rnnt_pruning" title="Permalink to this definition"></a></dt>
<dd><p>Prune the output of encoder(am) and prediction network(lm) with ranges
generated by <cite>get_rnnt_prune_ranges</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>am</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The encoder output, with shape (B, T, encoder_dim)</p></li>
<li><p><strong>lm</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The prediction network output, with shape (B, S + 1, decoder_dim)</p></li>
<li><p><strong>ranges</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A tensor containing the symbol indexes for each frame that we want to
keep. Its shape is (B, T, s_range), see the docs in
<cite>get_rnnt_prune_ranges</cite> for more details of this tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return the pruned am and lm with shape (B, T, s_range, C)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="expand-ragged-attributes">
<h2>expand_ragged_attributes<a class="headerlink" href="#expand-ragged-attributes" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.expand_ragged_attributes">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">expand_ragged_attributes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ret_arc_map</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ragged_attribute_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L1009-L1107"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.expand_ragged_attributes" title="Permalink to this definition"></a></dt>
<dd><p>Turn ragged labels attached to this FSA into linear (Tensor) labels,
expanding arcs into sequences of arcs as necessary to achieve this.
Supports autograd.  If <cite>fsas</cite> had no ragged attributes, returns <cite>fsas</cite>
itself.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This function will ensure that for final-arcs in the returned
fsa, the corresponding labels for all ragged attributes are -1; it will
add an extra arc at the end if necessary to ensure this, if the
original ragged attributes did not have -1 as their final element on
final-arcs (note: our intention is that -1’s on final arcs, like filler
symbols, are removed when making attributes ragged; this is what
fsa_from_unary_function_ragged() does if remove_filler==True (the
default).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The source Fsa</p></li>
<li><p><strong>ret_arc_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true, will return a pair (new_fsas, arc_map)
with <cite>arc_map</cite> a tensor of int32 that maps from arcs in the
result to arcs in <cite>fsas</cite>, with -1’s for newly created arcs.
If false, just returns new_fsas.</p></li>
<li><p><strong>ragged_attribute_names</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – If specified, just this list of ragged
attributes will be expanded to linear tensor attributes, and
the rest will stay ragged.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-aux-labels">
<h2>get_aux_labels<a class="headerlink" href="#get-aux-labels" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_aux_labels">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_aux_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">best_paths</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/decode.py#L117-L150"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_aux_labels" title="Permalink to this definition"></a></dt>
<dd><p>Extract aux_labels from the best-path FSAs and remove 0s and -1s.
:type best_paths: <code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>
:param best_paths: An Fsa with best_paths.arcs.num_axes() == 3, i.e.</p>
<blockquote>
<div><p>containing multiple FSAs, which is expected to be the result
of <cite>shortest_path</cite> (otherwise the returned values won’t
be meaningful).</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a list of lists of int, containing the label sequences we
decoded.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-best-matching-stats">
<h2>get_best_matching_stats<a class="headerlink" href="#get-best-matching-stats" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_best_matching_stats">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_best_matching_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">counts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_token</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_token</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_order</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L696-L786"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_best_matching_stats" title="Permalink to this definition"></a></dt>
<dd><p>For “query” sentences, this function gets the mean and variance of
scores from the best matching words-in-context in a set of provided “key”
sentences. This matching process matches the word and the words preceding
it, looking for the highest-order match it can find (it’s intended for
approximating the scores of models that see only left-context,
like language models). The intended application is in estimating the scores
of hypothesized transcripts, when we have actually computed the scores for
only a subset of the hypotheses.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This function only runs on CPU for now.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokens</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>) – <p>A ragged tensor of int32_t with 2 or 3 axes. If 2 axes, this represents
a collection of key and query sequences. If 3 axes, this represents a
set of such collections.</p>
<blockquote>
<div><p>2-axis example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span> <span class="n">the</span><span class="p">,</span> <span class="n">cat</span><span class="p">,</span> <span class="n">said</span><span class="p">,</span> <span class="n">eos</span> <span class="p">],</span> <span class="p">[</span> <span class="n">the</span><span class="p">,</span> <span class="n">cat</span><span class="p">,</span> <span class="n">fed</span><span class="p">,</span> <span class="n">eos</span> <span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>3-axis example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span> <span class="p">[</span> <span class="n">the</span><span class="p">,</span> <span class="n">cat</span><span class="p">,</span> <span class="n">said</span><span class="p">,</span> <span class="n">eos</span> <span class="p">],</span> <span class="p">[</span> <span class="n">the</span><span class="p">,</span> <span class="n">cat</span><span class="p">,</span> <span class="n">fed</span><span class="p">,</span> <span class="n">eos</span> <span class="p">]</span> <span class="p">],</span>
  <span class="p">[</span> <span class="p">[</span> <span class="n">hi</span><span class="p">,</span> <span class="n">my</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="ow">is</span><span class="p">,</span> <span class="n">eos</span> <span class="p">],</span> <span class="p">[</span> <span class="n">bye</span><span class="p">,</span> <span class="n">my</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="ow">is</span><span class="p">,</span> <span class="n">eos</span> <span class="p">]</span> <span class="p">],</span> <span class="o">...</span> <span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<p>where the words would actually be represented as integers,
The eos symbol is required if this code is to work as intended
(otherwise this code will not be able to recognize when we have reached
the beginnings of sentences when comparing histories).
bos symbols are allowed but not required.</p>
</p></li>
<li><p><strong>scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A one dim torch.tensor with scores.size() == tokens.NumElements(),
this is the item for which we are requesting best-matching values
(as means and variances in case there are multiple best matches).
In our anticipated use, these would represent scores of words in the
sentences, but they could represent anything.</p></li>
<li><p><strong>counts</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – An one dim torch.tensor with counts.size() == tokens.NumElements(),
containing 1 for words that are considered “keys” and 0 for
words that are considered “queries”.  Typically some entire
sentences will be keys and others will be queries.</p></li>
<li><p><strong>eos</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The value of the eos (end of sentence) symbol; internally, this
is used as an extra padding value before the first sentence in each
collection, so that it can act like a “bos” symbol.</p></li>
<li><p><strong>min_token</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The lowest possible token value, including the bos
symbol (e.g., might be -1).</p></li>
<li><p><strong>max_token</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The maximum possible token value.  Be careful not to
set this too large the implementation contains a part which
takes time and space O(max_token - min_token).</p></li>
<li><p><strong>max_order</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The maximum n-gram order to ever return in the
<cite>ngram_order</cite> output; the output will be the minimum of max_order
and the actual order matched; or max_order if we matched all the
way to the beginning of both sentences. The main reason this is
needed is that we need a finite number to return at the
beginning of sentences.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><dl class="simple">
<dt>Returns a tuple of four torch.tensor (mean, var, counts_out, ngram_order)</dt><dd><dl class="simple">
<dt>mean:</dt><dd><p>For query positions, will contain the mean of the scores at the
best matching key positions, or zero if that is undefined because
there are no key positions at all.  For key positions,
you can treat the output as being undefined (actually they
are treated the same as queries, but won’t match with only
themselves because we don’t match at singleton intervals).</p>
</dd>
<dt>var:</dt><dd><p>Like <cite>mean</cite>, but contains the (centered) variance
of the best matching positions.</p>
</dd>
<dt>counts_out:</dt><dd><p>The number of key positions that contributed to the <cite>mean</cite>
and <cite>var</cite> statistics.  This should only be zero if <cite>counts</cite>
was all zero.</p>
</dd>
<dt>ngram_order:</dt><dd><p>The n-gram order corresponding to the best matching
positions found at each query position, up to a maximum of
<cite>max_order</cite>; will be <cite>max_order</cite> if we matched all
the way to the beginning of a sentence.</p>
</dd>
</dl>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-lattice">
<h2>get_lattice<a class="headerlink" href="#get-lattice" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_lattice">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_lattice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">log_prob</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_prob_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoding_graph</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search_beam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_beam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_active_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_active_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subsampling_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/decode.py#L17-L95"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_lattice" title="Permalink to this definition"></a></dt>
<dd><p>Get the decoding lattice from a decoding graph and  log_softmax output.
:type log_prob: <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>
:param log_prob: Output from a log_softmax layer of shape <code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">T,</span> <span class="pre">C)</span></code>.
:type log_prob_len: <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>
:param log_prob_len: A tensor of shape <code class="docutils literal notranslate"><span class="pre">(N,)</span></code> containing number of valid frames from</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">log_prob</span></code> before padding.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decoding_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An Fsa, the decoding graph. It can be either an <code class="docutils literal notranslate"><span class="pre">HLG</span></code> or an <code class="docutils literal notranslate"><span class="pre">H</span></code>.
You can use <a class="reference internal" href="#k2.ctc_topo" title="k2.ctc_topo"><code class="xref py py-func docutils literal notranslate"><span class="pre">ctc_topo()</span></code></a> to build an <code class="docutils literal notranslate"><span class="pre">H</span></code>.</p></li>
<li><p><strong>search_beam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Decoding beam, e.g. 20.  Smaller is faster, larger is more exact
(less pruning). This is the default value; it may be modified by
<cite>min_active_states</cite> and <cite>max_active_states</cite>.</p></li>
<li><p><strong>output_beam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Beam to prune output, similar to lattice-beam in Kaldi.  Relative
to best path of output.</p></li>
<li><p><strong>min_active_states</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Minimum number of FSA states that are allowed to be active on any given
frame for any given intersection/composition task. This is advisory,
in that it will try not to have fewer than this number active.
Set it to zero if there is no constraint.</p></li>
<li><p><strong>max_active_states</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Maximum number of FSA states that are allowed to be active on any given
frame for any given intersection/composition task. This is advisory,
in that it will try not to exceed that but may not always succeed.
You can use a very large number if no constraint is needed.</p></li>
<li><p><strong>subsampling_factor</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The subsampling factor of the model.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An FsaVec containing the decoding result. It has axes [utt][state][arc].</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-rnnt-logprobs">
<h2>get_rnnt_logprobs<a class="headerlink" href="#get-rnnt-logprobs" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_rnnt_logprobs">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_rnnt_logprobs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">am</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">termination_symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnnt_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'regular'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L65-L225"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_rnnt_logprobs" title="Permalink to this definition"></a></dt>
<dd><p>Reduces RNN-T problem (the simple case, where joiner network is just
addition), to a compact, standard form that can then be given
(with boundaries) to mutual_information_recursion().
This function is called from rnnt_loss_simple(), but may be useful for
other purposes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lm</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>Language model part of un-normalized logprobs of symbols, to be added to
acoustic model part before normalizing.  Of shape:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">S</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="n">C</span><span class="p">]</span>
</pre></div>
</div>
<p>where B is the batch size, S is the maximum sequence length of
the symbol sequence, possibly including the EOS symbol; and
C is size of the symbol vocabulary, including the termination/next-frame
symbol.
Conceptually, lm[b][s] is a vector of length [C] representing the
“language model” part of the un-normalized logprobs of symbols,
given all symbols <em>earlier than</em> s in the sequence.  The reason
we still need this for position S is that we may still be emitting
the termination/next-frame symbol at this point.</p>
</p></li>
<li><p><strong>am</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>Acoustic-model part of un-normalized logprobs of symbols, to be added
to language-model part before normalizing.  Of shape:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">T</span><span class="p">][</span><span class="n">C</span><span class="p">]</span>
</pre></div>
</div>
<p>where B is the batch size, T is the maximum sequence length of
the acoustic sequences (in frames); and C is size of the symbol
vocabulary, including the termination/next-frame symbol.  It reflects
the “acoustic” part of the probability of any given symbol appearing
next on this frame.</p>
</p></li>
<li><p><strong>symbols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A LongTensor of shape [B][S], containing the symbols at each position
of the sequence.</p></li>
<li><p><strong>termination_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The identity of the termination symbol, must be in {0..C-1}</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a optional LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame] that is treated as
[0, 0, S, T]
if boundary is not supplied.
Most likely you will want begin_symbol and begin_frame to be zero.</p></li>
<li><p><strong>rnnt_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – <p>Specifies the type of rnnt paths: <cite>regular</cite>, <cite>modified</cite> or <cite>constrained</cite>.
<cite>regular</cite>: The regular rnnt that taking you to the next frame only if</p>
<blockquote>
<div><p>emitting a blank (i.e., emitting a symbol does not take you
to the next frame).</p>
</div></blockquote>
<dl class="simple">
<dt><cite>modified</cite>: A modified version of rnnt that will take you to the next</dt><dd><p>frame either emitting a blank or a non-blank symbol.</p>
</dd>
<dt><cite>constrained</cite>: A version likes the modified one that will go to the next</dt><dd><p>frame when you emit a non-blank symbol, but this is done
by “forcing” you to take the blank transition from the
<em>next</em> context on the <em>current</em> frame, e.g. if we emit
c given “a b” context, we are forced to emit “blank”
given “b c” context on the current frame.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><blockquote>
<div><dl>
<dt>(px, py) (the names are quite arbitrary).</dt><dd><dl class="simple">
<dt>px: logprobs, of shape [B][S][T+1] if rnnt_type is regular,</dt><dd><p>[B][S][T] if rnnt_type is not regular.</p>
</dd>
</dl>
<p>py: logprobs, of shape [B][S+1][T]</p>
</dd>
</dl>
</div></blockquote>
<p>in the recursion:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">==</span> <span class="s2">&quot;regular&quot;</span><span class="p">:</span>
   <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">],</span>
                      <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">!=</span> <span class="s2">&quot;regular&quot;</span><span class="p">:</span>
   <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                      <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="o">..</span> <span class="n">where</span> <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="ow">is</span> <span class="n">the</span> <span class="s2">&quot;joint score&quot;</span> <span class="n">of</span> <span class="n">the</span> <span class="n">pair</span> <span class="n">of</span> <span class="n">subsequences</span>
<span class="n">of</span> <span class="n">length</span> <span class="n">s</span> <span class="ow">and</span> <span class="n">t</span> <span class="n">respectively</span><span class="o">.</span>  <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="n">represents</span> <span class="n">the</span>
<span class="n">probability</span> <span class="n">of</span> <span class="n">extending</span> <span class="n">the</span> <span class="n">subsequences</span> <span class="n">of</span> <span class="n">length</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">)</span> <span class="n">by</span> <span class="n">one</span> <span class="ow">in</span>
<span class="n">the</span> <span class="n">s</span> <span class="n">direction</span><span class="p">,</span> <span class="n">given</span> <span class="n">the</span> <span class="n">particular</span> <span class="n">symbol</span><span class="p">,</span> <span class="ow">and</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>
<span class="n">represents</span> <span class="n">the</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">extending</span> <span class="n">the</span> <span class="n">subsequences</span> <span class="n">of</span> <span class="n">length</span>
<span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">)</span> <span class="n">by</span> <span class="n">one</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">t</span> <span class="n">direction</span><span class="p">,</span>
<span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span> <span class="n">of</span> <span class="n">emitting</span> <span class="n">the</span> <span class="n">termination</span><span class="o">/</span><span class="nb">next</span><span class="o">-</span><span class="n">frame</span> <span class="n">symbol</span><span class="o">.</span>

<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">==</span> <span class="s2">&quot;regular&quot;</span><span class="p">,</span> <span class="n">px</span><span class="p">[:,:,</span><span class="n">T</span><span class="p">]</span> <span class="n">equals</span> <span class="o">-</span><span class="n">infinity</span><span class="p">,</span> <span class="n">meaning</span> <span class="n">on</span> <span class="n">the</span>
<span class="s2">&quot;one-past-the-last&quot;</span> <span class="n">frame</span> <span class="n">we</span> <span class="n">cannot</span> <span class="n">emit</span> <span class="nb">any</span> <span class="n">symbols</span><span class="o">.</span>
<span class="n">This</span> <span class="ow">is</span> <span class="n">simply</span> <span class="n">a</span> <span class="n">way</span> <span class="n">of</span> <span class="n">incorporating</span>
<span class="n">the</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">the</span> <span class="n">termination</span> <span class="n">symbol</span> <span class="n">on</span> <span class="n">the</span> <span class="n">last</span> <span class="n">frame</span><span class="o">.</span>
</pre></div>
</div>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-rnnt-logprobs-joint">
<h2>get_rnnt_logprobs_joint<a class="headerlink" href="#get-rnnt-logprobs-joint" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_rnnt_logprobs_joint">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_rnnt_logprobs_joint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">termination_symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnnt_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'regular'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L341-L447"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_rnnt_logprobs_joint" title="Permalink to this definition"></a></dt>
<dd><p>Reduces RNN-T problem to a compact, standard form that can then be given
(with boundaries) to mutual_information_recursion().
This function is called from rnnt_loss().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The output of joiner network, with shape (B, T, S + 1, C),
i.e. batch, time_seq_len, symbol_seq_len+1, num_classes</p></li>
<li><p><strong>symbols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A LongTensor of shape [B][S], containing the symbols at each position
of the sequence.</p></li>
<li><p><strong>termination_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The identity of the termination symbol, must be in {0..C-1}</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a optional LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame] that is treated as
[0, 0, S, T]
if boundary is not supplied.
Most likely you will want begin_symbol and begin_frame to be zero.</p></li>
<li><p><strong>rnnt_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – <p>Specifies the type of rnnt paths: <cite>regular</cite>, <cite>modified</cite> or <cite>constrained</cite>.
<cite>regular</cite>: The regular rnnt that taking you to the next frame only if</p>
<blockquote>
<div><p>emitting a blank (i.e., emitting a symbol does not take you
to the next frame).</p>
</div></blockquote>
<dl class="simple">
<dt><cite>modified</cite>: A modified version of rnnt that will take you to the next</dt><dd><p>frame either emitting a blank or a non-blank symbol.</p>
</dd>
<dt><cite>constrained</cite>: A version likes the modified one that will go to the next</dt><dd><p>frame when you emit a non-blank symbol, but this is done
by “forcing” you to take the blank transition from the
<em>next</em> context on the <em>current</em> frame, e.g. if we emit
c given “a b” context, we are forced to emit “blank”
given “b c” context on the current frame.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>(px, py) (the names are quite arbitrary):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">px</span><span class="p">:</span> <span class="n">logprobs</span><span class="p">,</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">S</span><span class="p">][</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">rnnt_type</span> <span class="ow">is</span> <span class="n">regular</span><span class="p">,</span>
                       <span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">S</span><span class="p">][</span><span class="n">T</span><span class="p">]</span> <span class="k">if</span> <span class="n">rnnt_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">regular</span><span class="o">.</span>
<span class="n">py</span><span class="p">:</span> <span class="n">logprobs</span><span class="p">,</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">S</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="n">T</span><span class="p">]</span>
</pre></div>
</div>
<p>in the recursion:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">==</span> <span class="s2">&quot;regular&quot;</span><span class="p">:</span>
   <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">],</span>
                      <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">!=</span> <span class="s2">&quot;regular&quot;</span><span class="p">:</span>
   <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                      <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>length s and t respectively.  px[b][s][t] represents the probability of
extending the subsequences of length (s,t) by one in the s direction,
given the particular symbol, and py[b][s][t] represents the probability
of extending the subsequences of length (s,t) by one in the t direction,
i.e. of emitting the termination/next-frame symbol.</p>
<p>if <cite>rnnt_type == “regular”</cite>, px[:,:,T] equals -infinity, meaning on the
“one-past-the-last” frame we cannot emit any symbols.
This is simply a way of incorporating
the probability of the termination symbol on the last frame.</p>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-rnnt-logprobs-pruned">
<h2>get_rnnt_logprobs_pruned<a class="headerlink" href="#get-rnnt-logprobs-pruned" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_rnnt_logprobs_pruned">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_rnnt_logprobs_pruned</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">termination_symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnnt_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'regular'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L1180-L1348"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_rnnt_logprobs_pruned" title="Permalink to this definition"></a></dt>
<dd><p>Construct px, py for mutual_information_recursion with pruned output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The pruned output of joiner network, with shape (B, T, s_range, C)</p></li>
<li><p><strong>symbols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The symbol sequences, a LongTensor of shape [B][S], and elements in
{0..C-1}.</p></li>
<li><p><strong>ranges</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A tensor containing the symbol ids for each frame that we want to keep.
It is a LongTensor of shape <code class="docutils literal notranslate"><span class="pre">[B][T][s_range]</span></code>, where <code class="docutils literal notranslate"><span class="pre">ranges[b,t,0]</span></code>
contains the begin symbol <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">s</span> <span class="pre">&lt;=</span> <span class="pre">S</span> <span class="pre">-</span> <span class="pre">s_range</span> <span class="pre">+</span> <span class="pre">1</span></code>, such that
<code class="docutils literal notranslate"><span class="pre">logits[b,t,:,:]</span></code> represents the logits with positions
<code class="docutils literal notranslate"><span class="pre">s,</span> <span class="pre">s</span> <span class="pre">+</span> <span class="pre">1,</span> <span class="pre">...</span> <span class="pre">s</span> <span class="pre">+</span> <span class="pre">s_range</span> <span class="pre">-</span> <span class="pre">1</span></code>.
See docs in <a class="reference internal" href="#k2.get_rnnt_prune_ranges" title="k2.get_rnnt_prune_ranges"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_rnnt_prune_ranges()</span></code></a> for more details of what
ranges contains.</p></li>
<li><p><strong>termination_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the termination symbol, with 0 &lt;= termination_symbol &lt; C</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – a optional LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame] that is treated as
[0, 0, S, T]
if boundary is not supplied.
Most likely you will want begin_symbol and begin_frame to be zero.</p></li>
<li><p><strong>rnnt_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – <p>Specifies the type of rnnt paths: <cite>regular</cite>, <cite>modified</cite> or <cite>constrained</cite>.
<cite>regular</cite>: The regular rnnt that taking you to the next frame only if</p>
<blockquote>
<div><p>emitting a blank (i.e., emitting a symbol does not take you
to the next frame).</p>
</div></blockquote>
<dl class="simple">
<dt><cite>modified</cite>: A modified version of rnnt that will take you to the next</dt><dd><p>frame whether emitting a blank or a non-blank symbol.</p>
</dd>
<dt><cite>constrained</cite>: A version likes the modified one that will go to the next</dt><dd><p>frame when you emit a non-blank symbol, but this is done
by “forcing” you to take the blank transition from the
<em>next</em> context on the <em>current</em> frame, e.g. if we emit
c given “a b” context, we are forced to emit “blank”
given “b c” context on the current frame.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>(px, py) (the names are quite arbitrary):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">px</span><span class="p">:</span> <span class="n">logprobs</span><span class="p">,</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">S</span><span class="p">][</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">rnnt_type</span> <span class="ow">is</span> <span class="n">regular</span><span class="p">,</span>
                       <span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">S</span><span class="p">][</span><span class="n">T</span><span class="p">]</span> <span class="k">if</span> <span class="n">rnnt_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">regular</span><span class="o">.</span>
<span class="n">py</span><span class="p">:</span> <span class="n">logprobs</span><span class="p">,</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">S</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="n">T</span><span class="p">]</span>
</pre></div>
</div>
<p>in the recursion:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">==</span> <span class="s2">&quot;regular&quot;</span><span class="p">:</span>
   <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">],</span>
                      <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">!=</span> <span class="s2">&quot;regular&quot;</span><span class="p">:</span>
   <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                      <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>length s and t respectively.  px[b][s][t] represents the probability of
extending the subsequences of length (s,t) by one in the s direction,
given the particular symbol, and py[b][s][t] represents the probability
of extending the subsequences of length (s,t) by one in the t direction,
i.e. of emitting the termination/next-frame symbol.</p>
<p>if <cite>rnnt_type == “regular”</cite>, px[:,:,T] equals -infinity, meaning on the
“one-past-the-last” frame we cannot emit any symbols.
This is simply a way of incorporating
the probability of the termination symbol on the last frame.</p>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-rnnt-logprobs-smoothed">
<h2>get_rnnt_logprobs_smoothed<a class="headerlink" href="#get-rnnt-logprobs-smoothed" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_rnnt_logprobs_smoothed">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_rnnt_logprobs_smoothed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">am</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">termination_symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lm_only_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">am_only_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnnt_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'regular'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L1471-L1704"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_rnnt_logprobs_smoothed" title="Permalink to this definition"></a></dt>
<dd><p>Reduces RNN-T problem (the simple case, where joiner network is just
addition), to a compact, standard form that can then be given
(with boundaries) to mutual_information_recursion().
This version allows you to make the loss-function one of the form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lm_only_scale</span> <span class="o">*</span> <span class="n">lm_probs</span> <span class="o">+</span>
<span class="n">am_only_scale</span> <span class="o">*</span> <span class="n">am_probs</span> <span class="o">+</span>
<span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">lm_only_scale</span><span class="o">-</span><span class="n">am_only_scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">combined_probs</span>
</pre></div>
</div>
<p>where lm_probs and am_probs are the probabilities given the lm and acoustic
model independently.</p>
<p>This function is called from
<a class="reference internal" href="#k2.rnnt_loss_smoothed" title="k2.rnnt_loss_smoothed"><code class="xref py py-func docutils literal notranslate"><span class="pre">rnnt_loss_smoothed()</span></code></a>, but may be useful for other purposes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lm</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>Language model part of un-normalized logprobs of symbols, to be added to
acoustic model part before normalizing.  Of shape:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">S</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="n">C</span><span class="p">]</span>
</pre></div>
</div>
<p>where B is the batch size, S is the maximum sequence length of
the symbol sequence, possibly including the EOS symbol; and
C is size of the symbol vocabulary, including the termination/next-frame
symbol.
Conceptually, lm[b][s] is a vector of length [C] representing the
“language model” part of the un-normalized logprobs of symbols,
given all symbols <em>earlier than</em> s in the sequence.  The reason
we still need this for position S is that we may still be emitting
the termination/next-frame symbol at this point.</p>
</p></li>
<li><p><strong>am</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>Acoustic-model part of un-normalized logprobs of symbols, to be added
to language-model part before normalizing.  Of shape:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">B</span><span class="p">][</span><span class="n">T</span><span class="p">][</span><span class="n">C</span><span class="p">]</span>
</pre></div>
</div>
<p>where B is the batch size, T is the maximum sequence length of
the acoustic sequences (in frames); and C is size of the symbol
vocabulary, including the termination/next-frame symbol.  It reflects
the “acoustic” part of the probability of any given symbol appearing
next on this frame.</p>
</p></li>
<li><p><strong>symbols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A LongTensor of shape [B][S], containing the symbols at each position
of the sequence.</p></li>
<li><p><strong>termination_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The identity of the termination symbol, must be in {0..C-1}</p></li>
<li><p><strong>lm_only_scale</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – the scale on the “LM-only” part of the loss.</p></li>
<li><p><strong>am_only_scale</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – the scale on the “AM-only” part of the loss, for which we use
an “averaged” LM (averaged over all histories, so effectively unigram).</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a optional LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame] that is treated as
[0, 0, S, T]
if boundary is not supplied.
Most likely you will want begin_symbol and begin_frame to be zero.</p></li>
<li><p><strong>rnnt_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – <p>Specifies the type of rnnt paths: <cite>regular</cite>, <cite>modified</cite> or <cite>constrained</cite>.
<cite>regular</cite>: The regular rnnt that taking you to the next frame only if</p>
<blockquote>
<div><p>emitting a blank (i.e., emitting a symbol does not take you
to the next frame).</p>
</div></blockquote>
<dl class="simple">
<dt><cite>modified</cite>: A modified version of rnnt that will take you to the next</dt><dd><p>frame either emitting a blank or a non-blank symbol.</p>
</dd>
<dt><cite>constrained</cite>: A version likes the modified one that will go to the next</dt><dd><p>frame when you emit a non-blank symbol, but this is done
by “forcing” you to take the blank transition from the
<em>next</em> context on the <em>current</em> frame, e.g. if we emit
c given “a b” context, we are forced to emit “blank”
given “b c” context on the current frame.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><dl>
<dt>(px, py) (the names are quite arbitrary).</dt><dd><dl class="simple">
<dt>px: logprobs, of shape [B][S][T+1] if rnnt_type == “regular”,</dt><dd><p>[B][S][T] if rnnt_type != “regular”.</p>
</dd>
</dl>
<p>py: logprobs, of shape [B][S+1][T]</p>
</dd>
</dl>
<p>in the recursion:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">==</span> <span class="s2">&quot;regular&quot;</span><span class="p">:</span>
   <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">],</span>
                      <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">if</span> <span class="n">rnnt_type</span> <span class="o">!=</span> <span class="s2">&quot;regular&quot;</span><span class="p">:</span>
   <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                      <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="o">..</span> <span class="n">where</span> <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="ow">is</span> <span class="n">the</span> <span class="s2">&quot;joint score&quot;</span> <span class="n">of</span> <span class="n">the</span> <span class="n">pair</span> <span class="n">of</span> <span class="n">subsequences</span>
<span class="n">of</span> <span class="n">length</span> <span class="n">s</span> <span class="ow">and</span> <span class="n">t</span> <span class="n">respectively</span><span class="o">.</span>  <span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="n">represents</span> <span class="n">the</span>
<span class="n">probability</span> <span class="n">of</span> <span class="n">extending</span> <span class="n">the</span> <span class="n">subsequences</span> <span class="n">of</span> <span class="n">length</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">)</span> <span class="n">by</span> <span class="n">one</span> <span class="ow">in</span>
<span class="n">the</span> <span class="n">s</span> <span class="n">direction</span><span class="p">,</span> <span class="n">given</span> <span class="n">the</span> <span class="n">particular</span> <span class="n">symbol</span><span class="p">,</span> <span class="ow">and</span> <span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>
<span class="n">represents</span> <span class="n">the</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">extending</span> <span class="n">the</span> <span class="n">subsequences</span> <span class="n">of</span> <span class="n">length</span>
<span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">)</span> <span class="n">by</span> <span class="n">one</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">t</span> <span class="n">direction</span><span class="p">,</span>
<span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span> <span class="n">of</span> <span class="n">emitting</span> <span class="n">the</span> <span class="n">termination</span><span class="o">/</span><span class="nb">next</span><span class="o">-</span><span class="n">frame</span> <span class="n">symbol</span><span class="o">.</span>

<span class="n">px</span><span class="p">[:,:,</span><span class="n">T</span><span class="p">]</span> <span class="n">equals</span> <span class="o">-</span><span class="n">infinity</span><span class="p">,</span> <span class="n">meaning</span> <span class="n">on</span> <span class="n">the</span> <span class="s2">&quot;one-past-the-last&quot;</span> <span class="n">frame</span>
<span class="n">we</span> <span class="n">cannot</span> <span class="n">emit</span> <span class="nb">any</span> <span class="n">symbols</span><span class="o">.</span>  <span class="n">This</span> <span class="ow">is</span> <span class="n">simply</span> <span class="n">a</span> <span class="n">way</span> <span class="n">of</span> <span class="n">incorporating</span>
<span class="n">the</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">the</span> <span class="n">termination</span> <span class="n">symbol</span> <span class="n">on</span> <span class="n">the</span> <span class="n">last</span> <span class="n">frame</span><span class="o">.</span>
</pre></div>
</div>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-rnnt-prune-ranges">
<h2>get_rnnt_prune_ranges<a class="headerlink" href="#get-rnnt-prune-ranges" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_rnnt_prune_ranges">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_rnnt_prune_ranges</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">px_grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">py_grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s_range</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L636-L780"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_rnnt_prune_ranges" title="Permalink to this definition"></a></dt>
<dd><p>Get the pruning ranges of normal rnnt loss according to the grads
of px and py returned by mutual_information_recursion.</p>
<p>For each sequence with T frames, we will generate a tensor with the shape of
(T, s_range) containing the information that which symbols will be token
into consideration for each frame. For example, here is a sequence with 10
frames and the corresponding symbols are <cite>[A B C D E F]</cite>, if the s_range
equals 3, one possible ranges tensor will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>
</pre></div>
</div>
<p>which means we only consider <cite>[A B C]</cite> at frame 0, 1, 2, 3, and <cite>[B C D]</cite>
at frame 4, 5, 6, <cite>[D E F]</cite> at frame 7, 8, 9.</p>
<p>We can only consider limited number of symbols because frames and symbols
are monotonic aligned, theoretically it can only generate particular range
of symbols given a particular frame.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the generated tensor ranges (assuming batch size is 1), ranges[:, 0]
is a monotonic increasing tensor from 0 to <cite>len(symbols) - s_range</cite> and
it satisfies <cite>ranges[t+1, 0] - ranges[t, 0] &lt; s_range</cite> which means we
won’t skip any symbols.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>px_grad</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The gradient of px, see docs in <cite>mutual_information_recursion</cite> for more
details of px.</p></li>
<li><p><strong>py_grad</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The gradient of py, see docs in <cite>mutual_information_recursion</cite> for more
details of py.</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – a LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame]</p></li>
<li><p><strong>s_range</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – How many symbols to keep for each frame.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A tensor with the shape of (B, T, s_range) containing the indexes of the
kept symbols for each frame.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-rnnt-prune-ranges-deprecated">
<h2>get_rnnt_prune_ranges_deprecated<a class="headerlink" href="#get-rnnt-prune-ranges-deprecated" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.get_rnnt_prune_ranges_deprecated">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">get_rnnt_prune_ranges_deprecated</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">px_grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">py_grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s_range</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L786-L910"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.get_rnnt_prune_ranges_deprecated" title="Permalink to this definition"></a></dt>
<dd><p>Get the pruning ranges of normal rnnt loss according to the grads
of px and py returned by mutual_information_recursion.</p>
<p>For each sequence with T frames, we will generate a tensor with the shape of
(T, s_range) containing the information that which symbols will be token
into consideration for each frame. For example, here is a sequence with 10
frames and the corresponding symbols are <cite>[A B C D E F]</cite>, if the s_range
equals 3, one possible ranges tensor will be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>
</pre></div>
</div>
<p>which means we only consider <cite>[A B C]</cite> at frame 0, 1, 2, 3, and <cite>[B C D]</cite>
at frame 4, 5, 6, <cite>[D E F]</cite> at frame 7, 8, 9.</p>
<p>We can only consider limited number of symbols because frames and symbols
are monotonic aligned, theoretically it can only generate particular range
of symbols given a particular frame.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the generated tensor ranges (assuming batch size is 1), ranges[:, 0]
is a monotonic increasing tensor from 0 to <cite>len(symbols) - s_range</cite> and
it satisfies <cite>ranges[t+1, 0] - ranges[t, 0] &lt; s_range</cite> which means we
won’t skip any symbols.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>px_grad</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The gradient of px, see docs in <cite>mutual_information_recursion</cite> for more
details of px.</p></li>
<li><p><strong>py_grad</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The gradient of py, see docs in <cite>mutual_information_recursion</cite> for more
details of py.</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – a LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame]</p></li>
<li><p><strong>s_range</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – How many symbols to keep for each frame.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A tensor with the shape of (B, T, s_range) containing the indexes of the
kept symbols for each frame.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="index-add">
<h2>index_add<a class="headerlink" href="#index-add" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.index_add">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">index_add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_out</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/ops.py#L121-L156"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.index_add" title="Permalink to this definition"></a></dt>
<dd><p>It implements in_out[index[i]] += value[i].</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It has similar semantics with <cite>torch.Tensor.index_add_</cite> except
that:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>index.dtype == torch.int32</cite></p></li>
<li><p><cite>-1 &lt;= index[i] &lt; in_out.shape[0]</cite></p></li>
<li><p><cite>index[i] == -1</cite> is ignored.</p></li>
<li><p><cite>index</cite> has to be a 1-D <strong>contiguous</strong> tensor.</p></li>
</ul>
</div></blockquote>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><cite>in_out</cite> is modified <strong>in-place</strong>.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This functions does NOT support autograd.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A 1-D <strong>contiguous</strong> tensor with dtype <cite>torch.int32</cite>.
Must satisfy <cite>-1 &lt;= index[i] &lt; in_out.shape[0]</cite></p></li>
<li><p><strong>value</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A 1-D or 2-D tensor with dtype <cite>torch.int32</cite>, <cite>torch.float32</cite>,
or <cite>torch.float64</cite>.
Must satisfy <cite>index.shape[0] == value.shape[0]</cite></p></li>
<li><p><strong>in_out</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A 1-D or 2-D tensor with the same dtype as <cite>value</cite>. It satisfies
<cite>in_out.shape[1] == value.shape[1]</cite> if it is a 2-D tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return None.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="index-fsa">
<h2>index_fsa<a class="headerlink" href="#index-fsa" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.index_fsa">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">index_fsa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indexes</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/ops.py#L159-L195"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.index_fsa" title="Permalink to this definition"></a></dt>
<dd><p>Select a list of FSAs from <cite>src</cite> with a 1-D tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec.</p></li>
<li><p><strong>indexes</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A 1-D <cite>torch.Tensor</cite> of dtype <cite>torch.int32</cite> containing
the ids of FSAs to select.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return an FsaVec containing only those FSAs specified by <cite>indexes</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="index-select">
<h2>index_select<a class="headerlink" href="#index-select" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.index_select">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">index_select</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/ops.py#L83-L118"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.index_select" title="Permalink to this definition"></a></dt>
<dd><p>Returns a new tensor which indexes the input tensor along dimension 0
using the entries in <cite>index</cite>.</p>
<p>If the entry in <cite>index</cite> is -1, then the corresponding entry in the
returned tensor is 0.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><cite>index.dtype == torch.int32</cite> and <cite>index.ndim == 1</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The input tensor. Either 1-D or 2-D with dtype <cite>torch.int32</cite>,
<cite>torch.int64</cite>, <cite>torch.float32</cite>, or <cite>torch.float64</cite>.</p></li>
<li><p><strong>index</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – 1-D tensor of dtype <cite>torch.int32</cite> containing the indexes.
If an entry is -1, the corresponding entry in the returned value
is 0. The elements of <cite>index</cite> should be in the range
<cite>[-1..src.shape[0]-1]</cite>.</p></li>
<li><p><strong>default_value</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Used only when <cite>src</cite> is a 1-D tensor. It sets ans[i] to default_value
if index[i] is -1.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A tensor with shape <code class="docutils literal notranslate"><span class="pre">(index.numel(),</span> <span class="pre">*src.shape[1:])</span></code> and dtype the
same as <cite>src</cite>, e.g. if <cite>src.ndim == 1</cite>, <cite>ans.shape</cite> would be
<cite>(index.shape[0],)</cite>; if <cite>src.ndim == 2</cite>, <cite>ans.shape</cite> would be
<cite>(index.shape[0], src.shape[1])</cite>.
Will satisfy <cite>ans[i] == src[index[i]]</cite> if <cite>src.ndim == 1</cite>,
or <cite>ans[i, j] == src[index[i], j]</cite> if <cite>src.ndim == 2</cite>, except for
entries where <cite>index[i] == -1</cite> which will be zero.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="intersect">
<h2>intersect<a class="headerlink" href="#intersect" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.intersect">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">intersect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a_fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b_fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">treat_epsilons_specially</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ret_arc_maps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L331-L413"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.intersect" title="Permalink to this definition"></a></dt>
<dd><p>Compute the intersection of two FSAs.</p>
<p>When <cite>treat_epsilons_specially</cite> is True, this function works only on CPU.
When <cite>treat_epsilons_specially</cite> is False and both <cite>a_fsa</cite> and <cite>b_fsa</cite>
are on GPU, then this function works on GPU; in this case, the two
input FSAs do not need to be arc sorted.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a_fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The first input FSA. It can be either a single FSA or an FsaVec.</p></li>
<li><p><strong>b_fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The second input FSA. it can be either a single FSA or an FsaVec.
If both a_fsa and b_fsa are FsaVec, they must contain the same
number of FSAs.</p></li>
<li><p><strong>treat_epsilons_specially</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True, epsilons will be treated as epsilon, meaning epsilon arcs can
match with an implicit epsilon self-loop.
If False, epsilons will be treated as real, normal symbols (to have
them treated as epsilons in this case you may have to add epsilon
self-loops to whichever of the inputs is naturally epsilon-free).</p></li>
<li><p><strong>ret_arc_maps</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <p>If False, return the resulting Fsa. If True, return a tuple
containing three entries:</p>
<blockquote>
<div><ul>
<li><p>the resulting Fsa</p></li>
<li><p>a_arc_map, a 1-D torch.Tensor with dtype torch.int32.
a_arc_map[i] is the arc index in a_fsa that corresponds
to the i-th arc in the resulting Fsa. a_arc_map[i] is -1
if the i-th arc in the resulting Fsa has no corresponding
arc in a_fsa.</p></li>
<li><p>b_arc_map, a 1-D torch.Tensor with dtype torch.int32.
b_arc_map[i] is the arc index in b_fsa that corresponds
to the i-th arc in the resulting Fsa. b_arc_map[i] is -1
if the i-th arc in the resulting Fsa has no corresponding
arc in b_fsa.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two input FSAs MUST be arc sorted if <cite>treat_epsilons_specially</cite>
is True.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The rules for assigning the attributes of the output Fsa are as follows:</p>
<ul class="simple">
<li><p>(1) For attributes where only one source (a_fsa or b_fsa) has that
attribute: Copy via arc_map, or use zero if arc_map has -1. This rule
works for both floating point and integer attributes.</p></li>
<li><p>(2) For attributes where both sources (a_fsa and b_fsa) have that
attribute: For floating point attributes: sum via arc_maps, or use zero
if arc_map has -1. For integer attributes, it’s not supported for now
(the attributes will be discarded and will not be kept in the output
FSA).</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If ret_arc_maps is False, return the result of intersecting a_fsa and
b_fsa. len(out_fsa.shape) is 2 if and only if the two input FSAs are
single FSAs; otherwise, len(out_fsa.shape) is 3.
If ret_arc_maps is True, it returns additionally two arc_maps:
a_arc_map and b_arc_map.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="intersect-dense">
<h2>intersect_dense<a class="headerlink" href="#intersect-dense" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.intersect_dense">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">intersect_dense</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a_fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b_fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_beam</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_arcs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1073741824</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">a_to_b_map</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seqframe_idx_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame_idx_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/autograd.py#L751-L824"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.intersect_dense" title="Permalink to this definition"></a></dt>
<dd><p>Intersect array of FSAs on CPU/GPU.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><cite>a_fsas</cite> MUST be arc sorted.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a_fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – Input FsaVec, i.e., <cite>decoding graphs</cite>, one per sequence. It might just
be a linear sequence of phones, or might be something more complicated.
Must have <cite>a_fsas.shape[0] == b_fsas.dim0()</cite> if <cite>a_to_b_map</cite> is None.
Otherwise, must have <cite>a_fsas.shape[0] == a_to_b_map.shape[0]</cite></p></li>
<li><p><strong>b_fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code>) – Input FSAs that correspond to neural network output.</p></li>
<li><p><strong>output_beam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Beam to prune output, similar to lattice-beam in Kaldi.  Relative
to best path of output.</p></li>
<li><p><strong>max_states</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The max number of states to prune the output, mainly to avoid
out-of-memory and numerical overflow, default 15,000,000.</p></li>
<li><p><strong>max_arcs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The max number of arcs to prune the output, mainly to avoid
out-of-memory and numerical overflow, default 1073741824(2^30).</p></li>
<li><p><strong>a_to_b_map</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – Maps from FSA-index in a to FSA-index in b to use for it.
If None, then we expect the number of FSAs in a_fsas to equal
b_fsas.dim0().  If set, then it should be a Tensor with ndim=1
and dtype=torch.int32, with a_to_b_map.shape[0] equal to the
number of FSAs in a_fsas (i.e. a_fsas.shape[0] if
len(a_fsas.shape) == 3, else 1); and elements 0 &lt;= i &lt; b_fsas.dim0().</p></li>
<li><p><strong>seqframe_idx_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – If set (e.g. to ‘seqframe’), an attribute in the output will be created
that encodes the sequence-index and the frame-index within that
sequence; this is equivalent to a row-index into b_fsas.values,
or, equivalently, an element in b_fsas.shape.</p></li>
<li><p><strong>frame_idx_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – If set (e.g. to ‘frame’, an attribute in the output will be created
that contains the frame-index within the corresponding sequence.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The result of the intersection (pruned to <cite>output_beam</cite>; this pruning
is exact, it uses forward and backward scores.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="intersect-dense-pruned">
<h2>intersect_dense_pruned<a class="headerlink" href="#intersect-dense-pruned" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.intersect_dense_pruned">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">intersect_dense_pruned</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a_fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b_fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search_beam</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_beam</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_active_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_active_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seqframe_idx_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame_idx_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_partial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/autograd.py#L667-L748"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.intersect_dense_pruned" title="Permalink to this definition"></a></dt>
<dd><p>Intersect array of FSAs on CPU/GPU.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><cite>a_fsas</cite> MUST be arc sorted.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a_fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – Input FsaVec, i.e., <cite>decoding graphs</cite>, one per sequence. It might just
be a linear sequence of phones, or might be something more complicated.
Must have either <cite>a_fsas.shape[0] == b_fsas.dim0()</cite>, or
<cite>a_fsas.shape[0] == 1</cite> in which case the graph is shared.</p></li>
<li><p><strong>b_fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code>) – Input FSAs that correspond to neural network output.</p></li>
<li><p><strong>search_beam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Decoding beam, e.g. 20.  Smaller is faster, larger is more exact
(less pruning). This is the default value; it may be modified by
<cite>min_active_states</cite> and <cite>max_active_states</cite>.</p></li>
<li><p><strong>output_beam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Beam to prune output, similar to lattice-beam in Kaldi.  Relative
to best path of output.</p></li>
<li><p><strong>min_active_states</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Minimum number of FSA states that are allowed to be active on any given
frame for any given intersection/composition task. This is advisory,
in that it will try not to have fewer than this number active.
Set it to zero if there is no constraint.</p></li>
<li><p><strong>max_active_states</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Maximum number of FSA states that are allowed to be active on any given
frame for any given intersection/composition task. This is advisory,
in that it will try not to exceed that but may not always succeed.
You can use a very large number if no constraint is needed.</p></li>
<li><p><strong>active</strong> (<em>allow_partial If true and there was no final state</em>) – we will treat all the states on the
last frame to be final state. If false, we only
care about the real final state in the decoding
graph on the last frame when generating lattice.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>:param<span class="classifier">we will treat all the states on the</span></dt><dd><p>last frame to be final state. If false, we only
care about the real final state in the decoding
graph on the last frame when generating lattice.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seqframe_idx_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – If set (e.g. to ‘seqframe’), an attribute in the output will be created
that encodes the sequence-index and the frame-index within that
sequence; this is equivalent to a row-index into b_fsas.values,
or, equivalently, an element in b_fsas.shape.</p></li>
<li><p><strong>frame_idx_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – If set (e.g. to ‘frame’, an attribute in the output will be created
that contains the frame-index within the corresponding sequence.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The result of the intersection.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="intersect-device">
<h2>intersect_device<a class="headerlink" href="#intersect-device" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.intersect_device">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">intersect_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a_fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b_fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b_to_a_map</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sorted_match_a</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ret_arc_maps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L250-L328"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.intersect_device" title="Permalink to this definition"></a></dt>
<dd><p>Compute the intersection of two FsaVecs treating epsilons
as real, normal symbols.</p>
<p>This function supports both CPU and GPU. But it is very slow on CPU.
That’s why this function name ends with <cite>_device</cite>. It is intended for GPU.
See <a class="reference internal" href="#k2.intersect" title="k2.intersect"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.intersect()</span></code></a> which is a more general interface
(it will call the same underlying code, IntersectDevice(), if
the inputs are on GPU and a_fsas is arc-sorted).</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Epsilons are treated as real, normal symbols.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The two inputs do not need to be arc-sorted.</p>
</div>
<p>Refer to <a class="reference internal" href="#k2.intersect" title="k2.intersect"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.intersect()</span></code></a> for how we assign the attributes of the
output FsaVec.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a_fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec (must have 3 axes, i.e., <cite>len(a_fsas.shape) == 3</cite>.</p></li>
<li><p><strong>b_fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec (must have 3 axes) on the same device as <cite>a_fsas</cite>.</p></li>
<li><p><strong>b_to_a_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>A 1-D torch.Tensor with dtype torch.int32 on the same device
as <cite>a_fsas</cite>. Map from FSA-id in <cite>b_fsas</cite> to the corresponding
FSA-id in <cite>a_fsas</cite> that we want to compose it with.
E.g. might be an identity map, or all-to-zero, or something the
user chooses.</p>
<dl class="simple">
<dt>Requires</dt><dd><ul>
<li><p><cite>b_to_a_map.shape[0] == b_fsas.shape[0]</cite></p></li>
<li><p><cite>0 &lt;= b_to_a_map[i] &lt; a_fsas.shape[0]</cite></p></li>
</ul>
</dd>
</dl>
</p></li>
<li><p><strong>sorted_match_a</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true, the arcs of a_fsas must be sorted by label (checked by
calling code via properties), and we’ll use a matching approach
that requires this.</p></li>
<li><p><strong>ret_arc_maps</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <p>If False, return the resulting Fsa. If True, return a tuple
containing three entries:</p>
<blockquote>
<div><ul>
<li><p>the resulting Fsa</p></li>
<li><p>a_arc_map, a 1-D torch.Tensor with dtype torch.int32.
a_arc_map[i] is the arc index in a_fsas that corresponds
to the i-th arc in the resulting Fsa. a_arc_map[i] is -1
if the i-th arc in the resulting Fsa has no corresponding
arc in a_fsas.</p></li>
<li><p>b_arc_map, a 1-D torch.Tensor with dtype torch.int32.
b_arc_map[i] is the arc index in b_fsas that corresponds
to the i-th arc in the resulting Fsa. b_arc_map[i] is -1
if the i-th arc in the resulting Fsa has no corresponding
arc in b_fsas.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If ret_arc_maps is False, return intersected FsaVec;
will satisfy <cite>ans.shape == b_fsas.shape</cite>.
If ret_arc_maps is True, it returns additionally two arc maps:
a_arc_map and b_arc_map.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="invert">
<h2>invert<a class="headerlink" href="#invert" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.invert">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">invert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ret_arc_map</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L886-L921"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.invert" title="Permalink to this definition"></a></dt>
<dd><p>Invert an FST, swapping the labels in the FSA with the auxiliary labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA. It can be either a single FSA or an FsaVec.</p></li>
<li><p><strong>ret_arc_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to return an extra arc map, which is a 1-D tensor with dtype
torch.int32. The returned arc_map[i] is the arc index in the input
fsa that corresponds to the i-th arc in the returned fsa. arc_map[i]
is -1 if the i-th arc in the returned fsa has no counterpart in the
input fsa.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If ret_arc_map is False, return the inverted Fsa, it’s top-sorted if
<cite>fsa</cite> is top-sorted.
If ret_arc_map is True, return an extra arc map.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="is-rand-equivalent">
<h2>is_rand_equivalent<a class="headerlink" href="#is-rand-equivalent" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.is_rand_equivalent">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">is_rand_equivalent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">treat_epsilons_specially</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">npath</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L309-L358"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.is_rand_equivalent" title="Permalink to this definition"></a></dt>
<dd><p>Check if the Fsa <cite>a</cite> appears to be equivalent to <cite>b</cite> by
randomly checking some symbol sequences in them.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It works only on CPU.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – One of the input FSA. It can be either a single FSA or an FsaVec.
Must be top-sorted and on CPU.</p></li>
<li><p><strong>b</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The other input FSA. It must have the same NumAxes() as a.
Must be top-sorted and on CPU.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – The semiring to be used for all weight measurements;
if false then we use ‘max’ on alternative paths; if
true we use ‘log-add’.</p></li>
<li><p><strong>beam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – beam &gt; 0 that affects pruning; the algorithm will only check
paths within <cite>beam</cite> of the total score of the lattice (for
tropical semiring, it’s max weight over all paths from start
state to final state; for log semiring, it’s log-sum probs over
all paths) in <cite>a</cite> or <cite>b</cite>.</p></li>
<li><p><strong>treat_epsilons_specially</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – We’ll do <cite>intersection</cite> between generated path and a or b when
check equivalence. Generally, if it’s true, we will treat
epsilons as epsilon when doing intersection; Otherwise, epsilons
will just be treated as any other symbol.</p></li>
<li><p><strong>delta</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Tolerance for path weights to check the equivalence.
If abs(weights_a, weights_b) &lt;= delta, we say the two
paths are equivalent.</p></li>
<li><p><strong>npath</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of paths will be generated to check the
equivalence of <cite>a</cite> and <cite>b</cite></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>True if the Fsa <cite>a</cite> appears to be equivalent to <cite>b</cite> by randomly
generating <cite>npath</cite> paths from one of them and then checking if the symbol
sequence exists in the other one and if the total weight for that symbol
sequence is the same in both FSAs.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="joint-mutual-information-recursion">
<h2>joint_mutual_information_recursion<a class="headerlink" href="#joint-mutual-information-recursion" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.joint_mutual_information_recursion">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">joint_mutual_information_recursion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">px</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">py</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/mutual_information.py#L314-L423"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.joint_mutual_information_recursion" title="Permalink to this definition"></a></dt>
<dd><p>A recursion that is useful for modifications of RNN-T and similar loss
functions, where the recursion probabilities have a number of terms and you
want them reported separately.  See mutual_information_recursion() for more
documentation of the basic aspects of this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>px</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a sequence of Tensors, each of the same shape [B][S][T+1]</p></li>
<li><p><strong>py</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a sequence of Tensor, each of the same shape [B][S+1][T],
the sequence must be the same length as px.</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – optionally, a LongTensor of shape [B][4] containing rows
[s_begin, t_begin, s_end, t_end], with 0 &lt;= s_begin &lt;= s_end &lt;= S
and 0 &lt;= t_begin &lt;= t_end &lt; T, defaulting to [0, 0, S, T].
These are the beginning and one-past-the-last positions in the x
and y sequences respectively, and can be used if not all
sequences are of the same length.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>a Tensor of shape (len(px), B),
whose sum over dim 0 is the total log-prob of the recursion mentioned
below, per sequence. The first element of the sequence of length len(px)
is “special”, in that it has an offset term reflecting the difference
between sum-of-log and log-of-sum; for more interpretable loss values,
the “main” part of your loss function should be first.</p>
<p>The recursion below applies if boundary == None, when it defaults
to (0, 0, S, T); where px_sum, py_sum are the sums of the elements of px
and py:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">tensor</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">containing</span> <span class="o">-</span><span class="n">infinity</span>
<span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="c1"># do the following in loop over s and t:</span>
<span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_add</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">px_sum</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">],</span>
                    <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">py_sum</span><span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">(</span><span class="k">if</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="k">return</span> <span class="n">b</span><span class="p">[:][</span><span class="n">S</span><span class="p">][</span><span class="n">T</span><span class="p">]</span>
</pre></div>
</div>
</p>
</dd>
</dl>
<p>This function lets you implement the above recursion efficiently, except
that it gives you a breakdown of the contribution from all the elements of
px and py separately.  As noted above, the first element of the
sequence is “special”.</p>
</dd></dl>

</section>
<section id="levenshtein-alignment">
<h2>levenshtein_alignment<a class="headerlink" href="#levenshtein-alignment" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.levenshtein_alignment">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">levenshtein_alignment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">refs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hyps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hyp_to_ref_map</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sorted_match_ref</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L1378-L1453"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.levenshtein_alignment" title="Permalink to this definition"></a></dt>
<dd><p>Get the levenshtein alignment of two FsaVecs</p>
<p>This function supports both CPU and GPU. But it is very slow on CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>refs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec (must have 3 axes, i.e., <cite>len(refs.shape) == 3</cite>. It is the
output Fsa of the <a class="reference internal" href="#k2.levenshtein_graph" title="k2.levenshtein_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">levenshtein_graph()</span></code></a>.</p></li>
<li><p><strong>hyps</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec (must have 3 axes) on the same device as <cite>refs</cite>. It is the
output Fsa of the <a class="reference internal" href="#k2.levenshtein_graph" title="k2.levenshtein_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">levenshtein_graph()</span></code></a>.</p></li>
<li><p><strong>hyp_to_ref_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>A 1-D torch.Tensor with dtype torch.int32 on the same device
as <cite>refs</cite>. Map from FSA-id in <cite>hpys</cite> to the corresponding
FSA-id in <cite>refs</cite> that we want to get levenshtein alignment with.
E.g. might be an identity map, or all-to-zero, or something the
user chooses.</p>
<dl class="simple">
<dt>Requires</dt><dd><ul>
<li><p><cite>hyp_to_ref_map.shape[0] == hyps.shape[0]</cite></p></li>
<li><p><cite>0 &lt;= hyp_to_ref_map[i] &lt; refs.shape[0]</cite></p></li>
</ul>
</dd>
</dl>
</p></li>
<li><p><strong>sorted_match_ref</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true, the arcs of refs must be sorted by label (checked by
calling code via properties), and we’ll use a matching approach
that requires this.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Returns an FsaVec containing the alignment information and satisfing
<cite>ans.Dim0() == hyps.Dim0()</cite>. Two attributes named <cite>ref_labels</cite> and
<cite>hyp_labels</cite> will be added to the returned FsaVec. <cite>ref_labels</cite> contains
the aligned sequences of refs and <cite>hyp_labels</cite> contains the aligned
sequences of hyps. You can get the levenshtein distance by calling
<cite>get_tot_scores</cite> on the returned FsaVec.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hyps</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">levenshtein_graph</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">refs</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">levenshtein_graph</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alignment</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">levenshtein_alignment</span><span class="p">(</span>
<span class="go">        refs, hyps,</span>
<span class="go">        hyp_to_ref_map=torch.tensor([0, 0], dtype=torch.int32),</span>
<span class="go">        sorted_match_ref=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alignment</span><span class="o">.</span><span class="n">labels</span>
<span class="go">tensor([ 1,  2,  0, -1,  1,  0,  0,  0, -1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alignment</span><span class="o">.</span><span class="n">ref_labels</span>
<span class="go">tensor([ 1,  2,  4, -1,  1,  2,  4,  0, -1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alignment</span><span class="o">.</span><span class="n">hyp_labels</span>
<span class="go">tensor([ 1,  2,  3, -1,  1,  3,  3,  2, -1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="n">alignment</span><span class="o">.</span><span class="n">get_tot_scores</span><span class="p">(</span>
<span class="go">        use_double_scores=False, log_semiring=False))</span>
<span class="go">tensor([1., 3.])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="levenshtein-graph">
<h2>levenshtein_graph<a class="headerlink" href="#levenshtein-graph" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.levenshtein_graph">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">levenshtein_graph</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ins_del_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">0.501</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L1333-L1375"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.levenshtein_graph" title="Permalink to this definition"></a></dt>
<dd><p>Construct levenshtein graphs from symbols.</p>
<p>See <a class="reference external" href="https://github.com/k2-fsa/k2/pull/828">https://github.com/k2-fsa/k2/pull/828</a> for more details about levenshtein
graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>symbols</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]) – <p>It can be one of the following types:</p>
<blockquote>
<div><ul>
<li><p>A list of list-of-integers, e..g, <cite>[ [1, 2], [1, 2, 3] ]</cite></p></li>
<li><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>.
Must have <cite>num_axes == 2</cite> and with dtype <cite>torch.int32</cite>.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>ins_del_score</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – The score on the self loops arcs in the graphs, the main idea of this
score is to set insertion and deletion penalty, which will affect the
shortest path searching produre.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Optional. It can be either a string (e.g., ‘cpu’, ‘cuda:0’) or a
torch.device.
By default, the returned FSA is on CPU.
If <cite>symbols</cite> is an instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>, the returned
FSA will on the same device as <cite>k2.RaggedTensor</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An FsaVec containing the returned levenshtein graphs, with “Dim0()”
the same as “len(symbols)”(List[List[int]]) or “dim0”(k2.RaggedTensor).</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="linear-fsa">
<h2>linear_fsa<a class="headerlink" href="#linear-fsa" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.linear_fsa">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">linear_fsa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L37-L68"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.linear_fsa" title="Permalink to this definition"></a></dt>
<dd><p>Construct an linear FSA from labels.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The scores of arcs in the returned FSA are all 0.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>]) – <p>It can be one of the following types:</p>
<blockquote>
<div><ul>
<li><p>A list of integers, e.g., <cite>[1, 2, 3]</cite></p></li>
<li><p>A list of list-of-integers, e..g, <cite>[ [1, 2], [1, 2, 3] ]</cite></p></li>
<li><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>.
Must have <cite>num_axes == 2</cite>.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Optional. It can be either a string (e.g., ‘cpu’, ‘cuda:0’) or a
torch.device.
If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, then the returned FSA is on CPU. It has to be None
if <code class="docutils literal notranslate"><span class="pre">labels</span></code> is an instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a list of integers, return an FSA</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a list of list-of-integers, return an FsaVec</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is an instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>, return
an FsaVec</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="linear-fsa-with-self-loops">
<h2>linear_fsa_with_self_loops<a class="headerlink" href="#linear-fsa-with-self-loops" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.linear_fsa_with_self_loops">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">linear_fsa_with_self_loops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsas</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L71-L99"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.linear_fsa_with_self_loops" title="Permalink to this definition"></a></dt>
<dd><p>Create a linear FSA with epsilon self-loops by first removing epsilon
transitions from the input linear FSA.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FSA or an FsaVec. It MUST be a linear FSA or a vector of linear FSAs.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return an FSA or FsaVec, where each FSA contains epsilon self-loops but
contains no epsilon transitions for arcs that are not self-loops.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="linear-fst">
<h2>linear_fst<a class="headerlink" href="#linear-fst" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.linear_fst">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">linear_fst</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aux_labels</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L102-L131"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.linear_fst" title="Permalink to this definition"></a></dt>
<dd><p>Construct a linear FST from labels and its corresponding
auxiliary labels.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The scores of arcs in the returned FST are all 0.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]) – A list of integers or a list of list of integers.</p></li>
<li><p><strong>aux_labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]) – A list of integers or a list of list of integers.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An FST if the labels is a list of integers.
A vector of FSTs (FsaVec) if the input is a list of list of integers.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="linear-fst-with-self-loops">
<h2>linear_fst_with_self_loops<a class="headerlink" href="#linear-fst-with-self-loops" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.linear_fst_with_self_loops">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">linear_fst_with_self_loops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsts</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L134-L226"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.linear_fst_with_self_loops" title="Permalink to this definition"></a></dt>
<dd><p>Create a linear FST with epsilon self-loops by first removing epsilon
transitions from the input linear FST.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The main difference to <a class="reference internal" href="#k2.linear_fsa_with_self_loops" title="k2.linear_fsa_with_self_loops"><code class="xref py py-func docutils literal notranslate"><span class="pre">linear_fsa_with_self_loops()</span></code></a> is that
aux_labels and scores are also kept here.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsas</strong> – An FST or an FstVec. It MUST be a linear FST or a vector of linear FSTs.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return an FST or FstVec, where each FST contains epsilon self-loops but
contains no epsilon transitions for arcs that are not self-loops.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="mutual-information-recursion">
<h2>mutual_information_recursion<a class="headerlink" href="#mutual-information-recursion" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.mutual_information_recursion">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">mutual_information_recursion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">px</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">py</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/mutual_information.py#L185-L298"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.mutual_information_recursion" title="Permalink to this definition"></a></dt>
<dd><p>A recursion that is useful in computing mutual information between two
sequences of real vectors, but may be useful more generally in
sequence-to-sequence tasks where monotonic alignment between pairs of
sequences is desired.  The definitions of the arguments are definitions that
would be used when computing this type of mutual information, but you can
also view them as arbitrary quantities and just make use of the formula
computed by this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>px</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>A torch.Tensor of some floating point type, with shape <code class="docutils literal notranslate"><span class="pre">[B][S][T+1]</span></code>,
where <code class="docutils literal notranslate"><span class="pre">B</span></code> is the batch size, <code class="docutils literal notranslate"><span class="pre">S</span></code> is the length of the <code class="docutils literal notranslate"><span class="pre">x</span></code> sequence
(including representations of <code class="docutils literal notranslate"><span class="pre">EOS</span></code> symbols but not <code class="docutils literal notranslate"><span class="pre">BOS</span></code> symbols),
and <code class="docutils literal notranslate"><span class="pre">T</span></code> is the length of the <code class="docutils literal notranslate"><span class="pre">y</span></code> sequence (including representations
of <code class="docutils literal notranslate"><span class="pre">EOS</span></code> symbols but not <code class="docutils literal notranslate"><span class="pre">BOS</span></code> symbols).  In the mutual information
application, <code class="docutils literal notranslate"><span class="pre">px[b][s][t]</span></code> would represent the following log odds
ratio; ignoring the b index on the right to make the notation more
compact:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">px</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span>  <span class="n">log</span> <span class="p">[</span> <span class="n">p</span><span class="p">(</span><span class="n">x_s</span> <span class="o">|</span> <span class="n">x_</span><span class="p">{</span><span class="mf">0.</span><span class="o">.</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">},</span> <span class="n">y_</span><span class="p">{</span><span class="mf">0.</span><span class="o">.</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">})</span> <span class="o">/</span> <span class="n">p</span><span class="p">(</span><span class="n">x_s</span><span class="p">)</span> <span class="p">]</span>
</pre></div>
</div>
<p>This expression also implicitly includes the log-probability of
choosing to generate an <code class="docutils literal notranslate"><span class="pre">x</span></code> value as opposed to a <code class="docutils literal notranslate"><span class="pre">y</span></code> value.  In
practice it might be computed as <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span></code>, where <code class="docutils literal notranslate"><span class="pre">a</span></code> is the log
probability of choosing to extend the sequence of length <code class="docutils literal notranslate"><span class="pre">(s,t)</span></code>
with an <code class="docutils literal notranslate"><span class="pre">x</span></code> as opposed to a <code class="docutils literal notranslate"><span class="pre">y</span></code> value; and <code class="docutils literal notranslate"><span class="pre">b</span></code> might in practice
be of the form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">log</span><span class="p">(</span><span class="n">N</span> <span class="n">exp</span> <span class="n">f</span><span class="p">(</span><span class="n">x_s</span><span class="p">,</span> <span class="n">y_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">})</span> <span class="o">/</span> <span class="n">sum_t</span><span class="s1">&#39;  exp f(x_s, y_t&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the number of terms that the sum over <code class="docutils literal notranslate"><span class="pre">t'</span></code> included,
which might include some or all of the other sequences as well as this
one.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>we don’t require <code class="docutils literal notranslate"><span class="pre">px</span></code> and <code class="docutils literal notranslate"><span class="pre">py</span></code> to be contiguous, but the
code assumes for optimization purposes that the <code class="docutils literal notranslate"><span class="pre">T</span></code> axis has
stride 1.</p>
</div>
</p></li>
<li><p><strong>py</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>A torch.Tensor of the same dtype as <code class="docutils literal notranslate"><span class="pre">px</span></code>, with shape <code class="docutils literal notranslate"><span class="pre">[B][S+1][T]</span></code>,
representing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">py</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">s</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span>  <span class="n">log</span> <span class="p">[</span> <span class="n">p</span><span class="p">(</span><span class="n">y_t</span> <span class="o">|</span> <span class="n">x_</span><span class="p">{</span><span class="mf">0.</span><span class="o">.</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">},</span> <span class="n">y_</span><span class="p">{</span><span class="mf">0.</span><span class="o">.</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">})</span> <span class="o">/</span> <span class="n">p</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span> <span class="p">]</span>
</pre></div>
</div>
<p>This function does not treat <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> differently; the only
difference is that for optimization purposes we assume the last axis
(the <code class="docutils literal notranslate"><span class="pre">t</span></code> axis) has stride of 1; this is true if <code class="docutils literal notranslate"><span class="pre">px</span></code> and <code class="docutils literal notranslate"><span class="pre">py</span></code> are
contiguous.</p>
</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – If supplied, a torch.LongTensor of shape <code class="docutils literal notranslate"><span class="pre">[B][4]</span></code>, where each
row contains <code class="docutils literal notranslate"><span class="pre">[s_begin,</span> <span class="pre">t_begin,</span> <span class="pre">s_end,</span> <span class="pre">t_end]</span></code>,
with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">s_begin</span> <span class="pre">&lt;=</span> <span class="pre">s_end</span> <span class="pre">&lt;=</span> <span class="pre">S</span></code> and <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">t_begin</span> <span class="pre">&lt;=</span> <span class="pre">t_end</span> <span class="pre">&lt;</span> <span class="pre">T</span></code>
(this implies that empty sequences are allowed).
If not supplied, the values <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">S,</span> <span class="pre">T]</span></code> will be assumed.
These are the beginning and one-past-the-last positions in the <code class="docutils literal notranslate"><span class="pre">x</span></code> and
<code class="docutils literal notranslate"><span class="pre">y</span></code> sequences respectively, and can be used if not all sequences are
of the same length.</p></li>
<li><p><strong>return_grad</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to return grads of <code class="docutils literal notranslate"><span class="pre">px</span></code> and <code class="docutils literal notranslate"><span class="pre">py</span></code>, this grad standing for the
occupation probability is the output of the backward with a
<code class="docutils literal notranslate"><span class="pre">fake</span> <span class="pre">gradient</span></code> the <code class="docutils literal notranslate"><span class="pre">fake</span> <span class="pre">gradient</span></code> is the same as the gradient
you’d get if you did <code class="docutils literal notranslate"><span class="pre">torch.autograd.grad((scores.sum()),</span> <span class="pre">[px,</span> <span class="pre">py])</span></code>.
This is useful to implement the pruned version of rnnt loss.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Returns a torch.Tensor of shape <code class="docutils literal notranslate"><span class="pre">[B]</span></code>, containing the log of the mutual
information between the b’th pair of sequences.  This is defined by
the following recursion on <code class="docutils literal notranslate"><span class="pre">p[b,s,t]</span></code> (where <code class="docutils literal notranslate"><span class="pre">p</span></code> is of shape
<code class="docutils literal notranslate"><span class="pre">[B,S+1,T+1]</span></code>), representing a mutual information between sub-sequences
of lengths <code class="docutils literal notranslate"><span class="pre">s</span></code> and <code class="docutils literal notranslate"><span class="pre">t</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>     p[b,0,0] = 0.0
if !modified:
     p[b,s,t] = log_add(p[b,s-1,t] + px[b,s-1,t],
                        p[b,s,t-1] + py[b,s,t-1])
if modified:
     p[b,s,t] = log_add(p[b,s-1,t-1] + px[b,s-1,t-1],
                        p[b,s,t-1] + py[b,s,t-1])
</pre></div>
</div>
<p>where we handle edge cases by treating quantities with negative indexes
as <strong>-infinity</strong>.  The extension to cases where the boundaries are
specified should be obvious; it just works on shorter sequences with
offsets into <code class="docutils literal notranslate"><span class="pre">px</span></code> and <code class="docutils literal notranslate"><span class="pre">py</span></code>.</p>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="mwer-loss">
<h2>mwer_loss<a class="headerlink" href="#mwer-loss" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.mwer_loss">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">mwer_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lattice</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ref_texts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nbest_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_paths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sum'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/mwer_loss.py#L127-L179"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.mwer_loss" title="Permalink to this definition"></a></dt>
<dd><p>Compute the Minimum loss given a lattice and corresponding ref_texts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lattice</strong> – An FsaVec with axes [utt][state][arc].</p></li>
<li><p><strong>ref_texts</strong> – <dl class="simple">
<dt>It can be one of the following types:</dt><dd><ul>
<li><p>A list of list-of-integers, e..g, <cite>[ [1, 2], [1, 2, 3] ]</cite></p></li>
<li><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>.
Must have <cite>num_axes == 2</cite> and with dtype <cite>torch.int32</cite>.</p></li>
</ul>
</dd>
</dl>
</p></li>
<li><p><strong>nbest_scale</strong> – Scale <cite>lattice.score</cite> before passing it to <a class="reference internal" href="#k2.random_paths" title="k2.random_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.random_paths()</span></code></a>.
A smaller value leads to more unique paths at the risk of being not
to sample the path with the best score.’’</p></li>
<li><p><strong>num_paths</strong> – Number of paths to <strong>sample</strong> from the lattice
using <a class="reference internal" href="#k2.random_paths" title="k2.random_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.random_paths()</span></code></a>.</p></li>
<li><p><strong>temperature</strong> – For long utterances, the dynamic range of scores will be too large
and the posteriors will be mostly 0 or 1.
To prevent this it might be a good idea to have an extra argument
that functions like a temperature.
We scale the logprobs by before doing the normalization.</p></li>
<li><p><strong>use_double_scores</strong> – True to use double precision floating point.
False to use single precision.</p></li>
<li><p><strong>reduction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[‘none’, ‘mean’, ‘sum’]) – <p>Specifies the reduction to apply to the output:
‘none’ | ‘sum’ | ‘mean’.
‘none’: no reduction will be applied.</p>
<blockquote>
<div><p>The returned ‘loss’ is a k2.RaggedTensor, with
loss.tot_size(0) == batch_size.
loss.tot_size(1) == total_num_paths_of_current_batch
If you want the MWER loss for each utterance, just do:
<cite>loss_per_utt = loss.sum()</cite>
Then loss_per_utt.shape[0] should be batch_size.
See more example usages in ‘k2/python/tests/mwer_test.py’</p>
</div></blockquote>
<p>’sum’: sum loss of each path over the whole batch together.
‘mean’: divide above ‘sum’ by total num paths over the whole batch.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Minimum Word Error Rate loss.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="one-best-decoding">
<h2>one_best_decoding<a class="headerlink" href="#one-best-decoding" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.one_best_decoding">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">one_best_decoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lattice</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/decode.py#L98-L114"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.one_best_decoding" title="Permalink to this definition"></a></dt>
<dd><p>Get the best path from a lattice.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lattice</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The decoding lattice returned by <a class="reference internal" href="#k2.get_lattice" title="k2.get_lattice"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_lattice()</span></code></a>.</p></li>
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use double precision floating point in the computation.
False to use single precision.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An FsaVec containing linear paths.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="properties-to-str">
<h2>properties_to_str<a class="headerlink" href="#properties-to-str" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.properties_to_str">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">properties_to_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_properties.py#L48-L58"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.properties_to_str" title="Permalink to this definition"></a></dt>
<dd><p>Convert properties to a string for debug purpose.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>p</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – An integer returned by <code class="xref py py-func docutils literal notranslate"><span class="pre">get_properties()</span></code>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A string representation of the input properties.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="prune-on-arc-post">
<h2>prune_on_arc_post<a class="headerlink" href="#prune-on-arc-post" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.prune_on_arc_post">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">prune_on_arc_post</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_prob</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L976-L1006"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.prune_on_arc_post" title="Permalink to this definition"></a></dt>
<dd><p>Remove arcs whose posteriors are less than the given threshold.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec. Must have 3 axes.</p></li>
<li><p><strong>threshold_prob</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Arcs whose posteriors are less than this value are removed.
.. note:: 0 &lt; threshold_prob &lt; 1</p></li>
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use double precision during computation; False to use
single precision.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return a pruned FsaVec.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="pruned-ranges-to-lattice">
<h2>pruned_ranges_to_lattice<a class="headerlink" href="#pruned-ranges-to-lattice" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.pruned_ranges_to_lattice">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">pruned_ranges_to_lattice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ranges</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frames</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.RaggedArc</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.pruned_ranges_to_lattice" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="random-fsa">
<h2>random_fsa<a class="headerlink" href="#random-fsa" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.random_fsa">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">random_fsa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">acyclic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_symbol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_num_arcs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_arcs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L644-L663"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.random_fsa" title="Permalink to this definition"></a></dt>
<dd><p>Generate a random Fsa.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>acyclic</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true, generated Fsa will be acyclic.</p></li>
<li><p><strong>max_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – <dl class="simple">
<dt>Maximum symbol on arcs. Generated arc symbols will be in range</dt><dd><p>[-1,max_symbol], note -1 is kFinalSymbol; must be at least 0;</p>
</dd>
<dt>min_num_arcs:</dt><dd><p>Minimum number of arcs; must be at least 0.</p>
</dd>
<dt>max_num_arcs:</dt><dd><p>Maximum number of arcs; must be &gt;= min_num_arcs.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="random-fsa-vec">
<h2>random_fsa_vec<a class="headerlink" href="#random-fsa-vec" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.random_fsa_vec">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">random_fsa_vec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_num_fsas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_fsas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">acyclic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_symbol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_num_arcs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_arcs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L666-L693"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.random_fsa_vec" title="Permalink to this definition"></a></dt>
<dd><p>Generate a random FsaVec.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>min_num_fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Minimum number of fsas we’ll generated in the returned FsaVec;
must be at least 1.</p></li>
<li><p><strong>max_num_fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Maximum number of fsas we’ll generated in the returned FsaVec;
must be &gt;= min_num_fsas.</p></li>
<li><p><strong>acyclic</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true, generated Fsas will be acyclic.</p></li>
<li><p><strong>max_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Maximum symbol on arcs. Generated arcs’ symbols will be in range
[-1,max_symbol], note -1 is kFinalSymbol; must be at least 0;</p></li>
<li><p><strong>min_num_arcs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Minimum number of arcs in each Fsa; must be at least 0.</p></li>
<li><p><strong>max_num_arcs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Maximum number of arcs in each Fsa; must be &gt;= min_num_arcs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="random-paths">
<h2>random_paths<a class="headerlink" href="#random-paths" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.random_paths">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">random_paths</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_paths</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L924-L973"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.random_paths" title="Permalink to this definition"></a></dt>
<dd><p>Compute pseudo-random paths through the FSAs in this vector of FSAs
(this object must have 3 axes, <cite>self.arcs.num_axes() == 3</cite>)</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It does not support autograd.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Do not be confused by the function name. There is no
randomness at all, thus no <cite>seed</cite>. It uses a deterministic algorithm
internally, similar to arithmetic coding
(see <a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_coding">https://en.wikipedia.org/wiki/Arithmetic_coding</a>).</p>
<p>Look into the C++ implementation code for more details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – A FsaVec, i.e., <cite>len(fsas.shape) == 3</cite></p></li>
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true, do computation with double-precision,
else float (single-precision)</p></li>
<li><p><strong>num_paths</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of paths requested through each FSA. FSAs that have no successful
paths will have zero paths returned.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>[fsa][path][arc_pos]; the final
sub-lists (indexed with arc_pos) are sequences of arcs starting from the
start state and terminating in the final state. The values are arc_idx012,
i.e. arc indexes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Returns a k2.RaggedTensor (dtype is torch.int32) with 3 axes</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="remove-epsilon">
<h2>remove_epsilon<a class="headerlink" href="#remove-epsilon" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.remove_epsilon">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">remove_epsilon</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L699-L737"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.remove_epsilon" title="Permalink to this definition"></a></dt>
<dd><p>Remove epsilons (symbol zero) in the input Fsa.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Call <a class="reference internal" href="#k2.connect" title="k2.connect"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.connect()</span></code></a> if you are using a GPU version.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – <p>The input FSA. It can be either a single FSA or an FsaVec.
Works either for CPU or GPU, but the algorithm is different.
We can only use the CPU algorithm if the input is top-sorted,
and the GPU algorithm, while it works for CPU, may not be
very fast.</p>
<p><cite>fsa</cite> must be free of epsilon loops that have score
greater than 0.</p>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The resulting Fsa is equivalent to the input <cite>fsa</cite> under the
tropical semiring but will be epsilon-free.  Any linear tensor
attributes, such as ‘aux_labels’, will have been turned into
ragged labels after removing fillers (i.e. labels whose
value equals fsa.XXX_filler if the attribute name is XXX),
counting -1’s on final-arcs as fillers even if the filler
value for that attribute is not -1.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="remove-epsilon-and-add-self-loops">
<h2>remove_epsilon_and_add_self_loops<a class="headerlink" href="#remove-epsilon-and-add-self-loops" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.remove_epsilon_and_add_self_loops">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">remove_epsilon_and_add_self_loops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_filler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L746-L774"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.remove_epsilon_and_add_self_loops" title="Permalink to this definition"></a></dt>
<dd><p>Remove epsilons (symbol zero) in the input Fsa, and then add
epsilon self-loops to all states in the input Fsa (usually as
a preparation for intersection with treat_epsilons_specially=0).</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Call <a class="reference internal" href="#k2.connect" title="k2.connect"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.connect()</span></code></a> if you are using a GPU version.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA. It can be either a single FSA or an FsaVec.</p></li>
<li><p><strong>remove_filler</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true, we will remove any <cite>filler values</cite> of attributes when
converting linear to ragged attributes.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The resulting Fsa.   See <a class="reference internal" href="#k2.remove_epsilon" title="k2.remove_epsilon"><code class="xref py py-func docutils literal notranslate"><span class="pre">remove_epsilon()</span></code></a> for details.
The only epsilons will be epsilon self-loops on all states.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="remove-epsilon-self-loops">
<h2>remove_epsilon_self_loops<a class="headerlink" href="#remove-epsilon-self-loops" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.remove_epsilon_self_loops">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">remove_epsilon_self_loops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L657-L676"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.remove_epsilon_self_loops" title="Permalink to this definition"></a></dt>
<dd><p>Remove epsilon self-loops of an Fsa or an FsaVec.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Unlike <a class="reference internal" href="#k2.remove_epsilon" title="k2.remove_epsilon"><code class="xref py py-func docutils literal notranslate"><span class="pre">remove_epsilon()</span></code></a>, this funciton removes only
epsilon self-loops.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA. It can be either a single FSA or an FsaVec.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code> that has no epsilon self-loops on every
non-final state.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="replace-fsa">
<h2>replace_fsa<a class="headerlink" href="#replace-fsa" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.replace_fsa">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">replace_fsa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbol_begin_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ret_arc_map</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L1110-L1157"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.replace_fsa" title="Permalink to this definition"></a></dt>
<dd><p>Replace arcs in index FSA with the corresponding fsas in a vector of
FSAs(src). For arcs in <cite>index</cite> with label
<cite>symbol_range_begin &lt;= label &lt; symbol_range_begin + src.Dim0()</cite> will be
replaced with fsa indexed <cite>label - symbol_begin_range</cite> in <cite>src</cite>.
The destination state of the arc in <cite>index</cite> is identified with the
<cite>final-state</cite> of the corresponding FSA in <cite>src</cite>, and the arc in <cite>index</cite>
will become an epsilon arc leading to a new state in the output that is
a copy of the start-state of the corresponding FSA in <cite>src</cite>. Arcs with
labels outside this range are just copied. Labels on final-arcs in <cite>src</cite>
(Which will be -1) would be set to 0(epsilon) in the result fsa.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Attributes of the result inherits from <cite>index</cite> and <cite>src</cite> via
<cite>arc_map_index</cite> and <cite>arc_map_src</cite>, But if there are attributes
with same name, only the attributes with dtype <cite>torch.float32</cite>
are supported, the other kinds of attributes are discarded.
See docs in <cite>fsa_from_binary_function_tensor</cite> for details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – Fsa that we’ll be inserting into the result, MUST have 3 axes.</p></li>
<li><p><strong>index</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The Fsa that is to be replaced, It can be a single FSA or a vector of
FSAs.</p></li>
<li><p><strong>symbol_range_begin</strong> – Beginning of the range of symbols that are to be replaced with Fsas.</p></li>
<li><p><strong>ret_arc_map</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if true, will return a tuple
(new_fsas, arc_map_index, arc_map_src) with <cite>arc_map_index</cite> and
<cite>arc_map_src</cite> tensors of int32 that maps from arcs in the result to
arcs in <cite>index</cite> and <cite>src</cite> , with -1’s for the arcs not mapped.
If false, just returns new_fsas.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="reverse">
<h2>reverse<a class="headerlink" href="#reverse" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.reverse">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">reverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L679-L696"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.reverse" title="Permalink to this definition"></a></dt>
<dd><p>Reverse the input Fsa.  If the input Fsa accepts string ‘x’ with weight
‘x.weight’, then the reversed Fsa accepts the reverse of string ‘x’ with
weight ‘x.weight.reverse’. As the Fsas of k2 run on the Log-semiring or
Tropical-semiring, the ‘weight.reverse’ will equal to the orignal ‘weight’.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA. It can be either a single FSA or an FsaVec.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code> which has been reversed.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="rnnt-loss">
<h2>rnnt_loss<a class="headerlink" href="#rnnt-loss" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.rnnt_loss">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">rnnt_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">termination_symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnnt_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'regular'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delay_penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L450-L540"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.rnnt_loss" title="Permalink to this definition"></a></dt>
<dd><p>A normal RNN-T loss, which uses a ‘joiner’ network output as input,
i.e. a 4 dimensions tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The output of joiner network, with shape (B, T, S + 1, C),
i.e. batch, time_seq_len, symbol_seq_len+1, num_classes</p></li>
<li><p><strong>symbols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The symbol sequences, a LongTensor of shape [B][S], and elements
in {0..C-1}.</p></li>
<li><p><strong>termination_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the termination symbol, with 0 &lt;= termination_symbol &lt; C</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a optional LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame] that is treated as
[0, 0, S, T] if boundary is not supplied.
Most likely you will want begin_symbol and begin_frame to be zero.</p></li>
<li><p><strong>rnnt_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – <p>Specifies the type of rnnt paths: <cite>regular</cite>, <cite>modified</cite> or <cite>constrained</cite>.
<cite>regular</cite>: The regular rnnt that taking you to the next frame only if</p>
<blockquote>
<div><p>emitting a blank (i.e., emitting a symbol does not take you
to the next frame).</p>
</div></blockquote>
<dl class="simple">
<dt><cite>modified</cite>: A modified version of rnnt that will take you to the next</dt><dd><p>frame either emitting a blank or a non-blank symbol.</p>
</dd>
<dt><cite>constrained</cite>: A version likes the modified one that will go to the next</dt><dd><p>frame when you emit a non-blank symbol, but this is done
by “forcing” you to take the blank transition from the
<em>next</em> context on the <em>current</em> frame, e.g. if we emit
c given “a b” context, we are forced to emit “blank”
given “b c” context on the current frame.</p>
</dd>
</dl>
</p></li>
<li><p><strong>delay_penalty</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – A constant value to penalize symbol delay, this may be
needed when training with time masking, to avoid the time-masking
encouraging the network to delay symbols.
See <a class="reference external" href="https://github.com/k2-fsa/k2/issues/955">https://github.com/k2-fsa/k2/issues/955</a> for more details.</p></li>
<li><p><strong>reduction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Specifies the reduction to apply to the output: <cite>none</cite>, <cite>mean</cite> or <cite>sum</cite>.
<cite>none</cite>: no reduction will be applied.
<cite>mean</cite>: apply <cite>torch.mean</cite> over the batches.
<cite>sum</cite>: the output will be summed.
Default: <cite>mean</cite></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If recursion is <cite>none</cite>, returns a tensor of shape (B,), containing the
total RNN-T loss values for each element of the batch, otherwise a scalar
with the reduction applied.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="rnnt-loss-pruned">
<h2>rnnt_loss_pruned<a class="headerlink" href="#rnnt-loss-pruned" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.rnnt_loss_pruned">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">rnnt_loss_pruned</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">termination_symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnnt_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'regular'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delay_penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_hat_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L1351-L1468"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.rnnt_loss_pruned" title="Permalink to this definition"></a></dt>
<dd><p>A RNN-T loss with pruning, which uses the output of a pruned ‘joiner’
network as input, i.e. a 4 dimensions tensor with shape (B, T, s_range, C),
s_range means the number of symbols kept for each frame.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The pruned output of joiner network, with shape (B, T, s_range, C),
i.e. batch, time_seq_len, prune_range, num_classes</p></li>
<li><p><strong>symbols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A LongTensor of shape [B][S], containing the symbols at each position
of the sequence.</p></li>
<li><p><strong>ranges</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A tensor containing the symbol ids for each frame that we want to keep.
It is a LongTensor of shape <code class="docutils literal notranslate"><span class="pre">[B][T][s_range]</span></code>, where <code class="docutils literal notranslate"><span class="pre">ranges[b,t,0]</span></code>
contains the begin symbol <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">s</span> <span class="pre">&lt;=</span> <span class="pre">S</span> <span class="pre">-</span> <span class="pre">s_range</span> <span class="pre">+1</span></code>, such that
<code class="docutils literal notranslate"><span class="pre">logits[b,t,:,:]</span></code> represents the logits with positions
<code class="docutils literal notranslate"><span class="pre">s,</span> <span class="pre">s</span> <span class="pre">+</span> <span class="pre">1,</span> <span class="pre">...</span> <span class="pre">s</span> <span class="pre">+</span> <span class="pre">s_range</span> <span class="pre">-</span> <span class="pre">1</span></code>.
See docs in <a class="reference internal" href="#k2.get_rnnt_prune_ranges" title="k2.get_rnnt_prune_ranges"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_rnnt_prune_ranges()</span></code></a> for more details of what ranges
contains.</p></li>
<li><p><strong>termination_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The identity of the termination symbol, must be in {0..C-1}</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame] that is treated as
[0, 0, S, T] if boundary is not supplied.
Most likely you will want begin_symbol and begin_frame to be zero.</p></li>
<li><p><strong>rnnt_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – <p>Specifies the type of rnnt paths: <cite>regular</cite>, <cite>modified</cite> or <cite>constrained</cite>.
<cite>regular</cite>: The regular rnnt that taking you to the next frame only if</p>
<blockquote>
<div><p>emitting a blank (i.e., emitting a symbol does not take you
to the next frame).</p>
</div></blockquote>
<dl class="simple">
<dt><cite>modified</cite>: A modified version of rnnt that will take you to the next</dt><dd><p>frame either emitting a blank or a non-blank symbol.</p>
</dd>
<dt><cite>constrained</cite>: A version likes the modified one that will go to the next</dt><dd><p>frame when you emit a non-blank symbol, but this is done
by “forcing” you to take the blank transition from the
<em>next</em> context on the <em>current</em> frame, e.g. if we emit
c given “a b” context, we are forced to emit “blank”
given “b c” context on the current frame.</p>
</dd>
</dl>
</p></li>
<li><p><strong>delay_penalty</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – A constant value to penalize symbol delay, this may be
needed when training with time masking, to avoid the time-masking
encouraging the network to delay symbols.
See <a class="reference external" href="https://github.com/k2-fsa/k2/issues/955">https://github.com/k2-fsa/k2/issues/955</a> for more details.</p></li>
<li><p><strong>reduction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Specifies the reduction to apply to the output: <cite>none</cite>, <cite>mean</cite> or <cite>sum</cite>.
<cite>none</cite>: no reduction will be applied.
<cite>mean</cite>: apply <cite>torch.mean</cite> over the batches.
<cite>sum</cite>: the output will be summed.
Default: <cite>mean</cite></p></li>
<li><p><strong>use_hat_loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True, we compute the Hybrid Autoregressive Transducer (HAT) loss from
<a class="reference external" href="https://arxiv.org/abs/2003.07705">https://arxiv.org/abs/2003.07705</a>. This is a variant of RNN-T that models
the blank distribution separately as a Bernoulli distribution, and the
non-blanks are modeled as a multinomial. This formulation may be useful
for performing internal LM estimation, as described in the paper.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If reduction is <cite>none</cite>, returns a tensor of shape (B,), containing the
total RNN-T loss values for each sequence of the batch, otherwise a scalar
with the reduction applied.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="rnnt-loss-simple">
<h2>rnnt_loss_simple<a class="headerlink" href="#rnnt-loss-simple" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.rnnt_loss_simple">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">rnnt_loss_simple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">am</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">termination_symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnnt_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'regular'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delay_penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L228-L338"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.rnnt_loss_simple" title="Permalink to this definition"></a></dt>
<dd><p>A simple case of the RNN-T loss, where the ‘joiner’ network is just
addition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lm</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – language-model part of unnormalized log-probs of symbols, with shape
(B, S+1, C), i.e. batch, symbol_seq_len+1, num_classes</p></li>
<li><p><strong>am</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – acoustic-model part of unnormalized log-probs of symbols, with shape
(B, T, C), i.e. batch, frame, num_classes</p></li>
<li><p><strong>symbols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – the symbol sequences, a LongTensor of shape [B][S], and elements in
{0..C-1}.</p></li>
<li><p><strong>termination_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the termination symbol, with 0 &lt;= termination_symbol &lt; C</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a optional LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame] that is treated as
[0, 0, S, T]
if boundary is not supplied.
Most likely you will want begin_symbol and begin_frame to be zero.</p></li>
<li><p><strong>rnnt_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – <p>Specifies the type of rnnt paths: <cite>regular</cite>, <cite>modified</cite> or <cite>constrained</cite>.
<cite>regular</cite>: The regular rnnt that taking you to the next frame only if</p>
<blockquote>
<div><p>emitting a blank (i.e., emitting a symbol does not take you
to the next frame).</p>
</div></blockquote>
<dl class="simple">
<dt><cite>modified</cite>: A modified version of rnnt that will take you to the next</dt><dd><p>frame either emitting a blank or a non-blank symbol.</p>
</dd>
<dt><cite>constrained</cite>: A version likes the modified one that will go to the next</dt><dd><p>frame when you emit a non-blank symbol, but this is done
by “forcing” you to take the blank transition from the
<em>next</em> context on the <em>current</em> frame, e.g. if we emit
c given “a b” context, we are forced to emit “blank”
given “b c” context on the current frame.</p>
</dd>
</dl>
</p></li>
<li><p><strong>delay_penalty</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – A constant value to penalize symbol delay, this may be
needed when training with time masking, to avoid the time-masking
encouraging the network to delay symbols.
See <a class="reference external" href="https://github.com/k2-fsa/k2/issues/955">https://github.com/k2-fsa/k2/issues/955</a> for more details.</p></li>
<li><p><strong>reduction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Specifies the reduction to apply to the output: <cite>none</cite>, <cite>mean</cite> or <cite>sum</cite>.
<cite>none</cite>: no reduction will be applied.
<cite>mean</cite>: apply <cite>torch.mean</cite> over the batches.
<cite>sum</cite>: the output will be summed.
Default: <cite>mean</cite></p></li>
<li><p><strong>return_grad</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to return grads of px and py, this grad standing for the
occupation probability is the output of the backward with a
<cite>fake gradient</cite>, the <cite>fake gradient</cite> is the same as the gradient you’d
get if you did <cite>torch.autograd.grad((-loss.sum()), [px, py])</cite>, note, the
loss here is the loss with reduction “none”.
This is useful to implement the pruned version of rnnt loss.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If return_grad is False, returns a tensor of shape (B,), containing the
total RNN-T loss values for each element of the batch if reduction equals
to “none”, otherwise a scalar with the reduction applied.
If return_grad is True, the grads of px and py, which is the output of
backward with a <a href="#id1"><span class="problematic" id="id2">`</span></a>fake gradient`(see above), will be returned too. And the
returned value will be a tuple like (loss, (px_grad, py_grad)).</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="rnnt-loss-smoothed">
<h2>rnnt_loss_smoothed<a class="headerlink" href="#rnnt-loss-smoothed" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.rnnt_loss_smoothed">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">rnnt_loss_smoothed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">am</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">termination_symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lm_only_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">am_only_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boundary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnnt_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'regular'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delay_penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L1707-L1829"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.rnnt_loss_smoothed" title="Permalink to this definition"></a></dt>
<dd><p>A simple case of the RNN-T loss, where the ‘joiner’ network is just
addition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lm</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – language-model part of unnormalized log-probs of symbols, with shape
(B, S+1, C), i.e. batch, symbol_seq_len+1, num_classes.
These are assumed to be well-normalized, in the sense that we could
use them as probabilities separately from the am scores</p></li>
<li><p><strong>am</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – acoustic-model part of unnormalized log-probs of symbols, with shape
(B, T, C), i.e. batch, frame, num_classes</p></li>
<li><p><strong>symbols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – the symbol sequences, a LongTensor of shape [B][S], and elements in
{0..C-1}.</p></li>
<li><p><strong>termination_symbol</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the termination symbol, with 0 &lt;= termination_symbol &lt; C</p></li>
<li><p><strong>lm_only_scale</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – the scale on the “LM-only” part of the loss.</p></li>
<li><p><strong>am_only_scale</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – the scale on the “AM-only” part of the loss, for which we use
an “averaged” LM (averaged over all histories, so effectively unigram).</p></li>
<li><p><strong>boundary</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – a LongTensor of shape [B, 4] with elements interpreted as
[begin_symbol, begin_frame, end_symbol, end_frame] that is treated as
[0, 0, S, T]
if boundary is not supplied.
Most likely you will want begin_symbol and begin_frame to be zero.</p></li>
<li><p><strong>rnnt_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – <p>Specifies the type of rnnt paths: <cite>regular</cite>, <cite>modified</cite> or <cite>constrained</cite>.
<cite>regular</cite>: The regular rnnt that taking you to the next frame only if</p>
<blockquote>
<div><p>emitting a blank (i.e., emitting a symbol does not take you
to the next frame).</p>
</div></blockquote>
<dl class="simple">
<dt><cite>modified</cite>: A modified version of rnnt that will take you to the next</dt><dd><p>frame whether emitting a blank or a non-blank symbol.</p>
</dd>
<dt><cite>constrained</cite>: A version likes the modified one that will go to the next</dt><dd><p>frame when you emit a non-blank symbol, but this is done
by “forcing” you to take the blank transition from the
<em>next</em> context on the <em>current</em> frame, e.g. if we emit
c given “a b” context, we are forced to emit “blank”
given “b c” context on the current frame.</p>
</dd>
</dl>
</p></li>
<li><p><strong>delay_penalty</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – A constant value to penalize symbol delay, this may be
needed when training with time masking, to avoid the time-masking
encouraging the network to delay symbols.
See <a class="reference external" href="https://github.com/k2-fsa/k2/issues/955">https://github.com/k2-fsa/k2/issues/955</a> for more details.</p></li>
<li><p><strong>reduction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Specifies the reduction to apply to the output: <cite>none</cite>, <cite>mean</cite> or <cite>sum</cite>.
<cite>none</cite>: no reduction will be applied.
<cite>mean</cite>: apply <cite>torch.mean</cite> over the batches.
<cite>sum</cite>: the output will be summed.
Default: <cite>mean</cite></p></li>
<li><p><strong>return_grad</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to return grads of px and py, this grad standing for the
occupation probability is the output of the backward with a
<cite>fake gradient</cite>, the <cite>fake gradient</cite> is the same as the gradient you’d
get if you did <cite>torch.autograd.grad((-loss.sum()), [px, py])</cite>, note, the
loss here is the loss with reduction “none”.
This is useful to implement the pruned version of rnnt loss.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If return_grad is False, returns a tensor of shape (B,), containing the
total RNN-T loss values for each element of the batch if reduction equals
to “none”, otherwise a scalar with the reduction applied.
If return_grad is True, the grads of px and py, which is the output of
backward with a <a href="#id3"><span class="problematic" id="id4">`</span></a>fake gradient`(see above), will be returned too. And the
returned value will be a tuple like (loss, (px_grad, py_grad)).</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="shortest-path">
<h2>shortest_path<a class="headerlink" href="#shortest-path" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.shortest_path">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">shortest_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L598-L620"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.shortest_path" title="Permalink to this definition"></a></dt>
<dd><p>Return the shortest paths as linear FSAs from the start state
to the final state in the tropical semiring.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It uses the opposite sign. That is, It uses <cite>max</cite> instead of <cite>min</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA. It can be either a single FSA or an FsaVec.</p></li>
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – False to use float, i.e., single precision floating point, for scores.
True to use double.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>FsaVec, it contains the best paths as linear FSAs</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="simple-ragged-index-select">
<h2>simple_ragged_index_select<a class="headerlink" href="#simple-ragged-index-select" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.simple_ragged_index_select">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">simple_ragged_index_select</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src:</span> <span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indexes:</span> <span class="pre">k2::RaggedAny</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.simple_ragged_index_select" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="swoosh-l">
<h2>swoosh_l<a class="headerlink" href="#swoosh-l" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.swoosh_l">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">swoosh_l</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_prob</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.swoosh_l" title="Permalink to this definition"></a></dt>
<dd><p>Compute <code class="docutils literal notranslate"><span class="pre">swoosh_l(x)</span> <span class="pre">=</span> <span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">exp(x-4))</span> <span class="pre">-</span> <span class="pre">0.08x</span> <span class="pre">-</span> <span class="pre">0.035</span></code>,
and optionally apply dropout.
If x.requires_grad is True, it returns <code class="docutils literal notranslate"><span class="pre">dropout(swoosh_l(l))</span></code>.
In order to reduce momory, the function derivative <code class="docutils literal notranslate"><span class="pre">swoosh_l'(x)</span></code>
is encoded into 8-bits.
If x.requires_grad is False, it returns <code class="docutils literal notranslate"><span class="pre">swoosh_l(x)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – A Tensor.</p></li>
<li><p><strong>dropout_prob</strong> – A float number. The default value is 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="swoosh-l-forward">
<h2>swoosh_l_forward<a class="headerlink" href="#swoosh-l-forward" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.swoosh_l_forward">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">swoosh_l_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.swoosh_l_forward" title="Permalink to this definition"></a></dt>
<dd><p>Compute <code class="docutils literal notranslate"><span class="pre">swoosh_l(x)</span> <span class="pre">=</span> <span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">exp(x-4))</span> <span class="pre">-</span> <span class="pre">0.08x</span> <span class="pre">-</span> <span class="pre">0.035</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – A Tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="swoosh-l-forward-and-deriv">
<h2>swoosh_l_forward_and_deriv<a class="headerlink" href="#swoosh-l-forward-and-deriv" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.swoosh_l_forward_and_deriv">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">swoosh_l_forward_and_deriv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.swoosh_l_forward_and_deriv" title="Permalink to this definition"></a></dt>
<dd><p>Compute <code class="docutils literal notranslate"><span class="pre">swoosh_l(x)</span> <span class="pre">=</span> <span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">exp(x-4))</span> <span class="pre">-</span> <span class="pre">0.08x</span> <span class="pre">-</span> <span class="pre">0.035</span></code>,
and also the derivative <code class="docutils literal notranslate"><span class="pre">swoosh_l'(x)</span> <span class="pre">=</span> <span class="pre">0.92</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(x-4))</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{swoosh_l}'(x) &amp;= -0.08 + \exp(x-4) / (1 + \exp(x-4)) \\
                    &amp;= -0.08 + (1 -  1 / (1 + \exp(x-4))) \\
                    &amp;= 0.92 - 1 / (1 + \exp(x-4))\end{split}\]</div>
<p><code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">+</span> <span class="pre">exp(x-4)</span></code> might be infinity, but <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(x-4))</span></code> will
be 0 in that case. This is partly why we rearranged the expression above, to
avoid infinity / infinity = nan.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – A Tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="swoosh-r">
<h2>swoosh_r<a class="headerlink" href="#swoosh-r" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.swoosh_r">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">swoosh_r</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_prob</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.swoosh_r" title="Permalink to this definition"></a></dt>
<dd><p>Compute <code class="docutils literal notranslate"><span class="pre">swoosh_r(x)</span> <span class="pre">=</span> <span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">exp(x-1))</span> <span class="pre">-</span> <span class="pre">0.08x</span> <span class="pre">-</span> <span class="pre">0.313261687</span></code>,
and optionally apply dropout.
If x.requires_grad is True, it returns <code class="docutils literal notranslate"><span class="pre">dropout(swoosh_r(l))</span></code>.
In order to reduce momory, the function derivative <code class="docutils literal notranslate"><span class="pre">swoosh_r'(x)</span></code>
is encoded into 8-bits.
If x.requires_grad is False, it returns <code class="docutils literal notranslate"><span class="pre">swoosh_r(x)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – A Tensor.</p></li>
<li><p><strong>dropout_prob</strong> – A float number. The default value is 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="swoosh-r-forward">
<h2>swoosh_r_forward<a class="headerlink" href="#swoosh-r-forward" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.swoosh_r_forward">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">swoosh_r_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.swoosh_r_forward" title="Permalink to this definition"></a></dt>
<dd><p>Compute <code class="docutils literal notranslate"><span class="pre">swoosh_r(x)</span> <span class="pre">=</span> <span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">exp(x-1))</span> <span class="pre">-</span> <span class="pre">0.08x</span> <span class="pre">-</span> <span class="pre">0.313261687</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – A Tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="swoosh-r-forward-and-deriv">
<h2>swoosh_r_forward_and_deriv<a class="headerlink" href="#swoosh-r-forward-and-deriv" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.swoosh_r_forward_and_deriv">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">swoosh_r_forward_and_deriv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.swoosh_r_forward_and_deriv" title="Permalink to this definition"></a></dt>
<dd><p>Compute <code class="docutils literal notranslate"><span class="pre">swoosh_r(x)</span> <span class="pre">=</span> <span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">exp(x-1))</span> <span class="pre">-</span> <span class="pre">0.08x</span> <span class="pre">-</span> <span class="pre">0.313261687</span></code>,
and also the derivative <code class="docutils literal notranslate"><span class="pre">swoosh_r'(x)</span> <span class="pre">=</span> <span class="pre">0.92</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(x-1))</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{swoosh_r}'(x) &amp;= -0.08 + \exp(x-1) / (1 + \exp(x-1)) \\
                    &amp;= -0.08 + (1 -  1 / (1 + \exp(x-1))) \\
                    &amp;= 0.92 - 1 / (1 + \exp(x-1))\end{split}\]</div>
<p><code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">+</span> <span class="pre">exp(x-1)</span></code> might be infinity, but <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(x-1))</span></code> will
be 0 in that case. This is partly why we rearranged the expression above, to
avoid infinity / infinity = nan.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – A Tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-dot">
<h2>to_dot<a class="headerlink" href="#to-dot" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.to_dot">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">to_dot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L109-L247"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.to_dot" title="Permalink to this definition"></a></dt>
<dd><p>Visualize an Fsa via graphviz.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Graphviz is needed only when this function is called.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA to be visualized.</p></li>
<li><p><strong>title</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Optional. The title of the resulting visualization.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Digraph</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Diagraph from grahpviz.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-str">
<h2>to_str<a class="headerlink" href="#to-str" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.to_str">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">to_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openfst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L32-L61"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.to_str" title="Permalink to this definition"></a></dt>
<dd><p>Convert an Fsa to a string.  This version prints out all integer
labels and integer ragged labels on the same line as each arc, the
same format accepted by Fsa.from_str().</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned string can be used to construct an Fsa with Fsa.from_str(),
but you would need to know the names of the auxiliary labels and ragged
labels.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>openfst</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Optional. If true, we negate the scores during the conversion.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A string representation of the Fsa.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-str-simple">
<h2>to_str_simple<a class="headerlink" href="#to-str-simple" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.to_str_simple">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">to_str_simple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openfst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L64-L85"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.to_str_simple" title="Permalink to this definition"></a></dt>
<dd><p>Convert an Fsa to a string.  This is less complete than Fsa.to_str(),
fsa.__str__(), or to_str_full(), meaning it prints only fsa.aux_labels and
no ragged labels, not printing any other attributes.  This is used in
testing.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned string can be used to construct an Fsa.  See also to_str().</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>openfst</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Optional. If true, we negate the scores during the conversion.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A string representation of the Fsa.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-tensor">
<h2>to_tensor<a class="headerlink" href="#to-tensor" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.to_tensor">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">to_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/utils.py#L88-L106"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.to_tensor" title="Permalink to this definition"></a></dt>
<dd><p>Convert an Fsa to a Tensor.</p>
<p>You can save the tensor to disk and read it later
to construct an Fsa.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned Tensor contains only the transition rules, e.g.,
arcs. You may want to save its aux_labels separately if any.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input Fsa.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>torch.Tensor</cite> of dtype <cite>torch.int32</cite>. It is a 2-D tensor
if the input is a single FSA. It is a 1-D tensor if the input
is a vector of FSAs.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="top-sort">
<h2>top_sort<a class="headerlink" href="#top-sort" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.top_sort">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">top_sort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L229-L247"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.top_sort" title="Permalink to this definition"></a></dt>
<dd><p>Sort an FSA topologically.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It returns a new FSA. The input FSA is NOT changed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The input FSA to be sorted. It can be either a single FSA
or a vector of FSAs.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>It returns a single FSA if the input is a single FSA; it returns
a vector of FSAs if the input is a vector of FSAs.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="trivial-graph">
<h2>trivial_graph<a class="headerlink" href="#trivial-graph" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.trivial_graph">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">trivial_graph</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_token</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L1309-L1330"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.trivial_graph" title="Permalink to this definition"></a></dt>
<dd><p>Create a trivial graph which has only two states. On state 0, there are
<cite>max_token</cite> self loops(i.e. a loop for each symbol from 1 to max_token), and
state 1 is the final state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_token</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The maximum token ID (inclusive). We assume that token IDs
are contiguous (from 1 to <cite>max_token</cite>).</p></li>
<li><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Optional. It can be either a string (e.g., ‘cpu’,
‘cuda:0’) or a torch.device.
If it is None, then the returned FSA is on CPU.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Returns the expected trivial graph on the given device.
Note: The returned graph does not contain arcs with label being 0.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="union">
<h2>union<a class="headerlink" href="#union" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.union">
<span class="sig-prename descclassname"><span class="pre">k2.</span></span><span class="sig-name descname"><span class="pre">union</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsas</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L1456-L1475"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.union" title="Permalink to this definition"></a></dt>
<dd><p>Compute the union of a FsaVec.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>We require that every fsa in fsas is non-empty, i.e.,
contains at least two states</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsas</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – A FsaVec. That is, len(fsas.shape) == 3.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A single Fsa that is the union of the input fsas.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="ctcloss">
<h2>CtcLoss<a class="headerlink" href="#ctcloss" title="Permalink to this headline"></a></h2>
<section id="forward">
<h3>forward<a class="headerlink" href="#forward" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.CtcLoss.forward">
<span class="sig-prename descclassname"><span class="pre">CtcLoss.</span></span><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decoding_graph</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_fsa_vec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delay_penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_lengths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/ctc_loss.py#L62-L151"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.CtcLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Compute the CTC loss given a decoding graph and a dense fsa vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decoding_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec. It can be the composition result of a CTC topology
and a transcript.</p></li>
<li><p><strong>dense_fsa_vec</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code>) – It represents the neural network output. Refer to the help
information in <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.DenseFsaVec</span></code>.</p></li>
<li><p><strong>delay_penalty</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – A constant to penalize symbol delay, which is used to make symbol
emit earlier for streaming models. It is almost the same as the
<cite>delay_penalty</cite> in our <cite>rnnt_loss</cite>, See
<a class="reference external" href="https://github.com/k2-fsa/k2/issues/955">https://github.com/k2-fsa/k2/issues/955</a> and
<a class="reference external" href="https://arxiv.org/pdf/2211.00490.pdf">https://arxiv.org/pdf/2211.00490.pdf</a> for more details.</p></li>
<li><p><strong>target_lengths</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – Used only when <cite>reduction</cite> is <cite>mean</cite>. It is a 1-D tensor of batch
size representing lengths of the targets, e.g., number of phones or
number of word pieces in a sentence.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If <cite>reduction</cite> is <cite>none</cite>, return a 1-D tensor with size equal to batch
size. If <cite>reduction</cite> is <cite>mean</cite> or <cite>sum</cite>, return a scalar.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="decodestateinfo">
<h2>DecodeStateInfo<a class="headerlink" href="#decodestateinfo" title="Permalink to this headline"></a></h2>
</section>
<section id="densefsavec">
<h2>DenseFsaVec<a class="headerlink" href="#densefsavec" title="Permalink to this headline"></a></h2>
<section id="init">
<h3>__init__<a class="headerlink" href="#init" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.DenseFsaVec.__init__">
<span class="sig-prename descclassname"><span class="pre">DenseFsaVec.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">log_probs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">supervision_segments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_truncate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/dense_fsa_vec.py#L43-L145"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.DenseFsaVec.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Construct a DenseFsaVec from neural net log-softmax outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_probs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A 3-D tensor of dtype <cite>torch.float32</cite> with shape <cite>(N, T, C)</cite>,
where <cite>N</cite> is the number of sequences, <cite>T</cite> the maximum input
length, and <cite>C</cite> the number of output classes.</p></li>
<li><p><strong>supervision_segments</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – <p>A 2-D <strong>CPU</strong> tensor of dtype <cite>torch.int32</cite> with 3 columns.
Each row contains information for a supervision segment. Column 0
is the <cite>sequence_index</cite> indicating which sequence this segment
comes from; column 1 specifies the <cite>start_frame</cite> of this segment
within the sequence; column 2 contains the <cite>duration</cite> of this
segment.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li><p><cite>0 &lt; start_frame + duration &lt;= T + allow_truncate</cite></p></li>
<li><p><cite>0 &lt;= start_frame &lt; T</cite></p></li>
<li><p><cite>duration &gt; 0</cite></p></li>
</ul>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>If the resulting dense fsa vec is used as an input to
<cite>k2.intersect_dense</cite>, then the last column, i.e., the duration
column, has to be sorted in <strong>decreasing</strong> order.
That is, the first supervision_segment (the first row) has the
largest duration.
Otherwise, you don’t need to sort the last column.</p>
<p><cite>k2.intersect_dense</cite> is often used in the training stage, so
you should usually sort dense fsa vecs by its duration
in training. <cite>k2.intersect_dense_pruned</cite> is usually used in the
decoding stage, so you don’t need to sort dense fsa vecs in
decoding.</p>
</div>
</p></li>
<li><p><strong>allow_truncate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If not zero, it truncates at most this number of frames from
duration in case start_frame + duration &gt; T.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="from-dense-fsa-vec">
<h3>_from_dense_fsa_vec<a class="headerlink" href="#from-dense-fsa-vec" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.DenseFsaVec._from_dense_fsa_vec">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">DenseFsaVec.</span></span><span class="sig-name descname"><span class="pre">_from_dense_fsa_vec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dense_fsa_vec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scores</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/dense_fsa_vec.py#L158-L177"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.DenseFsaVec._from_dense_fsa_vec" title="Permalink to this definition"></a></dt>
<dd><p>Construct a DenseFsaVec from <cite>_k2.DenseFsaVec</cite> and <cite>scores</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is intended for internal use. Users will normally not use it.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dense_fsa_vec</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code>) – An instance of <cite>_k2.DenseFsaVec</cite>.</p></li>
<li><p><strong>scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The <cite>scores</cite> of <cite>_k2.DenseFsaVec</cite> for back propagation.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An instance of DenseFsaVec.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="to">
<h3>to<a class="headerlink" href="#to" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.DenseFsaVec.to">
<span class="sig-prename descclassname"><span class="pre">DenseFsaVec.</span></span><span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/dense_fsa_vec.py#L206-L228"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.DenseFsaVec.to" title="Permalink to this definition"></a></dt>
<dd><p>Move the DenseFsaVec onto a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – An instance of <cite>torch.device</cite> or a string that can be used to
construct a <cite>torch.device</cite>, e.g., ‘cpu’, ‘cuda:0’.
It supports only cpu and cuda devices.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Returns a new DenseFsaVec which is this object copied to the given
device (or this object itself, if the device was the same).</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="device">
<h3>device<a class="headerlink" href="#device" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.DenseFsaVec.device">
<span class="sig-prename descclassname"><span class="pre">DenseFsaVec.</span></span><span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#k2.DenseFsaVec.device" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="duration">
<h3>duration<a class="headerlink" href="#duration" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.DenseFsaVec.duration">
<span class="sig-prename descclassname"><span class="pre">DenseFsaVec.</span></span><span class="sig-name descname"><span class="pre">duration</span></span><a class="headerlink" href="#k2.DenseFsaVec.duration" title="Permalink to this definition"></a></dt>
<dd><p>Return the duration (on CPU) of each seq.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="determinizeweightpushingtype">
<h2>DeterminizeWeightPushingType<a class="headerlink" href="#determinizeweightpushingtype" title="Permalink to this headline"></a></h2>
<section id="name">
<h3>name<a class="headerlink" href="#name" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.DeterminizeWeightPushingType.name">
<span class="sig-prename descclassname"><span class="pre">DeterminizeWeightPushingType.</span></span><span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#k2.DeterminizeWeightPushingType.name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="value">
<h3>value<a class="headerlink" href="#value" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.DeterminizeWeightPushingType.value">
<span class="sig-prename descclassname"><span class="pre">DeterminizeWeightPushingType.</span></span><span class="sig-name descname"><span class="pre">value</span></span><a class="headerlink" href="#k2.DeterminizeWeightPushingType.value" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
</section>
<section id="fsa">
<h2>Fsa<a class="headerlink" href="#fsa" title="Permalink to this headline"></a></h2>
<section id="getattr">
<h3>__getattr__<a class="headerlink" href="#getattr" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.__getattr__">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">__getattr__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L482-L500"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.__getattr__" title="Permalink to this definition"></a></dt>
<dd><p>Note: for attributes that exist as properties, e.g.
self.labels, self.properties, self.requires_grad, we won’t
reach this code because Python checks the class dict before
calling getattr.  The same is true for instance attributes
such as self.{_tensor_attr,_non_tensor_attr,_cache,_properties}</p>
<p>The ‘virtual’ members of this class are those in self._tensor_attr
and self._non_tensor_attr.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="getitem">
<h3>__getitem__<a class="headerlink" href="#getitem" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.__getitem__">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">__getitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">i</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L1016-L1048"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.__getitem__" title="Permalink to this definition"></a></dt>
<dd><p>Get the i-th FSA.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><cite>self</cite> has to be an FsaVec, i.e. len(self.shape) == 3</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>i</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The i-th FSA to select. 0 &lt;= i &lt; self.arcs.dim0().</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The i-th FSA. Note it is a single FSA.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id5">
<h3>__init__<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.__init__">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">arcs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aux_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">properties</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L119-L240"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Build an Fsa from a tensor with optional aux_labels.</p>
<p>It is useful when loading an Fsa from file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>arcs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedArc</span></code>]) – <p>When the <cite>arcs</cite> is an instance of <cite>torch.Tensor</cite>, it is
a torch tensor of dtype <cite>torch.int32</cite> with 4 columns.
Each row represents an arc. Column 0 is the src_state,
column 1 the dest_state, column 2 the label, and column
3 the score.
When the <cite>arcs</cite> is an instance of <cite>_k2.RaggedArc</cite>, it is a
Ragged containing <cite>_k2.Arc</cite> returned by internal functions
(i.e. C++/CUDA functions) or got from other Fsa object
by <cite>fsa.arcs</cite>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Scores are floats and their binary pattern is
<strong>reinterpreted</strong> as integers and saved in a tensor
of dtype <cite>torch.int32</cite>.</p>
</div>
</p></li>
<li><p><strong>aux_labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Optional. If not None, it associates an aux_label with every arc,
so it has as many rows as <cite>tensor</cite>. It is a 1-D tensor of dtype
<cite>torch.int32</cite> or <cite>k2.RaggedTensor</cite> whose <cite>dim0</cite> equals to the
number of arcs.</p></li>
<li><p><strong>properties</strong> – Tensor properties if known (should only be provided by
internal code, as they are not checked; intended for use
by <code class="xref py py-func docutils literal notranslate"><span class="pre">clone()</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An instance of Fsa.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="setattr">
<h3>__setattr__<a class="headerlink" href="#setattr" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.__setattr__">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">__setattr__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L382-L430"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.__setattr__" title="Permalink to this definition"></a></dt>
<dd><div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>We save a reference to <cite>value</cite>. If you need to change <cite>value</cite>
afterwards, please consider passing a copy of it.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the attribute.</p></li>
<li><p><strong>value</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Value of the attribute.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="str">
<h3>__str__<a class="headerlink" href="#str" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.__str__">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L311-L316"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this object</p>
<p>For visualization and debug only.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-arc-post">
<h3>_get_arc_post<a class="headerlink" href="#get-arc-post" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa._get_arc_post">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">_get_arc_post</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L754-L788"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa._get_arc_post" title="Permalink to this definition"></a></dt>
<dd><p>Compute scores on arcs, representing log probabilities;
with log_semiring=True you could call these log posteriors,
but if log_semiring=False they can only be interpreted as the
difference between the best-path score and the score of the
best path that includes this arc.</p>
<p>This version is not differentiable; see also <a class="reference internal" href="#k2.Fsa.get_arc_post" title="k2.Fsa.get_arc_post"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_arc_post()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, use double precision.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, use log semiring, else tropical.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch.Tensor with shape equal to (num_arcs,)
and non-positive elements.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-backward-scores">
<h3>_get_backward_scores<a class="headerlink" href="#get-backward-scores" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa._get_backward_scores">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">_get_backward_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L695-L732"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa._get_backward_scores" title="Permalink to this definition"></a></dt>
<dd><p>Compute backward-scores, i.e. total weight (or best-path weight)
from each state to the final state.</p>
<p>For internal k2 use. Not differentiable.</p>
<p>See also <a class="reference internal" href="#k2.Fsa.get_backward_scores" title="k2.Fsa.get_backward_scores"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_backward_scores()</span></code></a> which is differentiable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use <cite>double precision</cite> floating point.
False to use <cite>single precision</cite>.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use log semiring (log-sum), false to use tropical (i.e. max
on scores).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch.Tensor with shape equal to (num_states,)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-entering-arcs">
<h3>_get_entering_arcs<a class="headerlink" href="#get-entering-arcs" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa._get_entering_arcs">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">_get_entering_arcs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L828-L842"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa._get_entering_arcs" title="Permalink to this definition"></a></dt>
<dd><p>Compute, for each state, the index of the best arc entering it.</p>
<p>For internal k2 use.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use <cite>double precision</cite> floating point.
False to use <cite>single precision</cite>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-forward-scores">
<h3>_get_forward_scores<a class="headerlink" href="#get-forward-scores" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa._get_forward_scores">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">_get_forward_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L590-L624"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa._get_forward_scores" title="Permalink to this definition"></a></dt>
<dd><p>Get (and compute if necessary) cached property
<cite>self.forward_scores_xxx_yyy</cite> (where xxx indicates float-type and
yyy indicates semiring).</p>
<p>For use by internal k2 code; returns the total score from start-state to
each state.  Not differentiable; see <a class="reference internal" href="#k2.Fsa.get_forward_scores" title="k2.Fsa.get_forward_scores"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_forward_scores()</span></code></a> which is
the differentiable version.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use <cite>double precision</cite> floating point.
False to use <cite>single precision</cite>.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use log semiring (log-sum), false to use tropical (i.e. max
on scores).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-tot-scores">
<h3>_get_tot_scores<a class="headerlink" href="#get-tot-scores" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa._get_tot_scores">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">_get_tot_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L646-L674"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa._get_tot_scores" title="Permalink to this definition"></a></dt>
<dd><p>Compute total-scores (one per FSA) as the best-path score.</p>
<p>This version is not differentiable; see also <a class="reference internal" href="#k2.Fsa.get_tot_scores" title="k2.Fsa.get_tot_scores"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_tot_scores()</span></code></a>
which is differentiable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True, use <cite>double precision</cite> floating point; false;
else single precision.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use log semiring (log-sum), false to use tropical (i.e. max
on scores).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="invalidate-cache">
<h3>_invalidate_cache_<a class="headerlink" href="#invalidate-cache" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa._invalidate_cache_">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">_invalidate_cache_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L242-L273"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa._invalidate_cache_" title="Permalink to this definition"></a></dt>
<dd><p>Intended for internal use only so its
name begins with an underline.</p>
<p>Also, it changes <cite>self</cite> in-place.</p>
<p>Currently, it is used only when the <cite>scores</cite> field
are re-assigned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scores_only</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – It True, it invalidates only cached entries related
to scores. If False, the whole cache is invalidated.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="as-dict">
<h3>as_dict<a class="headerlink" href="#as-dict" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.as_dict">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">as_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L1060-L1079"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.as_dict" title="Permalink to this definition"></a></dt>
<dd><p>Convert this Fsa to a dict (probably for purposes of serialization
, e.g., torch.save).</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><cite>self.requires_grad</cite> attribute is not saved.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>dict</cite> that can be used to reconstruct this FSA by using
<cite>Fsa.from_dict</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="convert-attr-to-ragged">
<h3><a class="reference internal" href="#convert-attr-to-ragged">convert_attr_to_ragged</a><a class="headerlink" href="#convert-attr-to-ragged" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.convert_attr_to_ragged_">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">convert_attr_to_ragged_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L1408-L1441"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.convert_attr_to_ragged_" title="Permalink to this definition"></a></dt>
<dd><p>Convert the attribute given by <cite>name</cite> from a 1-D torch.tensor
to a k2.RaggedTensor.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This function ends with an underscore, meaning it changes the FSA
<strong>in-place</strong>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The attribute name. This attribute is expected to be a 1-D tensor
with dtype torch.int32.</p></li>
<li><p><strong>remove_eps</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to remove 0s in the resulting ragged tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return self.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="draw">
<h3>draw<a class="headerlink" href="#draw" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.draw">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">draw</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">title</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L342-L380"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.draw" title="Permalink to this definition"></a></dt>
<dd><p>Render FSA as an image via graphviz, and return the Digraph object;
and optionally save to file <cite>filename</cite>.
<cite>filename</cite> must have a suffix that graphviz understands, such as
<cite>pdf</cite>, <cite>svg</cite> or <cite>png</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to install graphviz to use this function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">graphviz</span>
</pre></div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Filename to (optionally) save to, e.g. ‘foo.png’, ‘foo.svg’,
‘foo.png’  (must have a suffix that graphviz understands).</p></li>
<li><p><strong>title</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Title to be displayed in image, e.g. ‘A simple FSA example’</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Digraph</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="from-openfst">
<h3>from_openfst<a class="headerlink" href="#from-openfst" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.from_openfst">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">from_openfst</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">acceptor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_aux_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aux_label_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ragged_label_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L1313-L1371"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.from_openfst" title="Permalink to this definition"></a></dt>
<dd><p>Create an Fsa from a string in OpenFST format (or a slightly more
general format, if num_aux_labels &gt; 1). See also <a class="reference internal" href="#k2.Fsa.from_str" title="k2.Fsa.from_str"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_str()</span></code></a>.</p>
<p>The given string <cite>s</cite> consists of lines with the following format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">src_state</span> <span class="n">dest_state</span> <span class="n">label</span> <span class="p">[</span><span class="n">aux_label1</span> <span class="n">aux_label2</span><span class="o">...</span><span class="p">]</span> <span class="p">[</span><span class="n">cost</span><span class="p">]</span>
</pre></div>
</div>
<p>(the cost defaults to 0.0 if not present).</p>
<p>The line for the final state consists of two fields:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">final_state</span> <span class="p">[</span><span class="n">cost</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Fields are separated by space(s), tab(s) or both. The <cite>cost</cite>
field is a float, while other fields are integers.</p>
<p>There might be multiple final states. Also, OpenFst may omit the cost
if it is 0.0.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>We use <cite>cost</cite> here to indicate that its value will be negated so that
we can get <cite>scores</cite>. That is, <cite>score = -1 * cost</cite>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At most one of <cite>acceptor</cite>, <cite>num_aux_labels</cite>, and <cite>aux_label_names</cite>
must be supplied; if none are supplied, acceptor format is assumed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The input string. Refer to the above comment for its format.</p></li>
<li><p><strong>acceptor</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Set to true to denote acceptor format which is num_aux_labels == 0,
or false to denote transducer format (i.e. num_aux_labels == 1
with name ‘aux_labels’).</p></li>
<li><p><strong>num_aux_labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The number of auxiliary labels to expect on each line (in addition
to the ‘acceptor’ label; is 1 for traditional transducers but can be
any non-negative number.</p></li>
<li><p><strong>aux_label_names</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – If provided, the length of this list dictates the number of
aux_labels. By default the names are ‘aux_labels’, ‘aux_labels2’,
‘aux_labels3’ and so on.</p></li>
<li><p><strong>ragged_label_names</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – If provided, expect this number of ragged labels, in the order
of this list.  It is advisable that this list be in
alphabetical order, so that the format when we write back to
a string will be unchanged.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="from-str">
<h3>from_str<a class="headerlink" href="#from-str" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.from_str">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">from_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">acceptor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_aux_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aux_label_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ragged_label_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">openfst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L1231-L1311"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.from_str" title="Permalink to this definition"></a></dt>
<dd><p>Create an Fsa from a string in the k2 or OpenFst format.
(See also <a class="reference internal" href="#k2.Fsa.from_openfst" title="k2.Fsa.from_openfst"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_openfst()</span></code></a>).</p>
<p>The given string <cite>s</cite> consists of lines with the following format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">src_state</span> <span class="n">dest_state</span> <span class="n">label</span> <span class="p">[</span><span class="n">aux_label1</span> <span class="n">aux_label2</span><span class="o">...</span><span class="p">]</span> <span class="p">[</span><span class="n">score</span><span class="p">]</span>
</pre></div>
</div>
<p>The line for the final state consists of only one field:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">final_state</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Fields are separated by space(s), tab(s) or both. The <cite>score</cite>
field is a float, while other fields are integers.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The first column has to be non-decreasing.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The final state has the largest state number. There is <strong>ONLY</strong>
ONE final state. All arcs that are connected to the final state
have label -1. If there are aux_labels, they are also -1 for
arcs entering the final state.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At most one of <cite>acceptor</cite>, <cite>num_aux_labels</cite>, and <cite>aux_label_names</cite>
must be supplied; if none are supplied, acceptor format is assumed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The input string. Refer to the above comment for its format.</p></li>
<li><p><strong>acceptor</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Set to true to denote acceptor format which is num_aux_labels == 0,
or false to denote transducer format (i.e. num_aux_labels == 1
with name ‘aux_labels’).</p></li>
<li><p><strong>num_aux_labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The number of auxiliary labels to expect on each line (in addition
to the ‘acceptor’ label; is 1 for traditional transducers but can be
any non-negative number.  The names of the aux_labels default to
‘aux_labels’ then ‘aux_labels2’, ‘aux_labels3’ and so on.</p></li>
<li><p><strong>aux_label_names</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – If provided, the length of this list dictates the number of
aux_labels and this list dictates their names.</p></li>
<li><p><strong>ragged_label_names</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – If provided, expect this number of ragged labels, in the order
of this list.  It is advisable that this list be in
alphabetical order, so that the format when we write back to
a string will be unchanged.</p></li>
<li><p><strong>openfst</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true, will expect the OpenFST format (costs not scores, i.e.
negated; final-probs rather than final-state specified).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id6">
<h3>get_arc_post<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.get_arc_post">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">get_arc_post</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L790-L826"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.get_arc_post" title="Permalink to this definition"></a></dt>
<dd><p>Compute scores on arcs, representing log probabilities;
with log_semiring=True you could call these log posteriors,
but if log_semiring=False they can only be interpreted as the
difference between the best-path score and the score of the
best path that includes this arc.
This version is differentiable; see also <a class="reference internal" href="#k2.Fsa._get_arc_post" title="k2.Fsa._get_arc_post"><code class="xref py py-func docutils literal notranslate"><span class="pre">_get_arc_post()</span></code></a>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Because of how the autograd mechanics works and the
need to avoid circular references, this is not cached;
it’s best to store it if you’ll need it multiple times.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, use double precision.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, use log semiring, else tropical.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch.Tensor with shape equal to (num_arcs,)
and non-positive elements.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id7">
<h3>get_backward_scores<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.get_backward_scores">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">get_backward_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L734-L752"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.get_backward_scores" title="Permalink to this definition"></a></dt>
<dd><p>Compute backward-scores, i.e. total weight (or best-path weight)
from each state to the final state.</p>
<p>Supports autograd.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, use double precision.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, use log semiring, else tropical.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch.Tensor with shape equal to (num_states,)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-filler">
<h3>get_filler<a class="headerlink" href="#get-filler" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.get_filler">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">get_filler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attribute_name</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L318-L340"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.get_filler" title="Permalink to this definition"></a></dt>
<dd><p>Return the filler value associated with attribute names.</p>
<p>This is 0 unless otherwise specified, but you can override this by
for example, doing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fsa</span><span class="o">.</span><span class="n">foo_filler</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<p>which will mean the “filler” for attribute fsa.foo is -1; and this will
get propagated when you do FSA operations, like any other non-tensor
attribute.  The filler is the value that means “nothing is here” (like
epsilon).</p>
<dl class="simple">
<dt>Caution::</dt><dd><p>you should use a value that is castable to float and back to integer
without loss of precision, because currently the <cite>default_value</cite>
parameter of <cite>index_select</cite> in ./ops.py is a float.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id8">
<h3>get_forward_scores<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.get_forward_scores">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">get_forward_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L626-L644"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.get_forward_scores" title="Permalink to this definition"></a></dt>
<dd><p>Compute forward-scores, i.e. total weight (or best-path weight)
from start state to each state.</p>
<p>Supports autograd.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, use double precision.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, use log semiring, else tropical.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch.Tensor with shape equal to (num_states,)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id9">
<h3>get_tot_scores<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.get_tot_scores">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">get_tot_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_semiring</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L676-L693"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.get_tot_scores" title="Permalink to this definition"></a></dt>
<dd><p>Compute total-scores (one per FSA) as the
best-path score.</p>
<p>This version is differentiable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use <cite>double precision</cite> floating point;
False to use <cite>single precision</cite>.</p></li>
<li><p><strong>log_semiring</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use log semiring (log-sum), false to use tropical (i.e. max
on scores).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id10">
<h3>invert<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.invert">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">invert</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L983-L994"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.invert" title="Permalink to this definition"></a></dt>
<dd><p>Swap the <cite>labels</cite> and <cite>aux_labels</cite>.</p>
<p>If there are symbol tables associated with <cite>labels</cite> and
<cite>aux_labels</cite>, they are also swapped.</p>
<p>It is an error if the FSA contains no <cite>aux_labels</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new Fsa.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id11">
<h3><a href="#id125"><span class="problematic" id="id126">invert_</span></a><a class="headerlink" href="#id11" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.invert_">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">invert_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L924-L981"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.invert_" title="Permalink to this definition"></a></dt>
<dd><p>Swap the <cite>labels</cite> and <cite>aux_labels</cite>.</p>
<p>If there are symbol tables associated with <cite>labels</cite> and
<cite>aux_labels</cite>, they are also swapped.</p>
<p>It is an error if the FSA contains no <cite>aux_labels</cite>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The function name ends with an underscore which means this
is an <strong>in-place</strong> operation.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <cite>self</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="rename-tensor-attribute">
<h3><a class="reference internal" href="#rename-tensor-attribute">rename_tensor_attribute</a><a class="headerlink" href="#rename-tensor-attribute" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.rename_tensor_attribute_">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">rename_tensor_attribute_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dest_name</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L869-L922"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.rename_tensor_attribute_" title="Permalink to this definition"></a></dt>
<dd><p>Rename a tensor attribute (or, as a special case ‘labels’),
and also rename non-tensor attributes that are associated with it,
i.e. that have it as a prefix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src_name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The original name, exist as a tensor attribute, e.g. ‘aux_labels’,
or, as a special case, equal ‘labels’; special attributes ‘labels’
and ‘scores’ are allowed but won’t be deleted.</p></li>
<li><p><strong>dest_name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The new name, that we are renaming it to. If it already existed as
a tensor attribute, it will be rewritten; and any previously
existing non-tensor attributes that have this as a prefix will be
deleted.  As a special case, may equal ‘labels’.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return <cite>self</cite>.</p>
</dd>
</dl>
<dl class="simple">
<dt>Note::</dt><dd><p>It is OK if src_name and/or dest_name equals ‘labels’ or ‘scores’,
but these special attributes won’t be deleted.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="requires-grad">
<h3><a href="#id127"><span class="problematic" id="id128">requires_grad_</span></a><a class="headerlink" href="#requires-grad" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.requires_grad_">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L844-L867"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.requires_grad_" title="Permalink to this definition"></a></dt>
<dd><p>Change if autograd should record operations on this FSA:</p>
<p>Sets the <cite>scores</cite>’s requires_grad attribute in-place.</p>
<p>Returns this FSA.</p>
<p>You can test whether this object has the requires_grad property
true or false by accessing <a class="reference internal" href="#k2.Fsa.requires_grad" title="k2.Fsa.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> (handled in
<a class="reference internal" href="#k2.Fsa.__getattr__" title="k2.Fsa.__getattr__"><code class="xref py py-func docutils literal notranslate"><span class="pre">__getattr__()</span></code></a>).</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This is an <strong>in-place</strong> operation as you can see that the function
name ends with <cite>_</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If autograd should record operations on this FSA or not.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>This FSA itself.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="set-scores-stochastic">
<h3><a class="reference internal" href="#set-scores-stochastic">set_scores_stochastic</a><a class="headerlink" href="#set-scores-stochastic" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.set_scores_stochastic_">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">set_scores_stochastic_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L1382-L1406"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.set_scores_stochastic_" title="Permalink to this definition"></a></dt>
<dd><p>Normalize the given <cite>scores</cite> and assign it to <cite>self.scores</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scores</strong> – Tensor of scores of dtype torch.float32, and shape equal to
<cite>self.scores.shape</cite> (one axis). Will be normalized so the
sum, after exponentiating, of the scores leaving each state
that has at least one arc leaving it is 1.</p>
</dd>
</dl>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The function name ends with an underline indicating this function
will modify <cite>self</cite> <strong>in-place</strong>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id12">
<h3>to<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Fsa.to">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa.py#L1090-L1126"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Fsa.to" title="Permalink to this definition"></a></dt>
<dd><p>Move the FSA onto a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>]) – An instance of <cite>torch.device</cite> or a string that can be used to
construct a <cite>torch.device</cite>, e.g., ‘cpu’, ‘cuda:0’.
It supports only cpu and cuda devices.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Returns a new Fsa which is this object copied to the given device
(or this object itself, if the device was the same)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id13">
<h3>device<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.Fsa.device">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#k2.Fsa.device" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="grad">
<h3>grad<a class="headerlink" href="#grad" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.Fsa.grad">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">grad</span></span><a class="headerlink" href="#k2.Fsa.grad" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="num-arcs">
<h3>num_arcs<a class="headerlink" href="#num-arcs" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.Fsa.num_arcs">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">num_arcs</span></span><a class="headerlink" href="#k2.Fsa.num_arcs" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of arcs in this Fsa.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="properties">
<h3>properties<a class="headerlink" href="#properties" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.Fsa.properties">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">properties</span></span><a class="headerlink" href="#k2.Fsa.properties" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="properties-str">
<h3>properties_str<a class="headerlink" href="#properties-str" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.Fsa.properties_str">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">properties_str</span></span><a class="headerlink" href="#k2.Fsa.properties_str" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id14">
<h3>requires_grad<a class="headerlink" href="#id14" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.Fsa.requires_grad">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#k2.Fsa.requires_grad" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="shape">
<h3>shape<a class="headerlink" href="#shape" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.Fsa.shape">
<span class="sig-prename descclassname"><span class="pre">Fsa.</span></span><span class="sig-name descname"><span class="pre">shape</span></span><a class="headerlink" href="#k2.Fsa.shape" title="Permalink to this definition"></a></dt>
<dd><p>Returns:
<cite>(num_states, None)</cite> if this is an Fsa;
<cite>(num_fsas, None, None)</cite> if this is an FsaVec.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, …]</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="mwerloss">
<h2>MWERLoss<a class="headerlink" href="#mwerloss" title="Permalink to this headline"></a></h2>
<section id="id15">
<h3>forward<a class="headerlink" href="#id15" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.MWERLoss.forward">
<span class="sig-prename descclassname"><span class="pre">MWERLoss.</span></span><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lattice</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ref_texts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nbest_scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_paths</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/mwer_loss.py#L56-L124"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.MWERLoss.forward" title="Permalink to this definition"></a></dt>
<dd><p>Compute the Minimum Word Error loss given
a lattice and corresponding ref_texts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lattice</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec with axes [utt][state][arc].</p></li>
<li><p><strong>ref_texts</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]]) – <dl class="simple">
<dt>It can be one of the following types:</dt><dd><ul>
<li><p>A list of list-of-integers, e..g, <cite>[ [1, 2], [1, 2, 3] ]</cite></p></li>
<li><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">k2.RaggedTensor</span></code>.
Must have <cite>num_axes == 2</cite> and with dtype <cite>torch.int32</cite>.</p></li>
</ul>
</dd>
</dl>
</p></li>
<li><p><strong>nbest_scale</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Scale <cite>lattice.score</cite> before passing it to <a class="reference internal" href="#k2.random_paths" title="k2.random_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.random_paths()</span></code></a>.
A smaller value leads to more unique paths at the risk of being not
to sample the path with the best score.</p></li>
<li><p><strong>num_paths</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of paths to <strong>sample</strong> from the lattice
using <a class="reference internal" href="#k2.random_paths" title="k2.random_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.random_paths()</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Minimum Word Error Rate loss.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="nbest">
<h2>Nbest<a class="headerlink" href="#nbest" title="Permalink to this headline"></a></h2>
<section id="from-lattice">
<h3>from_lattice<a class="headerlink" href="#from-lattice" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Nbest.from_lattice">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">Nbest.</span></span><span class="sig-name descname"><span class="pre">from_lattice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lattice</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_paths</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_double_scores</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nbest_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/nbest.py#L100-L198"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Nbest.from_lattice" title="Permalink to this definition"></a></dt>
<dd><p>Construct an Nbest object by <strong>sampling</strong> <cite>num_paths</cite> from a lattice.</p>
<p>Each sampled path is a linear FSA.</p>
<p>We assume <cite>lattice.labels</cite> contains token IDs and <cite>lattice.aux_labels</cite>
contains word IDs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lattice</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec with axes [utt][state][arc].</p></li>
<li><p><strong>num_paths</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of paths to <strong>sample</strong> from the lattice
using <a class="reference internal" href="#k2.random_paths" title="k2.random_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.random_paths()</span></code></a>.</p></li>
<li><p><strong>use_double_scores</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True to use double precision in <a class="reference internal" href="#k2.random_paths" title="k2.random_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.random_paths()</span></code></a>.
False to use single precision.</p></li>
<li><p><strong>nbest_scale</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Scale <cite>lattice.score</cite> before passing it to <a class="reference internal" href="#k2.random_paths" title="k2.random_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">k2.random_paths()</span></code></a>.
A smaller value leads to more unique paths at the risk of being not
to sample the path with the best score.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Nbest</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return an Nbest instance.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id16">
<h3>intersect<a class="headerlink" href="#id16" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Nbest.intersect">
<span class="sig-prename descclassname"><span class="pre">Nbest.</span></span><span class="sig-name descname"><span class="pre">intersect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lats</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/nbest.py#L200-L238"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Nbest.intersect" title="Permalink to this definition"></a></dt>
<dd><p>Intersect this Nbest object with a lattice and get 1-best
path from the resulting FsaVec.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>We assume FSAs in <cite>self.fsa</cite> don’t have epsilon self-loops.
We also assume <cite>self.fsa.labels</cite> and <cite>lats.labels</cite> are token IDs.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>lats</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – An FsaVec. It can be the return value of
<code class="xref py py-func docutils literal notranslate"><span class="pre">whole_lattice_rescoring()</span></code>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Nbest</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return a new Nbest. This new Nbest shares the same shape with <cite>self</cite>,
while its <cite>fsa</cite> is the 1-best path from intersecting <cite>self.fsa</cite> and
<cite>lats</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="top-k">
<h3>top_k<a class="headerlink" href="#top-k" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Nbest.top_k">
<span class="sig-prename descclassname"><span class="pre">Nbest.</span></span><span class="sig-name descname"><span class="pre">top_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/nbest.py#L254-L292"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Nbest.top_k" title="Permalink to this definition"></a></dt>
<dd><p>Get a subset of paths in the Nbest. The resulting Nbest is regular
in that each sequence (i.e., utterance) has the same number of
paths (k).</p>
<p>We select the top-k paths according to the total_scores of each path.
If a utterance has less than k paths, then its last path, after sorting
by tot_scores in descending order, is repeated so that each utterance
has exactly k paths.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>k</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of paths in each utterance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Nbest</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return a new Nbest with a regular shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="total-scores">
<h3>total_scores<a class="headerlink" href="#total-scores" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.Nbest.total_scores">
<span class="sig-prename descclassname"><span class="pre">Nbest.</span></span><span class="sig-name descname"><span class="pre">total_scores</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/nbest.py#L240-L252"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.Nbest.total_scores" title="Permalink to this definition"></a></dt>
<dd><p>Get total scores of the FSAs in this Nbest.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since FSAs in Nbest are just linear FSAs, log-semirng and tropical
semiring produce the same total scores.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor with two axes [utt][path_scores].</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="onlinedenseintersecter">
<h2>OnlineDenseIntersecter<a class="headerlink" href="#onlinedenseintersecter" title="Permalink to this headline"></a></h2>
<section id="id17">
<h3>__init__<a class="headerlink" href="#id17" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.OnlineDenseIntersecter.__init__">
<span class="sig-prename descclassname"><span class="pre">OnlineDenseIntersecter.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decoding_graph</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_streams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search_beam</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_beam</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_active_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_active_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_partial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/online_dense_intersecter.py#L30-L106"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.OnlineDenseIntersecter.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Create a new online intersecter object.
:type decoding_graph: <code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>
:param decoding_graph: The decoding graph used in this intersecter.
:type num_streams: <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>
:param num_streams: How many streams can this intersecter handle parallelly.
:type search_beam: <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>
:param search_beam: Decoding beam, e.g. 20.  Smaller is faster, larger is more exact</p>
<blockquote>
<div><p>(less pruning). This is the default value; it may be modified by
<code class="docutils literal notranslate"><span class="pre">min_active_states</span></code> and <code class="docutils literal notranslate"><span class="pre">max_active_states</span></code>.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_beam</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Pruning beam for the output of intersection (vs. best path);
equivalent to kaldi’s lattice-beam.  E.g. 8.</p></li>
<li><p><strong>min_active_states</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Minimum number of FSA states that are allowed to be active on any
given frame for any given intersection/composition task. This is
advisory, in that it will try not to have fewer than this number
active. Set it to zero if there is no constraint.</p></li>
<li><p><strong>max_active_states</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Maximum number of FSA states that are allowed to be active on any
given frame for any given intersection/composition task. This is
advisory, in that it will try not to exceed that but may not always
succeed. You can use a very large number if no constraint is needed.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
</dd></dl>

</section>
<section id="decode">
<h3>decode<a class="headerlink" href="#decode" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.OnlineDenseIntersecter.decode">
<span class="sig-prename descclassname"><span class="pre">OnlineDenseIntersecter.</span></span><span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dense_fsas</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decode_states</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/online_dense_intersecter.py#L112-L141"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.OnlineDenseIntersecter.decode" title="Permalink to this definition"></a></dt>
<dd><p>Does intersection/composition for current chunk of nnet_output(given
by a DenseFsaVec), sequences in every chunk may come from different
sources.
:type dense_fsas: <code class="xref py py-class docutils literal notranslate"><span class="pre">DenseFsaVec</span></code>
:param dense_fsas: The neural-net output, with each frame containing the log-likes of</p>
<blockquote>
<div><p>each modeling unit.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>decode_states</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DecodeStateInfo</span></code>]) – A list of history decoding states for current batch of sequences,
its length equals to <code class="docutils literal notranslate"><span class="pre">dense_fsas.dim0()</span></code> (i.e. batch size).
Each element in <code class="docutils literal notranslate"><span class="pre">decode_states</span></code> belongs to the sequence at the
corresponding position in current batch.
For a new sequence(i.e. has no history states), just put <code class="docutils literal notranslate"><span class="pre">None</span></code>
at the corresponding position.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DecodeStateInfo</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return a tuple containing an Fsa and a List of new decoding states.
The Fsa which has 3 axes(i.e. (batch, state, arc)) contains the output
lattices. See the example in the constructor to get more info about
how to use the list of new decoding states.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="num-streams">
<h3>num_streams<a class="headerlink" href="#num-streams" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.OnlineDenseIntersecter.num_streams">
<span class="sig-prename descclassname"><span class="pre">OnlineDenseIntersecter.</span></span><span class="sig-name descname"><span class="pre">num_streams</span></span><a class="headerlink" href="#k2.OnlineDenseIntersecter.num_streams" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="raggedshape">
<h2>RaggedShape<a class="headerlink" href="#raggedshape" title="Permalink to this headline"></a></h2>
<section id="eq">
<h3>__eq__<a class="headerlink" href="#eq" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.__eq__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__eq__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#k2.RaggedShape.__eq__" title="Permalink to this definition"></a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if two shapes are equal. Otherwise, return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two shapes have to be on the same device. Otherwise, it throws
an exception.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">==</span> <span class="n">shape2</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span> <span class="o">==</span> <span class="n">shape2</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The shape that we want to compare with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the two shapes are the same.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id18">
<h3>__getitem__<a class="headerlink" href="#id18" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.__getitem__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__getitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">i</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.RaggedShape.__getitem__" title="Permalink to this definition"></a></dt>
<dd><p>Select the i-th sublist along axis 0.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It requires that this shape has at least 3 axes.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [x x]] [[x x x] [] [x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">[ [ x ] [ x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">[ [ x x x ] [ ] [ x x ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>i</strong> – The i-th sublist along axis 0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new ragged shape with one fewer axis.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id19">
<h3>__init__<a class="headerlink" href="#id19" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.__init__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#k2.RaggedShape.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Construct a ragged shape from a string.</p>
<p>An example string for a ragged shape with 2 axes is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="n">x</span> <span class="n">x</span><span class="p">]</span> <span class="p">[</span> <span class="p">]</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>An example string for a ragged shape with 3 axes is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[[</span><span class="n">x</span><span class="p">]</span> <span class="p">[]]</span> <span class="p">[[</span><span class="n">x</span><span class="p">]</span> <span class="p">[</span><span class="n">x</span> <span class="n">x</span><span class="p">]]</span> <span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [] [x x]] [[]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span>
<span class="go">[ [ [ x ] [ ] [ x x ] ] [ [ ] ] ]</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="ne">
<h3>__ne__<a class="headerlink" href="#ne" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.__ne__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__ne__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#k2.RaggedShape.__ne__" title="Permalink to this definition"></a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if two shapes are not equal. Otherwise, return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two shapes have to be on the same device. Otherwise, it throws
an exception.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">!=</span> <span class="n">shape2</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">!=</span> <span class="n">shape3</span>
<span class="go">False</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The shape that we want to compare with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the two shapes are not equal.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="repr">
<h3>__repr__<a class="headerlink" href="#repr" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.__repr__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__repr__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.RaggedShape.__repr__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x ] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id20">
<h3>__str__<a class="headerlink" href="#id20" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.__str__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.RaggedShape.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x ] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id21">
<h3>compose<a class="headerlink" href="#id21" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.compose">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">compose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.RaggedShape.compose" title="Permalink to this definition"></a></dt>
<dd><p>Compose <code class="docutils literal notranslate"><span class="pre">self</span></code> with a given shape.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">other</span></code> and <code class="docutils literal notranslate"><span class="pre">self</span></code> MUST be on the same device.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>In order to compose <code class="docutils literal notranslate"><span class="pre">self</span></code> with <code class="docutils literal notranslate"><span class="pre">other</span></code>, it has to
satisfy <code class="docutils literal notranslate"><span class="pre">self.tot_size(self.num_axes</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">==</span> <span class="pre">other.dim0</span></code></p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x x] [x x] [] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="n">shape2</span><span class="p">)</span>
<span class="go">[ [ [ x x x ] [ x x ] ] [ [ ] ] ]</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x x x] []] [[x] [x x x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x x x] [] [] [x x] [x] [] [x x x x] [] [x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="n">shape2</span><span class="p">)</span>
<span class="go">[ [ [ [ x ] [ x x x ] ] [ [ ] [ ] [ x x ] ] [ ] ] [ [ [ x ] ] [ [ ] [ x x x x ] [ ] [ x x ] ] ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="n">shape1</span><span class="o">.</span><span class="n">num_axes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">10</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The other shape that is to be composed with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a composed ragged shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-layer">
<h3>get_layer<a class="headerlink" href="#get-layer" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.get_layer">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">get_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.RaggedShape.get_layer" title="Permalink to this definition"></a></dt>
<dd><p>Returns a <cite>sub-shape</cite> of <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x] []] [[] [x x x] [x]] [[]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">[ [ x x x ] [ x x x ] [ x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">[ [ x x ] [ x ] [ ] [ ] [ x x x ] [ x ] [ ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>layer</strong> – Layer that is desired, from <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">..</span> <span class="pre">src.num_axes</span> <span class="pre">-</span> <span class="pre">2</span></code> (inclusive).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This returned shape will have <code class="docutils literal notranslate"><span class="pre">num_axes</span> <span class="pre">==</span> <span class="pre">2</span></code>, the minimal case of
a <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedShape</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="index">
<h3>index<a class="headerlink" href="#index" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.index">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_value_indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedShape</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.RaggedShape.index" title="Permalink to this definition"></a></dt>
<dd><p>Indexing operation on a ragged shape, returns <code class="docutils literal notranslate"><span class="pre">self[indexes]</span></code>, where elements
of <code class="docutils literal notranslate"><span class="pre">indexes</span></code> are interpreted as indexes into axis <code class="docutils literal notranslate"><span class="pre">axis</span></code> of self``.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">indexes</span></code> is a 1-D tensor and <code class="docutils literal notranslate"><span class="pre">indexes.dtype</span> <span class="pre">==</span> <span class="pre">torch.int32</span></code>.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [x] [x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span>
<span class="go">[ [ 0 10 ] [ 20 ] [ 30 40 50 ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub_shape</span><span class="p">,</span> <span class="n">value_indexes</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">indexes</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub_shape</span>
<span class="go">[ [ x x ] [ x x x ] [ x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_indexes</span>
<span class="go">tensor([0, 1, 3, 4, 5, 2], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">value_indexes</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([ 0., 10., 30., 40., 50., 20.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub_shape2</span><span class="p">,</span> <span class="n">value_indexes2</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">indexes</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub_shape2</span>
<span class="go">[ [ x x ] [ ] [ x ] [ x x ] [ x x x ] [ ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_indexes2</span>
<span class="go">tensor([0, 1, 2, 0, 1, 3, 4, 5], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x]] [[] [x x x] [x]] [[x] [] [] [x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">indexes</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
<span class="go">([ [ [ x x ] [ x ] ] [ [ x x x ] ] [ [ x ] [ ] [ x x ] ] ], tensor([0, 1, 2, 3, 4, 5, 7, 8, 9], dtype=torch.int32))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – The axis to be indexed. Must satisfy <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">axis</span> <span class="pre">&lt;</span> <span class="pre">self.num_axes</span></code>.</p></li>
<li><p><strong>indexes</strong> – Array of indexes, which will be interpreted as indexes into axis <code class="docutils literal notranslate"><span class="pre">axis</span></code>
of <code class="docutils literal notranslate"><span class="pre">self</span></code>, i.e. with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">indexes[i]</span> <span class="pre">&lt;</span> <span class="pre">self.tot_size(axis)</span></code>.
Note that if <code class="docutils literal notranslate"><span class="pre">axis</span></code> is 0, then -1 is also a valid entry in <code class="docutils literal notranslate"><span class="pre">index</span></code>,
in which case, an empty list is returned.</p></li>
<li><p><strong>need_value_indexes</strong> – <p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, it will return a torch.Tensor containing the indexes into
<code class="docutils literal notranslate"><span class="pre">ragged_tensor.data</span></code> that <code class="docutils literal notranslate"><span class="pre">ans.data</span></code> has, as in
<code class="docutils literal notranslate"><span class="pre">ans.data</span> <span class="pre">=</span> <span class="pre">ragged_tensor.data[value_indexes]</span></code>, where <code class="docutils literal notranslate"><span class="pre">ragged_tensor</span></code>
uses <code class="docutils literal notranslate"><span class="pre">self</span></code> as its shape.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It is currently not allowed to change the order on axes less than
<code class="docutils literal notranslate"><span class="pre">axis</span></code>, i.e. if <code class="docutils literal notranslate"><span class="pre">axis</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>, we require:
<code class="docutils literal notranslate"><span class="pre">IsMonotonic(self.row_ids(axis)[indexes])</span></code>.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return an indexed ragged shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="max-size">
<h3>max_size<a class="headerlink" href="#max-size" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.max_size">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">max_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.RaggedShape.max_size" title="Permalink to this definition"></a></dt>
<dd><p>Return the maximum number of elements of any sublist at the given axis.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [] [x] [x x] [x x x] [x x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">max_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x] [] [] []] [[x]] [[x x x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">max_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">max_size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">4</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> – <p>Compute the max size of this axis.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">axis</span></code> has to be greater than 0.</p>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return the maximum number of elements of sublists at the given <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="numel">
<h3>numel<a class="headerlink" href="#numel" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.numel">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">numel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.RaggedShape.numel" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of elements in this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x x x x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x] [] [] []] [[x]] [[x x x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">4</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Return the number of elements in this shape.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>It’s the number of <code class="docutils literal notranslate"><span class="pre">x</span></code>’s.</p>
</div>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="regular-ragged-shape">
<h3>regular_ragged_shape<a class="headerlink" href="#regular-ragged-shape" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.regular_ragged_shape">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">regular_ragged_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.RaggedShape.regular_ragged_shape" title="Permalink to this definition"></a></dt>
<dd><p>Create a ragged shape with 2 axes that has a regular structure.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="o">.</span><span class="n">regular_ragged_shape</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span>
<span class="go">[ [ x x x ] [ x x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">regular_ragged_shape</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span>
<span class="go">[ [ x x ] [ x x ] [ x x ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim0</strong> – Number of entries at axis 0.</p></li>
<li><p><strong>dim1</strong> – Number of entries in each sublist at axis 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged shape on CPU.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="remove-axis">
<h3>remove_axis<a class="headerlink" href="#remove-axis" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.remove_axis">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">remove_axis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.RaggedShape.remove_axis" title="Permalink to this definition"></a></dt>
<dd><p>Remove a certain axis.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">self.num_axes</span></code> MUST be greater than 2.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [] [x x]] [[x x x] [x x x x]] [[] [] []]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">[ [ x ] [ ] [ x x ] [ x x x ] [ x x x x ] [ ] [ ] [ ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">[ [ x x x ] [ x x x x x x x ] [ ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> – The axis to be removed.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged shape with one fewer axis.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="row-ids">
<h3>row_ids<a class="headerlink" href="#row-ids" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.row_ids">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">row_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.RaggedShape.row_ids" title="Permalink to this definition"></a></dt>
<dd><p>Return the row ids of a certain <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [] [x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 0, 2, 2, 2], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [] [x x]] [[x x x] [x] [x x x x] [] []] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([0, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – The axis whose row ids is to be returned.</p></li>
<li><p><strong>Hint</strong> – <code class="docutils literal notranslate"><span class="pre">axis</span> <span class="pre">&gt;=</span> <span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return the row ids of the given <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="row-splits">
<h3>row_splits<a class="headerlink" href="#row-splits" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.row_splits">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">row_splits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.RaggedShape.row_splits" title="Permalink to this definition"></a></dt>
<dd><p>Return the row splits of a certain <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [] [x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 2, 2, 5], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [] [x x]] [[x x x] [x] [x x x x] [] []] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 3, 8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([ 0,  1,  1,  3,  6,  7, 11, 11, 11], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – The axis whose row splits is to be returned.</p></li>
<li><p><strong>Hint</strong> – <code class="docutils literal notranslate"><span class="pre">axis</span> <span class="pre">&gt;=</span> <span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return the row splits of the given <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id22">
<h3>to<a class="headerlink" href="#id22" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.to">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.RaggedShape.to" title="Permalink to this definition"></a></dt>
<dd><p>Move this shape to the specified device.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If the shape is already on the specified device, the returned shape
shares the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[[x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span>
<span class="go">[ [ x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span>
<span class="go">[ [ x ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – An instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code>. It can be either a CPU device or
a CUDA device.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a shape on the given device.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="tot-size">
<h3>tot_size<a class="headerlink" href="#tot-size" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.tot_size">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">tot_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.RaggedShape.tot_size" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of elements at a certain``axis``.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x x] [x x x] []]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x]] [[x x]] [[x x x]] [[]] [[]] [[]] [[]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x]] [[x x]] [[x x x]] [[]] [[]] [[]] [[] []] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">6</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> – Return the number of elements for this <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return the number of elements at <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="tot-sizes">
<h3>tot_sizes<a class="headerlink" href="#tot-sizes" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedShape.tot_sizes">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">tot_sizes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span></span></span><a class="headerlink" href="#k2.RaggedShape.tot_sizes" title="Permalink to this definition"></a></dt>
<dd><p>Return total sizes of every axis in a tuple.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [ ] [x x x x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">tot_sizes</span><span class="p">()</span>
<span class="go">(3, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] []] [[x x x x]]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_sizes</span><span class="p">()</span>
<span class="go">(2, 3, 5)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return a tuple containing the total sizes of each axis.
<code class="docutils literal notranslate"><span class="pre">ans[i]</span></code> is the total size of axis <code class="docutils literal notranslate"><span class="pre">i</span></code> (for <code class="docutils literal notranslate"><span class="pre">i</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>).
For <code class="docutils literal notranslate"><span class="pre">i=0</span></code>, it is the <code class="docutils literal notranslate"><span class="pre">dim0</span></code> of this shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id23">
<h3>device<a class="headerlink" href="#id23" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedShape.device">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#k2.RaggedShape.device" title="Permalink to this definition"></a></dt>
<dd><p>Return the device of this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[[]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="dim0">
<h3>dim0<a class="headerlink" href="#dim0" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedShape.dim0">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">dim0</span></span><a class="headerlink" href="#k2.RaggedShape.dim0" title="Permalink to this definition"></a></dt>
<dd><p>Return number of sublists at axis 0.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x x x x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] []] [[]] [[x] [x x] [x x x]] [[]]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">4</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="num-axes">
<h3>num_axes<a class="headerlink" href="#num-axes" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedShape.num_axes">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">num_axes</span></span><a class="headerlink" href="#k2.RaggedShape.num_axes" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of axes of this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[[] []]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[]] [[]]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<section id="raggedtensor">
<h2>RaggedTensor<a class="headerlink" href="#raggedtensor" title="Permalink to this headline"></a></h2>
<section id="id24">
<h3>__eq__<a class="headerlink" href="#id24" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.__eq__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__eq__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#k2.RaggedTensor.__eq__" title="Permalink to this definition"></a></dt>
<dd><p>Compare two ragged tensors.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two tensors MUST have the same dtype. Otherwise,
it throws an exception.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">==</span>  <span class="n">b</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">try</span><span class="p">:</span>
<span class="gp">... </span>  <span class="n">c</span> <span class="o">==</span> <span class="n">b</span>
<span class="gp">... </span><span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;raised exception&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The tensor to be compared.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the two tensors are equal.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id25">
<h3>__getitem__<a class="headerlink" href="#id25" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.__getitem__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__getitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.RaggedTensor.__getitem__" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>__getitem__(self: _k2.ragged.RaggedTensor, i: int) -&gt; object</p></li>
</ol>
<p>Select the i-th sublist along axis 0.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Support for autograd is to be implemented.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1 3] [] [9]]  [[8]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1, 3],</span>
<span class="go">               [],</span>
<span class="go">               [9]],</span>
<span class="go">              [[8]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">RaggedTensor([[8]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [1 3] [9] [8] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              [9],</span>
<span class="go">              [8]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">tensor([1, 3], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">tensor([9], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>i</strong> – The i-th sublist along axis 0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new ragged tensor with one fewer axis. If <cite>num_axes == 2</cite>, the
return value will be a 1D tensor.</p>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>__getitem__(self: _k2.ragged.RaggedTensor, key: slice) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Slices sublists along axis 0 with the given range. Only support slicing step
equals to 1.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Support for autograd is to be implemented.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1 3] [] [9]]  [[8]] [[10 11]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1, 3],</span>
<span class="go">               [],</span>
<span class="go">               [9]],</span>
<span class="go">              [[8]],</span>
<span class="go">              [[10, 11]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="go">RaggedTensor([[[1, 3],</span>
<span class="go">               [],</span>
<span class="go">               [9]],</span>
<span class="go">              [[8]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="go">RaggedTensor([[[8]]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – Slice containing integer constants.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new ragged tensor with the same axes as original ragged tensor, but
only contains the sublists within the range.</p>
</dd>
</dl>
<ol class="arabic simple" start="3">
<li><p>__getitem__(self: _k2.ragged.RaggedTensor, key: torch.Tensor) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Slice a ragged tensor along axis 0 using a 1-D torch.int32 tensor.</p>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span> <span class="p">[</span><span class="mi">300</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">tensor([1, 2, 0], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="go">RaggedTensor([[300],</span>
<span class="go">              [-10, 0, -1],</span>
<span class="go">              [10, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">tensor([0, 1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="go">RaggedTensor([[10, 20],</span>
<span class="go">              [300]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="go">tensor([2, 3], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
<span class="go">RaggedTensor([[-10, 0, -1],</span>
<span class="go">              [-2, 4, 5]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [2, 3],</span>
<span class="go">               [0]],</span>
<span class="go">              [[10, 20]],</span>
<span class="go">              [[],</span>
<span class="go">               [2]],</span>
<span class="go">              [[1],</span>
<span class="go">               [2, 3],</span>
<span class="go">               [0]]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – A 1-D torch.int32 tensor containing the indexes to select along
axis 0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new ragged tensor with the same number of axes as <code class="docutils literal notranslate"><span class="pre">self</span></code> but
only contains the specified sublists.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="getstate">
<h3>__getstate__<a class="headerlink" href="#getstate" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.__getstate__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__getstate__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">k2.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span></span></span><a class="headerlink" href="#k2.RaggedTensor.__getstate__" title="Permalink to this definition"></a></dt>
<dd><p>Requires a tensor with 2 axes or 3 axes. Other number
of axes are not implemented yet.</p>
<p>This method is to support <code class="docutils literal notranslate"><span class="pre">pickle</span></code>, e.g., used by <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code>.
You are not expected to call it by yourself.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If this tensor has 2 axes, return a tuple containing
(self.row_splits(1), “row_ids1”, self.values).
If this tensor has 3 axes, return a tuple containing
(self.row_splits(1), “row_ids1”, self.row_splits(1),
“row_ids2”, self.values)</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>“row_ids1” and “row_ids2” in the returned value is for
backward compatibility.</p>
</div>
</dd></dl>

</section>
<section id="id26">
<h3>__init__<a class="headerlink" href="#id26" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.__init__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.RaggedTensor.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>__init__(self: _k2.ragged.RaggedTensor, data: list, dtype: object = None, device: object = ‘cpu’) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor with arbitrary number of axes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A ragged tensor has at least two axes.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              []], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [2, 3]],</span>
<span class="go">              [[4],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[]]</span> <span class="p">])</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [],</span>
<span class="go">              [[]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[]]</span> <span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [],</span>
<span class="go">              [[]]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – A list-of sublist(s) of integers or real numbers.
It can have arbitrary number of axes (at least two).</p></li>
<li><p><strong>dtype</strong> – Optional. If None, it infers the dtype from <code class="docutils literal notranslate"><span class="pre">data</span></code>
automatically, which is either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>__init__(self: _k2.ragged.RaggedTensor, data: list, dtype: object = None, device: str = ‘cpu’) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor with arbitrary number of axes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A ragged tensor has at least two axes.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              []], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [2, 3]],</span>
<span class="go">              [[4],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[]]</span> <span class="p">])</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [],</span>
<span class="go">              [[]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[]]</span> <span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [],</span>
<span class="go">              [[]]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – A list-of sublist(s) of integers or real numbers.
It can have arbitrary number of axes (at least two).</p></li>
<li><p><strong>dtype</strong> – Optional. If None, it infers the dtype from <code class="docutils literal notranslate"><span class="pre">data</span></code>
automatically, which is either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="3">
<li><p>__init__(self: _k2.ragged.RaggedTensor, s: str, dtype: object = None, device: object = ‘cpu’) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor from its string representation.</p>
<p>Fields are separated by space(s) <strong>or</strong> comma(s).</p>
<p>An example string for a 2-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>An example string for a 3-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]]</span> <span class="p">[[]]</span> <span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [1] [] [3 4] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [3, 4]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[] [3]]  [[10]] ]&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[],</span>
<span class="go">               [3]],</span>
<span class="go">              [[10]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1]], device=&#39;cuda:0&#39;, dtype=torch.float32)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Number of spaces or commas in <code class="docutils literal notranslate"><span class="pre">s</span></code> does not affect the result.
Of course, numbers have to be separated by at least one space or comma.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> – A string representation of a ragged tensor.</p></li>
<li><p><strong>dtype</strong> – The desired dtype of the tensor. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it tries
to infer the correct dtype from <code class="docutils literal notranslate"><span class="pre">s</span></code>, which is assumed to be
either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are:
<code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="4">
<li><p>__init__(self: _k2.ragged.RaggedTensor, s: str, dtype: object = None, device: str = ‘cpu’) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor from its string representation.</p>
<p>Fields are separated by space(s) <strong>or</strong> comma(s).</p>
<p>An example string for a 2-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>An example string for a 3-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]]</span> <span class="p">[[]]</span> <span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [1] [] [3 4] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [3, 4]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[] [3]]  [[10]] ]&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[],</span>
<span class="go">               [3]],</span>
<span class="go">              [[10]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1]], device=&#39;cuda:0&#39;, dtype=torch.float32)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Number of spaces or commas in <code class="docutils literal notranslate"><span class="pre">s</span></code> does not affect the result.
Of course, numbers have to be separated by at least one space or comma.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> – A string representation of a ragged tensor.</p></li>
<li><p><strong>dtype</strong> – The desired dtype of the tensor. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it tries
to infer the correct dtype from <code class="docutils literal notranslate"><span class="pre">s</span></code>, which is assumed to be
either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are:
<code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="5">
<li><p>__init__(self: _k2.ragged.RaggedTensor, shape: _k2.ragged.RaggedShape, value: torch.Tensor) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor from a shape and a value.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [] [x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span>
<span class="go">RaggedTensor([[10, 0],</span>
<span class="go">              [],</span>
<span class="go">              [20, 30, 40]], dtype=torch.float32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> – The shape of the tensor.</p></li>
<li><p><strong>value</strong> – The value of the tensor.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="6">
<li><p>__init__(self: _k2.ragged.RaggedTensor, tensor: torch.Tensor) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor from a torch tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It turns a regular tensor into a ragged tensor.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The input tensor has to have more than 1 dimension.
That is <code class="docutils literal notranslate"><span class="pre">tensor.ndim</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p>
<p>Also, if the input tensor is contiguous, <code class="docutils literal notranslate"><span class="pre">self</span></code>
will share the underlying memory with it. Otherwise,
memory of the input tensor is copied to create <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>Supported dtypes of the input tensor are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 1, 2],</span>
<span class="go">        [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 1, 2],</span>
<span class="go">              [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[10, 1, 2],</span>
<span class="go">              [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[10, -2,  2],</span>
<span class="go">        [ 3,  4,  5]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">)[:,</span> <span class="p">::</span><span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0,  4,  8],</span>
<span class="go">        [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 4, 8],</span>
<span class="go">              [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 4, 8],</span>
<span class="go">              [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[10,  4,  8],</span>
<span class="go">        [12, 16, 20]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 3</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[[ 0.,  1.,  2.,  3.],</span>
<span class="go">         [ 4.,  5.,  6.,  7.],</span>
<span class="go">         [ 8.,  9., 10., 11.]],</span>
<span class="go">        [[12., 13., 14., 15.],</span>
<span class="go">         [16., 17., 18., 19.],</span>
<span class="go">         [20., 21., 22., 23.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[0, 1, 2, 3],</span>
<span class="go">               [4, 5, 6, 7],</span>
<span class="go">               [8, 9, 10, 11]],</span>
<span class="go">              [[12, 13, 14, 15],</span>
<span class="go">               [16, 17, 18, 19],</span>
<span class="go">               [20, 21, 22, 23]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 2]], device=&#39;cuda:0&#39;, dtype=torch.float32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – An N-D (N &gt; 1) tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id27">
<h3>__ne__<a class="headerlink" href="#id27" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.__ne__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__ne__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#k2.RaggedTensor.__ne__" title="Permalink to this definition"></a></dt>
<dd><p>Compare two ragged tensors.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two tensors MUST have the same dtype. Otherwise,
it throws an exception.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">!=</span> <span class="n">a</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">!=</span> <span class="n">a</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The tensor to be compared.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the two tensors are NOT equal.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id28">
<h3>__repr__<a class="headerlink" href="#id28" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.__repr__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__repr__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.RaggedTensor.__repr__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [2, 3],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&#39;RaggedTensor([[1],\n              [2, 3],\n              []], dtype=torch.int32)&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 2]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="setstate">
<h3>__setstate__<a class="headerlink" href="#setstate" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.__setstate__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__setstate__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">k2.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#k2.RaggedTensor.__setstate__" title="Permalink to this definition"></a></dt>
<dd><p>Set the content of this class from <code class="docutils literal notranslate"><span class="pre">arg0</span></code>.</p>
<p>This method is to support <code class="docutils literal notranslate"><span class="pre">pickle</span></code>, e.g., used by torch.load().
You are not expected to call it by yourself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>arg0</strong> – It is the return value from the method <code class="docutils literal notranslate"><span class="pre">__getstate__</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id29">
<h3>__str__<a class="headerlink" href="#id29" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.__str__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.RaggedTensor.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [2, 3],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&#39;RaggedTensor([[1],\n              [2, 3],\n              []], dtype=torch.int32)&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 2]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="add">
<h3>add<a class="headerlink" href="#add" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.add">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.add" title="Permalink to this definition"></a></dt>
<dd><p>Add value scaled by alpha to source ragged tensor over the last axis.</p>
<p>It implements:</p>
<blockquote>
<div><p>dest[…][i][j] = src[…][i][j] + alpha * value[i]</p>
</div></blockquote>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[2, 4],</span>
<span class="go">              [3],</span>
<span class="go">              [5, 11]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[0, 2],</span>
<span class="go">              [-1],</span>
<span class="go">              [-1, 5]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> – The value to be added to the <code class="docutils literal notranslate"><span class="pre">self</span></code>, whose dimension MUST
equal the number of sublists along the last dimension of <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p></li>
<li><p><strong>alpha</strong> – The number used to scaled value before adding to <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a new RaggedTensor, sharing the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="arange">
<h3>arange<a class="headerlink" href="#arange" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.arange">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">arange</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">begin</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.arange" title="Permalink to this definition"></a></dt>
<dd><p>Return a sub-range of <code class="docutils literal notranslate"><span class="pre">self</span></code> containing indexes <code class="docutils literal notranslate"><span class="pre">begin</span></code>
through <code class="docutils literal notranslate"><span class="pre">end</span> <span class="pre">-</span> <span class="pre">1</span></code> along axis <code class="docutils literal notranslate"><span class="pre">axis</span></code> of <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">axis</span></code> argument may be confusing; its behavior is equivalent to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
  <span class="bp">self</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The returned tensor shares the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</div>
<p><strong>Example 1</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [],</span>
<span class="go">               [2]],</span>
<span class="go">              [[],</span>
<span class="go">               [4, 5],</span>
<span class="go">               []],</span>
<span class="go">              [[],</span>
<span class="go">               [1]],</span>
<span class="go">              [[]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[],</span>
<span class="go">               [4, 5],</span>
<span class="go">               []],</span>
<span class="go">              [[],</span>
<span class="go">               [1]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[[],</span>
<span class="go">               [4, 5],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [2],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([[2],</span>
<span class="go">              [],</span>
<span class="go">              [4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[]]],</span> <span class="p">[[[],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[[],</span>
<span class="go">                [1],</span>
<span class="go">                [2, 3]],</span>
<span class="go">               [[5, 8],</span>
<span class="go">                [],</span>
<span class="go">                [9]]],</span>
<span class="go">              [[[10],</span>
<span class="go">                [0],</span>
<span class="go">                []]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[[5, 8],</span>
<span class="go">               [],</span>
<span class="go">               [9]],</span>
<span class="go">              [[10],</span>
<span class="go">               [0],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[],</span>
<span class="go">              [1],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [5, 8],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
</pre></div>
</div>
<p><strong>Example 3</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[0],</span>
<span class="go">              [1],</span>
<span class="go">              [2],</span>
<span class="go">              [],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[0],</span>
<span class="go">              [-1],</span>
<span class="go">              [2],</span>
<span class="go">              [],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – The axis from which <code class="docutils literal notranslate"><span class="pre">begin</span></code> and <code class="docutils literal notranslate"><span class="pre">end</span></code> correspond to.</p></li>
<li><p><strong>begin</strong> – The beginning of the range (inclusive).</p></li>
<li><p><strong>end</strong> – The end of the range (exclusive).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="argmax">
<h3>argmax<a class="headerlink" href="#argmax" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.argmax">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">argmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.argmax" title="Permalink to this definition"></a></dt>
<dd><p>Return a tensor containing maximum value indexes within each sub-list along the
last axis of <code class="docutils literal notranslate"><span class="pre">self</span></code>, i.e. the max taken over the last axis, The index is -1
if the sub-list was empty or all values in the sub-list are less
than <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="go">tensor([ 0, -1, -1, -1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([ 0, -1, -1, -1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="go">tensor([ 3, -1,  7], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">tensor([ 3, -1,  7], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span>
<span class="go">(tensor(5, dtype=torch.int32), tensor(8, dtype=torch.int32))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="go">tensor([-1, -1,  7], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([ 3, -1,  7], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([ 3, -1,  7], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – A base value to compare. If values in a sublist are all less
than this value, then the <code class="docutils literal notranslate"><span class="pre">argmax</span></code> of this sublist is -1.
If a sublist is empty, the <code class="docutils literal notranslate"><span class="pre">argmax</span></code> of it is also -1.
If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the lowest value of <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code> is used.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a 1-D <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> tensor. It is on the same device
as <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id30">
<h3>cat<a class="headerlink" href="#id30" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.cat">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">cat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">srcs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedTensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.cat" title="Permalink to this definition"></a></dt>
<dd><p>Concatenate a list of ragged tensor over a specified axis.</p>
<p><strong>Example 1</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3, 2, 3]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              [],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [],</span>
<span class="go">              [9],</span>
<span class="go">              [0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [],</span>
<span class="go">              [-1],</span>
<span class="go">              [10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1, 3, 0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [-1],</span>
<span class="go">              [9, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 3, 0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [-1],</span>
<span class="go">              [9, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">RaggedTensor([[0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [],</span>
<span class="go">              [-1],</span>
<span class="go">              [10],</span>
<span class="go">              [1, 3],</span>
<span class="go">              [],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>srcs</strong> – A list (or a tuple) of ragged tensors to concatenate. They <strong>MUST</strong> all
have the same dtype and on the same device.</p></li>
<li><p><strong>axis</strong> – Only 0 and 1 are supported right now. If it is 1, then
<code class="docutils literal notranslate"><span class="pre">srcs[i].dim0</span></code> must all have the same value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a concatenated tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="clone">
<h3>clone<a class="headerlink" href="#clone" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.clone">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">clone</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.clone" title="Permalink to this definition"></a></dt>
<dd><p>Return a copy of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[10, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[-1, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[10, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[10, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id31">
<h3>index<a class="headerlink" href="#id31" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.index">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.RaggedTensor.index" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>index(self: _k2.ragged.RaggedTensor, indexes: _k2.ragged.RaggedTensor) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Index a ragged tensor with a ragged tensor.</p>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mf">13.5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indexes</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span>
<span class="go">RaggedTensor([[[10, 11],</span>
<span class="go">               [12, 13.5]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="go">RaggedTensor([[[10, 11]],</span>
<span class="go">              [[12, 13.5]],</span>
<span class="go">              [[10, 11],</span>
<span class="go">               [10, 11]]], dtype=torch.float32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="go">RaggedTensor([[[[[1, 0],</span>
<span class="go">                 [],</span>
<span class="go">                 [2]],</span>
<span class="go">                [[1, 2],</span>
<span class="go">                 [-1]]],</span>
<span class="go">               [[[],</span>
<span class="go">                 [3],</span>
<span class="go">                 [0, 0, 1]]]],</span>
<span class="go">              [[[[1, 0],</span>
<span class="go">                 [],</span>
<span class="go">                 [2]]]]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>indexes</strong> – <p>Its values must satisfy <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">values[i]</span> <span class="pre">&lt;</span> <span class="pre">self.dim0</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Its dtype has to be <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>.</p>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return indexed tensor.</p>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>index(self: _k2.ragged.RaggedTensor, indexes: torch.Tensor, axis: int, need_value_indexes: bool = False) -&gt; Tuple[_k2.ragged.RaggedTensor, Optional[torch.Tensor]]</p></li>
</ol>
<p>Indexing operation on ragged tensor, returns <code class="docutils literal notranslate"><span class="pre">self[indexes]</span></code>, where
the elements of <code class="docutils literal notranslate"><span class="pre">indexes</span></code> are interpreted as indexes into axis <code class="docutils literal notranslate"><span class="pre">axis</span></code> of
<code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">indexes</span></code> is a 1-D tensor and <code class="docutils literal notranslate"><span class="pre">indexes.dtype</span> <span class="pre">==</span> <span class="pre">torch.int32</span></code>.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.25</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">value_indexes</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 1, 2],</span>
<span class="go">              [0, 2, 3],</span>
<span class="go">              [],</span>
<span class="go">              [3, -1.25]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_indexes</span>
<span class="go">tensor([3, 4, 5, 0, 1, 2, 6, 7], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">value_indexes</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([ 0.0000,  1.0000,  2.0000,  0.0000,  2.0000,  3.0000,  3.0000, -1.2500])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[0, 1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [0, 2, 3]], dtype=torch.float32), tensor([3, 4, 5, 0, 1, 2], dtype=torch.int32))</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="n">i</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([0, 0, 0, 1, 1, 1, 1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">value_indexes</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[1, 3],</span>
<span class="go">               [2],</span>
<span class="go">               []],</span>
<span class="go">              [[2],</span>
<span class="go">               [5, 8],</span>
<span class="go">               [-1],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_indexes</span>
<span class="go">tensor([0, 1, 2, 6, 3, 4, 5], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">value_indexes</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([ 1,  3,  2,  2,  5,  8, -1], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indexes</strong> – <p>Array of indexes, which will be interpreted as indexes into axis <code class="docutils literal notranslate"><span class="pre">axis</span></code>
of <code class="docutils literal notranslate"><span class="pre">self</span></code>, i.e. with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">indexes[i]</span> <span class="pre">&lt;</span> <span class="pre">self.tot_size(axis)</span></code>.
Note that if <code class="docutils literal notranslate"><span class="pre">axis</span></code> is 0, then -1 is also a valid entry in <code class="docutils literal notranslate"><span class="pre">index</span></code>,
-1 as an index, which will result in an empty list (as if it were the index
into a position in <code class="docutils literal notranslate"><span class="pre">self</span></code> that had an empty list at that point).</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It is currently not allowed to change the order on axes less than
<code class="docutils literal notranslate"><span class="pre">axis</span></code>, i.e. if <code class="docutils literal notranslate"><span class="pre">axis</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>, we require:
<code class="docutils literal notranslate"><span class="pre">IsMonotonic(self.shape.row_ids(axis)[indexes])</span></code>.</p>
</div>
</p></li>
<li><p><strong>axis</strong> – The axis to be indexed. Must satisfy <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">axis</span> <span class="pre">&lt;</span> <span class="pre">self.num_axes</span></code>.</p></li>
<li><p><strong>need_value_indexes</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, it will return a torch.Tensor containing the indexes into
<code class="docutils literal notranslate"><span class="pre">self.values</span></code> that <code class="docutils literal notranslate"><span class="pre">ans.values</span></code> has, as in
<code class="docutils literal notranslate"><span class="pre">ans.values</span> <span class="pre">=</span> <span class="pre">self.values[value_indexes]</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>A ragged tensor, sharing the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> if <code class="docutils literal notranslate"><span class="pre">need_value_indexes</span></code> is False; a 1-D torch.tensor of
dtype <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> containing the indexes into <code class="docutils literal notranslate"><span class="pre">self.values</span></code> that
<code class="docutils literal notranslate"><span class="pre">ans.values</span></code> has.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Return a tuple containing</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="logsumexp">
<h3>logsumexp<a class="headerlink" href="#logsumexp" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.logsumexp">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">logsumexp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-</span> <span class="pre">inf</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.logsumexp" title="Permalink to this definition"></a></dt>
<dd><p>Compute the logsumexp of sublists over the last axis of this tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is similar to torch.logsumexp except it accepts a ragged tensor.
See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logsumexp.html">https://pytorch.org/docs/stable/generated/torch.logsumexp.html</a>
for definition of logsumexp.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a sublist is empty, the logsumexp for it is the provided
<code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation only supports float type input,
i.e., with dtype being torch.float32 or torch.float64.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[-0.25, -0.25, -0.25, -0.25],</span>
<span class="go">              [],</span>
<span class="go">              [-0.5, -0.5]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([1.1363,   -inf, 0.1931], grad_fn=&lt;LogSumExpFunction&gt;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor(-inf, grad_fn=&lt;SumBackward0&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.5000, 0.5000])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># if a is a 3-d ragged tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[[-0.25, -0.25, -0.25, -0.25]],</span>
<span class="go">              [[],</span>
<span class="go">               [-0.5, -0.5]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([1.1363,   -inf, 0.1931], grad_fn=&lt;LogSumExpFunction&gt;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor(-inf, grad_fn=&lt;SumBackward0&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.5000, 0.5000])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – If a sublist is empty, its logsumexp is this value.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a 1-D tensor with the same dtype of this tensor
containing the computed logsumexp.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="max">
<h3>max<a class="headerlink" href="#max" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.max">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.max" title="Permalink to this definition"></a></dt>
<dd><p>Return a tensor containing the maximum of each sub-list along the last
axis of <code class="docutils literal notranslate"><span class="pre">self</span></code>. The max is taken over the last axis or <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>,
whichever was larger.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="go">tensor([          3,           5, -2147483648, -2147483648,           9,</span>
<span class="go">        -2147483648,           8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=-</span><span class="mi">10</span><span class="p">)</span>
<span class="go">tensor([  3,   5, -10, -10,   9, -10,   8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="go">tensor([7, 7, 7, 7, 9, 7, 8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([ 3.,  5., -3., -3.,  9., -3.,  8.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([ 3,  5, -2, -2,  9, -2,  8], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – The base value to compare. If values in a sublist are all less
than this value, then the max of this sublist is <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.
If a sublist is empty, its max is also <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return 1-D tensor containing the max value of each sublist.
It shares the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="min">
<h3>min<a class="headerlink" href="#min" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.min">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.min" title="Permalink to this definition"></a></dt>
<dd><p>Return a tensor containing the minimum of each sub-list along the last
axis of <code class="docutils literal notranslate"><span class="pre">self</span></code>. The min is taken over the last axis or <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>,
whichever was smaller.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="go">tensor([ 0.0000e+00, -1.0000e+00,  3.4028e+38,  3.4028e+38,  1.0000e+00,</span>
<span class="go">         3.4028e+38,  2.0000e+00])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
<span class="go">tensor([ 0., -1., inf, inf,  1., inf,  2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="go">tensor([  0.,  -1., 100., 100.,   1., 100.,   2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="go">tensor([ 0, -1, 20, 20,  1, 20,  2], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
<span class="go">tensor([ 0., -1., 15., 15.,  1., 15.,  2.], device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – The base value to compare. If values in a sublist are all larger
than this value, then the minimum of this sublist is <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.
If a sublist is empty, its minimum is also <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return 1-D tensor containing the minimum of each sublist.
It shares the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="normalize">
<h3>normalize<a class="headerlink" href="#normalize" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.normalize">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">normalize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_log</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.normalize" title="Permalink to this definition"></a></dt>
<dd><p>Normalize a ragged tensor over the last axis.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">use_log</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the normalization per sublist is done as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Compute the log sum per sublist</p></li>
</ol>
<p>2. Subtract the log sum computed above from the sublist and return
it</p>
</div></blockquote>
<p>If <code class="docutils literal notranslate"><span class="pre">use_log</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the normalization per sublist is done as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Compute the sum per sublist</p></li>
<li><p>Divide the sublist by the above sum and return the resulting sublist</p></li>
</ol>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a sublist contains 3 elements <code class="docutils literal notranslate"><span class="pre">[a,</span> <span class="pre">b,</span> <span class="pre">c]</span></code>, then the log sum
is defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
</pre></div>
</div>
<p>The resulting sublist looks like below if <code class="docutils literal notranslate"><span class="pre">use_log</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">a</span> <span class="o">-</span> <span class="n">s</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">s</span><span class="p">,</span> <span class="n">c</span> <span class="o">-</span> <span class="n">s</span><span class="p">]</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">use_log</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the resulting sublist looks like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">a</span><span class="o">/</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">),</span> <span class="n">b</span><span class="o">/</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">),</span> <span class="n">c</span><span class="o">/</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">use_log</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">RaggedTensor([[0.25, 0.75],</span>
<span class="go">              [],</span>
<span class="go">              [1],</span>
<span class="go">              [0.2, 0.8]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">use_log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[-0.798139, -0.598139],</span>
<span class="go">              [],</span>
<span class="go">              [0],</span>
<span class="go">              [-1.03749, -0.437488]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">use_log</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">RaggedTensor([[[0.25, 0.75],</span>
<span class="go">               []],</span>
<span class="go">              [[1],</span>
<span class="go">               [0.2, 0.8]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">use_log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[[-0.798139, -0.598139],</span>
<span class="go">               []],</span>
<span class="go">              [[0],</span>
<span class="go">               [-1.03749, -0.437488]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
<span class="go">tensor([-0.7981, -0.5981])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_log</strong> – It indicates which kind of normalization to be applied.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a 1-D tensor, sharing the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id32">
<h3>numel<a class="headerlink" href="#id32" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.numel">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">numel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.RaggedTensor.numel" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return number of elements in this tensor. It equals to
<code class="docutils literal notranslate"><span class="pre">self.values.numel()</span></code>.</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1] [] []]  [[2 3]]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1] [] [3 4 5 6]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">5</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="pad">
<h3>pad<a class="headerlink" href="#pad" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.pad">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">pad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.pad" title="Permalink to this definition"></a></dt>
<dd><p>Pad a ragged tensor with 2-axes to a 2-D torch tensor.</p>
<p>For example, if <code class="docutils literal notranslate"><span class="pre">self</span></code> has the following values:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span><span class="p">]</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>Then it returns a 2-D tensor as follows if <code class="docutils literal notranslate"><span class="pre">padding_value</span></code> is 0 and
mode is <code class="docutils literal notranslate"><span class="pre">constant</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
</pre></div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It requires that <code class="docutils literal notranslate"><span class="pre">self.num_axes</span> <span class="pre">==</span> <span class="pre">2</span></code>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1, -1, -1, -1, -1],</span>
<span class="go">        [-1, -1, -1, -1, -1],</span>
<span class="go">        [ 2,  3, -1, -1, -1],</span>
<span class="go">        [ 5,  8,  9,  8,  2]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;replicate&#39;</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1,  1,  1,  1,  1],</span>
<span class="go">        [-1, -1, -1, -1, -1],</span>
<span class="go">        [ 2,  3,  3,  3,  3],</span>
<span class="go">        [ 5,  8,  9,  8,  2]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> – Valid values are: <code class="docutils literal notranslate"><span class="pre">constant</span></code>, <code class="docutils literal notranslate"><span class="pre">replicate</span></code>. If it is
<code class="docutils literal notranslate"><span class="pre">constant</span></code>, the given <code class="docutils literal notranslate"><span class="pre">padding_value</span></code> is used for filling.
If it is <code class="docutils literal notranslate"><span class="pre">replicate</span></code>, the last entry in a list is used for filling.
If a list is empty, then the given <cite>padding_value</cite> is also used for filling.</p></li>
<li><p><strong>padding_value</strong> – The filling value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 2-D torch tensor, sharing the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id33">
<h3>remove_axis<a class="headerlink" href="#id33" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.remove_axis">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">remove_axis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.remove_axis" title="Permalink to this definition"></a></dt>
<dd><p>Remove an axis; if it is not the first or last axis, this is done by appending
lists (effectively the axis is combined with the following axis).  If it is the
last axis it is just removed and the number of elements may be changed.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The tensor has to have more than two axes.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [],</span>
<span class="go">               [0, -1]],</span>
<span class="go">              [[],</span>
<span class="go">               [2, 3],</span>
<span class="go">               []],</span>
<span class="go">              [[0]],</span>
<span class="go">              [[]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [0, -1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [],</span>
<span class="go">              [0],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 0, -1],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [0],</span>
<span class="go">              []], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[]]],</span> <span class="p">[[[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[[1],</span>
<span class="go">                [],</span>
<span class="go">                [2]]],</span>
<span class="go">              [[[3, 4],</span>
<span class="go">                [],</span>
<span class="go">                [5, 6],</span>
<span class="go">                []]],</span>
<span class="go">              [[[],</span>
<span class="go">                [0]]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [],</span>
<span class="go">               [2]],</span>
<span class="go">              [[3, 4],</span>
<span class="go">               [],</span>
<span class="go">               [5, 6],</span>
<span class="go">               []],</span>
<span class="go">              [[],</span>
<span class="go">               [0]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [],</span>
<span class="go">               [2]],</span>
<span class="go">              [[3, 4],</span>
<span class="go">               [],</span>
<span class="go">               [5, 6],</span>
<span class="go">               []],</span>
<span class="go">              [[],</span>
<span class="go">               [0]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [[3, 4, 5, 6]],</span>
<span class="go">              [[0]]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> – The axis to move.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor with one fewer axes.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="remove-values-eq">
<h3>remove_values_eq<a class="headerlink" href="#remove-values-eq" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.remove_values_eq">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">remove_values_eq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.remove_values_eq" title="Permalink to this definition"></a></dt>
<dd><p>Returns a ragged tensor after removing all ‘values’ that equal a provided
target.  Leaves all layers of the shape except for the last one unaffected.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2, 3, 0, 3, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3, 2, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_eq</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 2, 0, 2],</span>
<span class="go">              [],</span>
<span class="go">              [2],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_eq</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 3, 0, 3],</span>
<span class="go">              [],</span>
<span class="go">              [3, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> – The target value to delete.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor whose values don’t contain the <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="remove-values-leq">
<h3>remove_values_leq<a class="headerlink" href="#remove-values-leq" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.remove_values_leq">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">remove_values_leq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.remove_values_leq" title="Permalink to this definition"></a></dt>
<dd><p>Returns a ragged tensor after removing all ‘values’ that are
equal to or less than a provided cutoff.
Leaves all layers of the shape except for the last one unaffected.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2, 3, 0, 3, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3, 2, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_leq</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[],</span>
<span class="go">              [],</span>
<span class="go">              [],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_leq</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[3, 3],</span>
<span class="go">              [],</span>
<span class="go">              [3, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_leq</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[2, 3, 3, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3, 2, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cutoff</strong> – Values less than or equal to this <code class="docutils literal notranslate"><span class="pre">cutoff</span></code> are deleted.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor whose values are all above <code class="docutils literal notranslate"><span class="pre">cutoff</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id34">
<h3><a href="#id129"><span class="problematic" id="id130">requires_grad_</span></a><a class="headerlink" href="#id34" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.requires_grad_">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.requires_grad_" title="Permalink to this definition"></a></dt>
<dd><p>Change if autograd should record operations on this tensor: Set
this tensor’s <a class="reference internal" href="#k2.RaggedTensor.requires_grad" title="k2.RaggedTensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> attribute <strong>in-place</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this tensor is not a float tensor, PyTorch will throw a
RuntimeError exception.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This method ends with an underscore, meaning it changes this tensor
<strong>in-place</strong>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[1]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> – If autograd should record operations on this tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return this tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="sort">
<h3><a href="#id131"><span class="problematic" id="id132">sort_</span></a><a class="headerlink" href="#sort" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.sort_">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">sort_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">descending</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_new2old_indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.RaggedTensor.sort_" title="Permalink to this definition"></a></dt>
<dd><p>Sort a ragged tensor over the last axis <strong>in-place</strong>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">sort_</span></code> ends with an underscore, meaning this operation
changes <code class="docutils literal notranslate"><span class="pre">self</span></code> <strong>in-place</strong>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_clone</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sort_</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([1, 0, 2, 4, 5, 3, 7, 6, 8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[3, 1, 0],</span>
<span class="go">              [5, 3, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3, 1, 0]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_clone</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">b</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([3., 1., 0., 5., 3., 2., 3., 1., 0.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_clone</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sort_</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor([2, 1, 0, 5, 4, 3, 8, 7, 6], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[0, 1, 3],</span>
<span class="go">              [2, 3, 5],</span>
<span class="go">              [],</span>
<span class="go">              [0, 1, 3]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_clone</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([0., 1., 3., 2., 3., 5., 0., 1., 3.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>descending</strong> – <code class="docutils literal notranslate"><span class="pre">True</span></code> to sort in <strong>descending</strong> order.
<code class="docutils literal notranslate"><span class="pre">False</span></code> to sort in <strong>ascending</strong> order.</p></li>
<li><p><strong>need_new2old_indexes</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, also returns a 1-D tensor, containing the indexes mapping
from the sorted elements to the unsorted elements. We can use
<code class="docutils literal notranslate"><span class="pre">self.clone().values[returned_tensor]</span></code> to get a sorted tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If <code class="docutils literal notranslate"><span class="pre">need_new2old_indexes</span></code> is False, returns None. Otherwise, returns
a 1-D tensor of dtype <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="sum">
<h3>sum<a class="headerlink" href="#sum" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.sum">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">sum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.RaggedTensor.sum" title="Permalink to this definition"></a></dt>
<dd><p>Compute the sum of sublists over the last axis of this tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a sublist is empty, the sum for it is the provided
<code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation supports autograd if this tensor is a float tensor,
i.e., with dtype being torch.float32 or torch.float64.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1 2] [] [5]]  [[10]] ]&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[[1, 2],</span>
<span class="go">               [],</span>
<span class="go">               [5]],</span>
<span class="go">              [[10]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0., 0., 2., 3.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([ 3.,  0.,  5., 10.], grad_fn=&lt;SumFunction&gt;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor(40., grad_fn=&lt;SumBackward0&gt;)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – This value is added to the sum of each sublist. So when
a sublist is empty, its sum is this value.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a 1-D tensor with the same dtype of this tensor
containing the computed sum.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id35">
<h3>to<a class="headerlink" href="#id35" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.to">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.RaggedTensor.to" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>to(self: _k2.ragged.RaggedTensor, device: object) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Transfer this tensor to a given device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">self</span></code> is already on the specified device, return a
ragged tensor sharing the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.
Otherwise, a new tensor is returned.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – The target device to move this tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a tensor on the given device.</p>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>to(self: _k2.ragged.RaggedTensor, device: str) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Transfer this tensor to a given device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">self</span></code> is already on the specified device, return a
ragged tensor sharing the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.
Otherwise, a new tensor is returned.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=1)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – The target device to move this tensor.
Note: The device is represented as a string.
Valid strings are: “cpu”, “cuda:0”, “cuda:1”, etc.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a tensor on the given device.</p>
</dd>
</dl>
<ol class="arabic simple" start="3">
<li><p>to(self: _k2.ragged.RaggedTensor, dtype: torch::dtype) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Convert this tensor to a specific dtype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">self</span></code> is already of the specified <cite>dtype</cite>, return
a ragged tensor sharing the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.
Otherwise, a new tensor is returned.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
</pre></div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Currently, only support dtypes <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and
<code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>. We can support other types if needed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dtype</strong> – The <cite>dtype</cite> this tensor should be converted to.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a tensor of the given <cite>dtype</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id36">
<h3>to_str_simple<a class="headerlink" href="#id36" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.to_str_simple">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">to_str_simple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.RaggedTensor.to_str_simple" title="Permalink to this definition"></a></dt>
<dd><p>Convert a ragged tensor to a string representation, which
is more compact than <code class="docutils literal notranslate"><span class="pre">self.__str__</span></code>.</p>
<p>An example output is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">RaggedTensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1, 2, 3],</span>
<span class="go">               [],</span>
<span class="go">               [0]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3, 10.5]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&#39;RaggedTensor([[[1, 2, 3],\n               [],\n               [0]],\n              [[2],\n               [3, 10.5]]], dtype=torch.float32)&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to_str_simple</span><span class="p">()</span>
<span class="go">&#39;RaggedTensor([[[1, 2, 3], [], [0]], [[2], [3, 10.5]]], dtype=torch.float32)&#39;</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="tolist">
<h3>tolist<a class="headerlink" href="#tolist" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.tolist">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">tolist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#k2.RaggedTensor.tolist" title="Permalink to this definition"></a></dt>
<dd><p>Turn a ragged tensor into a list of lists [of lists..].</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>You can pass the returned list to the constructor of <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[[], [1, 2], [3], []], [[5, 6, 7]], [[], [0, 2, 3], [], []]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mf">3.25</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[1.0], [2.0], [], [3.25, 2.5]]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of list of lists [of lists …] containing the same elements
and structure as <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id37">
<h3>tot_size<a class="headerlink" href="#id37" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.tot_size">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">tot_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.RaggedTensor.tot_size" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of elements of an given axis. If axis is 0, it’s
equivalent to the property <code class="docutils literal notranslate"><span class="pre">dim0</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [1 2 3] [] [5 8 ] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1 2 3] [] [5 8]] [[] [1 5 9 10 -1] [] [] []] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">10</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="unique">
<h3>unique<a class="headerlink" href="#unique" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RaggedTensor.unique">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">unique</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_num_repeats</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_new2old_indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedTensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedTensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.RaggedTensor.unique" title="Permalink to this definition"></a></dt>
<dd><p>If <code class="docutils literal notranslate"><span class="pre">self</span></code> has two axes, this will return the unique sub-lists
(in a possibly different order, but without repeats).
If <code class="docutils literal notranslate"><span class="pre">self</span></code> has 3 axes, it will do the above but separately for each
index on axis 0; if more than 3 axes, the earliest axes will be ignored.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It does not completely guarantee that all unique sequences will be
present in the output, as it relies on hashing and ignores collisions.
If several sequences have the same hash, only one of them is kept, even
if the actual content in the sequence is different.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Even if there are no repeated sequences, the output may be different
from <code class="docutils literal notranslate"><span class="pre">self</span></code>. That is, <cite>new2old_indexes</cite> may NOT be an identity map even
if nothing was removed.</p>
</div>
<p><strong>Example 1</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3],</span>
<span class="go">              [3, 1]], dtype=torch.int32), None, None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_num_repeats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3],</span>
<span class="go">              [3, 1]], dtype=torch.int32), RaggedTensor([[2, 1, 1, 2]], dtype=torch.int32), tensor([2, 5, 1, 0], dtype=torch.int32))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_num_repeats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3],</span>
<span class="go">              [3, 1]], dtype=torch.int32), RaggedTensor([[2, 1, 1, 2]], dtype=torch.int32), None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3],</span>
<span class="go">              [3, 1]], dtype=torch.int32), None, tensor([2, 5, 1, 0], dtype=torch.int32))</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="go">(RaggedTensor([[[1, 2],</span>
<span class="go">               [2, 1]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3],</span>
<span class="go">               [0, 1]],</span>
<span class="go">              [[],</span>
<span class="go">               [3],</span>
<span class="go">               [2, 3]]], dtype=torch.int32), None, None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_num_repeats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[[1, 2],</span>
<span class="go">               [2, 1]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3],</span>
<span class="go">               [0, 1]],</span>
<span class="go">              [[],</span>
<span class="go">               [3],</span>
<span class="go">               [2, 3]]], dtype=torch.int32), RaggedTensor([[3, 1],</span>
<span class="go">              [2, 1, 1],</span>
<span class="go">              [2, 1, 1]], dtype=torch.int32), tensor([ 0,  1,  5,  4,  6,  8, 11,  9], dtype=torch.int32))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_num_repeats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[[1, 2],</span>
<span class="go">               [2, 1]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3],</span>
<span class="go">               [0, 1]],</span>
<span class="go">              [[],</span>
<span class="go">               [3],</span>
<span class="go">               [2, 3]]], dtype=torch.int32), RaggedTensor([[3, 1],</span>
<span class="go">              [2, 1, 1],</span>
<span class="go">              [2, 1, 1]], dtype=torch.int32), None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[[1, 2],</span>
<span class="go">               [2, 1]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3],</span>
<span class="go">               [0, 1]],</span>
<span class="go">              [[],</span>
<span class="go">               [3],</span>
<span class="go">               [2, 3]]], dtype=torch.int32), None, tensor([ 0,  1,  5,  4,  6,  8, 11,  9], dtype=torch.int32))</span>
</pre></div>
</div>
<p><strong>Example 3</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3]], dtype=torch.int32), RaggedTensor([[1, 1, 1]], dtype=torch.int32), tensor([0, 2, 1], dtype=torch.int32))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>need_num_repeats</strong> – If True, it also returns the number of repeats of each sequence.</p></li>
<li><p><strong>need_new2old_indexes</strong> – <p>If true, it returns an extra 1-D tensor <cite>new2old_indexes</cite>.
If <cite>src</cite> has 2 axes, this tensor contains <cite>src_idx0</cite>;
if <cite>src</cite> has 3 axes, this tensor contains <cite>src_idx01</cite>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>For repeated sublists, only one of them is kept.
The choice of which one to keep is <strong>deterministic</strong> and
is an implementation detail.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>ans: A ragged tensor with the same number of axes as <code class="docutils literal notranslate"><span class="pre">self</span></code> and possibly
fewer elements due to removing repeated sequences on the last axis
(and with the last-but-one indexes possibly in a different order).</p></li>
<li><p>num_repeats: A tensor containing number of repeats of each returned
sequence if <code class="docutils literal notranslate"><span class="pre">need_num_repeats</span></code> is True; it is <code class="docutils literal notranslate"><span class="pre">None</span></code> otherwise.
If it is not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">num_repeats.num_axes</span></code> is always 2.
If <code class="docutils literal notranslate"><span class="pre">ans.num_axes</span></code> is 2, then <code class="docutils literal notranslate"><span class="pre">num_repeats.dim0</span> <span class="pre">==</span> <span class="pre">1</span></code> and
<code class="docutils literal notranslate"><span class="pre">num_repeats.numel()</span> <span class="pre">==</span> <span class="pre">ans.dim0</span></code>.
If <code class="docutils literal notranslate"><span class="pre">ans.num_axes</span></code> is 3, then <code class="docutils literal notranslate"><span class="pre">num_repeats.dim0</span> <span class="pre">==</span> <span class="pre">ans.dim0</span></code> and
<code class="docutils literal notranslate"><span class="pre">num_repeats.numel()</span> <span class="pre">==</span> <span class="pre">ans.tot_size(1)</span></code>.</p></li>
<li><p>new2old_indexes: A 1-D tensor whose i-th element specifies the
input sublist that the i-th output sublist corresponds to.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Returns a tuple containing</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id38">
<h3>device<a class="headerlink" href="#id38" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.device">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#k2.RaggedTensor.device" title="Permalink to this definition"></a></dt>
<dd><p>Return the device of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id39">
<h3>dim0<a class="headerlink" href="#id39" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.dim0">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">dim0</span></span><a class="headerlink" href="#k2.RaggedTensor.dim0" title="Permalink to this definition"></a></dt>
<dd><p>Return number of sublists at axis 0.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[]] [[] []]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">2</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="dtype">
<h3>dtype<a class="headerlink" href="#dtype" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.dtype">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">dtype</span></span><a class="headerlink" href="#k2.RaggedTensor.dtype" title="Permalink to this definition"></a></dt>
<dd><p>Return the dtype of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id40">
<h3>grad<a class="headerlink" href="#id40" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.grad">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">grad</span></span><a class="headerlink" href="#k2.RaggedTensor.grad" title="Permalink to this definition"></a></dt>
<dd><p>This attribute is <code class="docutils literal notranslate"><span class="pre">None</span></code> by default. PyTorch will set it
during <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p>
<p>The attribute will contain the gradients computed and future
calls to <code class="docutils literal notranslate"><span class="pre">backward()</span></code> will accumulate (add) gradients into it.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [3],</span>
<span class="go">              [5, 6],</span>
<span class="go">              []], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([ 3.,  3., 11.,  0.], grad_fn=&lt;SumFunction&gt;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0., 0., 1., 2., 2.])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="is-cuda">
<h3>is_cuda<a class="headerlink" href="#is-cuda" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.is_cuda">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">is_cuda</span></span><a class="headerlink" href="#k2.RaggedTensor.is_cuda" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the tensor is stored on the GPU, <code class="docutils literal notranslate"><span class="pre">False</span></code>
otherwise.</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_cuda</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">is_cuda</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id41">
<h3>num_axes<a class="headerlink" href="#id41" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.num_axes">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">num_axes</span></span><a class="headerlink" href="#k2.RaggedTensor.num_axes" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of axes of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [] [] [] [] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[] []] [[]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k24</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="s1">&#39;[ [ [[] [1]] [[3 4] []] ]  [ [[1]] [[2] [3 4]] ] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">4</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return number of axes of this tensor, which is at least 2.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id42">
<h3>requires_grad<a class="headerlink" href="#id42" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.requires_grad">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#k2.RaggedTensor.requires_grad" title="Permalink to this definition"></a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if gradients need to be computed for this tensor.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id43">
<h3>shape<a class="headerlink" href="#id43" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.shape">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">shape</span></span><a class="headerlink" href="#k2.RaggedTensor.shape" title="Permalink to this definition"></a></dt>
<dd><p>Return the shape of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="go">[ [ x x ] [ ] [ x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">&lt;class &#39;_k2.ragged.RaggedShape&#39;&gt;</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="values">
<h3>values<a class="headerlink" href="#values" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RaggedTensor.values">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">values</span></span><a class="headerlink" href="#k2.RaggedTensor.values" title="Permalink to this definition"></a></dt>
<dd><p>Return the underlying memory as a 1-D tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span>
<span class="go">tensor([ 1,  2,  5,  8,  9, 10], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [8, -1, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [-3, -1, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [-2],</span>
<span class="go">              [],</span>
<span class="go">              [-3, -1, 10]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<section id="rnntdecodingconfig">
<h2>RnntDecodingConfig<a class="headerlink" href="#rnntdecodingconfig" title="Permalink to this headline"></a></h2>
<section id="id44">
<h3>__init__<a class="headerlink" href="#id44" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingConfig.__init__">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingConfig.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.RnntDecodingConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_history_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_contexts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#k2.RnntDecodingConfig.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Construct a RnntDecodingConfig object, it contains the parameters
needed by rnnt decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> – It indicates how many symbols we are using, equals the
largest-symbol plus one.</p></li>
<li><p><strong>decoder_history_len</strong> – The number of symbols of history the
decoder takes; will normally be one or two
(“stateless decoder”), our RNN-T decoding setup does not
support unlimited decoder context such as with LSTMs.</p></li>
<li><p><strong>beam</strong> – <cite>beam</cite> imposes a limit on the score of a state, relative to the
best-scoring state on the same frame.  E.g. 10.</p></li>
<li><p><strong>max_states</strong> – <cite>max_states</cite> is a limit on the number of distinct states that
we allow per frame, per stream; the number of states will not
be allowed to exceed this limit.</p></li>
<li><p><strong>max_contexts</strong> – <cite>max_contexts</cite> is a limit on the number of distinct contexts
that we allow per frame, per stream; the number of contexts
will not be allowed to exceed this limit.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="beam">
<h3>beam<a class="headerlink" href="#beam" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RnntDecodingConfig.beam">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingConfig.</span></span><span class="sig-name descname"><span class="pre">beam</span></span><a class="headerlink" href="#k2.RnntDecodingConfig.beam" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="decoder-history-len">
<h3>decoder_history_len<a class="headerlink" href="#decoder-history-len" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RnntDecodingConfig.decoder_history_len">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingConfig.</span></span><span class="sig-name descname"><span class="pre">decoder_history_len</span></span><a class="headerlink" href="#k2.RnntDecodingConfig.decoder_history_len" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="max-contexts">
<h3>max_contexts<a class="headerlink" href="#max-contexts" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RnntDecodingConfig.max_contexts">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingConfig.</span></span><span class="sig-name descname"><span class="pre">max_contexts</span></span><a class="headerlink" href="#k2.RnntDecodingConfig.max_contexts" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="max-states">
<h3>max_states<a class="headerlink" href="#max-states" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RnntDecodingConfig.max_states">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingConfig.</span></span><span class="sig-name descname"><span class="pre">max_states</span></span><a class="headerlink" href="#k2.RnntDecodingConfig.max_states" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="vocab-size">
<h3>vocab_size<a class="headerlink" href="#vocab-size" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.RnntDecodingConfig.vocab_size">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingConfig.</span></span><span class="sig-name descname"><span class="pre">vocab_size</span></span><a class="headerlink" href="#k2.RnntDecodingConfig.vocab_size" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
</section>
<section id="rnntdecodingstream">
<h2>RnntDecodingStream<a class="headerlink" href="#rnntdecodingstream" title="Permalink to this headline"></a></h2>
<section id="id45">
<h3>__init__<a class="headerlink" href="#id45" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingStream.__init__">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingStream.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fsa</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_decode.py#L33-L51"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.RnntDecodingStream.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Create a new rnnt decoding stream.</p>
<p>Every sequence(wave data) needs a decoding stream, this function is
expected to be called when a new sequence comes. We support different
decoding graphs for different streams.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fsa</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code>) – The decoding graph used in this stream.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A rnnt decoding stream object, which will be combined into
<code class="xref py py-class docutils literal notranslate"><span class="pre">RnntDecodingStreams</span></code> to do decoding together with other
sequences in parallel.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id46">
<h3>__str__<a class="headerlink" href="#id46" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingStream.__str__">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingStream.</span></span><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_decode.py#L53-L58"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.RnntDecodingStream.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this object</p>
<p>For visualization and debug only.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="rnntdecodingstreams">
<h2>RnntDecodingStreams<a class="headerlink" href="#rnntdecodingstreams" title="Permalink to this headline"></a></h2>
<section id="id47">
<h3>__init__<a class="headerlink" href="#id47" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingStreams.__init__">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingStreams.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_streams</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_decode.py#L66-L90"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.RnntDecodingStreams.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Combines multiple RnntDecodingStream objects to create a
RnntDecodingStreams object, then all these RnntDecodingStreams can do
decoding in parallel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src_streams</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">RnntDecodingStream</span></code>]) – A list of RnntDecodingStream object to be combined.</p></li>
<li><p><strong>config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">RnntDecodingConfig</span></code>) – A configuration object which contains decoding parameters like
<cite>vocab-size</cite>, <cite>decoder_history_len</cite>, <cite>beam</cite>, <cite>max_states</cite>,
<cite>max_contexts</cite> etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a RnntDecodingStreams object.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id48">
<h3>__str__<a class="headerlink" href="#id48" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingStreams.__str__">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingStreams.</span></span><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_decode.py#L92-L100"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.RnntDecodingStreams.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this object</p>
<p>For visualization and debug only.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="advance">
<h3>advance<a class="headerlink" href="#advance" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingStreams.advance">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingStreams.</span></span><span class="sig-name descname"><span class="pre">advance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logprobs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_decode.py#L124-L135"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.RnntDecodingStreams.advance" title="Permalink to this definition"></a></dt>
<dd><p>Advance decoding streams by one frame.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>logprobs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – A tensor of shape [tot_contexts][num_symbols], containing log-probs
of symbols given the contexts output by <cite>get_contexts()</cite>. It
satisfies <cite>logprobs.Dim0() == shape.TotSize(1)</cite>, shape is returned
by <cite>get_contexts()</cite>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="format-output">
<h3>format_output<a class="headerlink" href="#format-output" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingStreams.format_output">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingStreams.</span></span><span class="sig-name descname"><span class="pre">format_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_frames</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_partial</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t2s2c_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_decode.py#L149-L297"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.RnntDecodingStreams.format_output" title="Permalink to this definition"></a></dt>
<dd><p>Generate the lattice Fsa currently got.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The attributes of the generated lattice is a union of the attributes
of all the decoding graphs. For example, if <cite>self</cite> contains three
individual stream, each stream has its own decoding graphs, graph[0]
has attributes attr1, attr2; graph[1] has attributes attr1, attr3;
graph[2] has attributes attr3, attr4; then the generated lattice has
attributes attr1, attr2, attr3, attr4.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_frames</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – A List containing the number of frames we want to gather for each
stream (note: the frames we have ever received for the corresponding
stream). It MUST satisfy <cite>len(num_frames) == self.num_streams</cite>.</p></li>
<li><p><strong>allow_partial</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If true and there is no final state active, we will treat all the
states on the last frame to be final state.
If false, we only care about the real final state in the
decoding graph on the last frame when generating lattice.
Default False.</p></li>
<li><p><strong>log_probs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]) – A tensor of shape [t2s2c_shape.tot_size(2)][num_symbols].
It’s a stacked tensor of logprobs passed to function <cite>advance</cite>
during decoding.</p></li>
<li><p><strong>t2s2c_shape</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedShape</span></code>]) – It is short for time2stream2context_shape,
which describes shape of log_probs used to generate lattice.
Used to generate arc_map_token
and make the whole decoding process differentiable.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Fsa</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return the lattice Fsa with all the attributes propagated.
The returned Fsa has 3 axes with <cite>fsa.dim0==self.num_streams</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-contexts">
<h3>get_contexts<a class="headerlink" href="#get-contexts" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingStreams.get_contexts">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingStreams.</span></span><span class="sig-name descname"><span class="pre">get_contexts</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_decode.py#L102-L122"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.RnntDecodingStreams.get_contexts" title="Permalink to this definition"></a></dt>
<dd><p>This function must be called prior to evaluating the joiner network
for a particular frame.  It tells the calling code for which contexts
it must evaluate the joiner network.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedShape</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Return a two-element tuple containing a RaggedShape and a tensor.</p>
<dl class="simple">
<dt>shape:</dt><dd><p>A RaggedShape with 2 axes, representing [stream][context].</p>
</dd>
<dt>contexts:</dt><dd><p>A tensor of shape [tot_contexts][decoder_history_len], where
tot_contexts == shape-&gt;TotSize(1) and decoder_history_len comes from
the config, it represents the number of symbols in the context of
the decoder network (assumed to be finite). It contains the token
ids into the vocabulary(i.e. <cite>0 &lt;= value &lt; vocab_size</cite>).
Its dtype is torch.int32.</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="terminate-and-flush-to-streams">
<h3>terminate_and_flush_to_streams<a class="headerlink" href="#terminate-and-flush-to-streams" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.RnntDecodingStreams.terminate_and_flush_to_streams">
<span class="sig-prename descclassname"><span class="pre">RnntDecodingStreams.</span></span><span class="sig-name descname"><span class="pre">terminate_and_flush_to_streams</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_decode.py#L137-L147"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.RnntDecodingStreams.terminate_and_flush_to_streams" title="Permalink to this definition"></a></dt>
<dd><p>Terminate the decoding process of current RnntDecodingStreams object.
It will update the decoding states and store the decoding results
currently got to each of the individual streams.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We can not decode with this object anymore after calling
terminate_and_flush_to_streams().</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="symboltable">
<h2>SymbolTable<a class="headerlink" href="#symboltable" title="Permalink to this headline"></a></h2>
<section id="id49">
<h3>add<a class="headerlink" href="#id49" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.SymbolTable.add">
<span class="sig-prename descclassname"><span class="pre">SymbolTable.</span></span><span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symbol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/symbol_table.py#L165-L195"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.SymbolTable.add" title="Permalink to this definition"></a></dt>
<dd><p>Add a new symbol to the SymbolTable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>symbol</strong> (<em>~Symbol</em>) – The symbol to be added.</p></li>
<li><p><strong>index</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Optional int id to which the symbol should be assigned.
If it is not available, a ValueError will be raised.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The int id to which the symbol has been assigned.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="from-file">
<h3>from_file<a class="headerlink" href="#from-file" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.SymbolTable.from_file">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SymbolTable.</span></span><span class="sig-name descname"><span class="pre">from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/symbol_table.py#L108-L131"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.SymbolTable.from_file" title="Permalink to this definition"></a></dt>
<dd><p>Build a symbol table from file.</p>
<p>Every line in the symbol table file has two fields separated by
space(s), tab(s) or both. The following is an example file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">eps</span><span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">a</span> <span class="mi">1</span>
<span class="n">b</span> <span class="mi">2</span>
<span class="n">c</span> <span class="mi">3</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filename</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the symbol table file. Its format is documented above.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">SymbolTable</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">SymbolTable</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id50">
<h3>from_str<a class="headerlink" href="#id50" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.SymbolTable.from_str">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SymbolTable.</span></span><span class="sig-name descname"><span class="pre">from_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">s</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/symbol_table.py#L75-L106"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.SymbolTable.from_str" title="Permalink to this definition"></a></dt>
<dd><p>Build a symbol table from a string.</p>
<p>The string consists of lines. Every line has two fields separated
by space(s), tab(s) or both. The first field is the symbol and the
second the integer id of the symbol.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>s</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The input string with the format described above.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">SymbolTable</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">SymbolTable</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="get">
<h3>get<a class="headerlink" href="#get" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.SymbolTable.get">
<span class="sig-prename descclassname"><span class="pre">SymbolTable.</span></span><span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/symbol_table.py#L197-L212"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.SymbolTable.get" title="Permalink to this definition"></a></dt>
<dd><p>Get a symbol for an id or get an id for a symbol</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>k</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, ~Symbol]) – If it is an id, it tries to find the symbol corresponding
to the id; if it is a symbol, it tries to find the id
corresponding to the symbol.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[~Symbol, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An id or a symbol depending on the given <cite>k</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="merge">
<h3>merge<a class="headerlink" href="#merge" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.SymbolTable.merge">
<span class="sig-prename descclassname"><span class="pre">SymbolTable.</span></span><span class="sig-name descname"><span class="pre">merge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/symbol_table.py#L214-L231"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.SymbolTable.merge" title="Permalink to this definition"></a></dt>
<dd><p>Create a union of two SymbolTables.
Raises an AssertionError if the same IDs are occupied by
different symbols.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SymbolTable</span></code>) – A symbol table to merge with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">SymbolTable</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A new symbol table.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-file">
<h3>to_file<a class="headerlink" href="#to-file" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.SymbolTable.to_file">
<span class="sig-prename descclassname"><span class="pre">SymbolTable.</span></span><span class="sig-name descname"><span class="pre">to_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/k2-fsa/k2/blob/master/k2/python/k2/symbol_table.py#L144-L163"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#k2.SymbolTable.to_file" title="Permalink to this definition"></a></dt>
<dd><p>Serialize the SymbolTable to a file.</p>
<p>Every line in the symbol table file has two fields separated by
space(s), tab(s) or both. The following is an example file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">eps</span><span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">a</span> <span class="mi">1</span>
<span class="n">b</span> <span class="mi">2</span>
<span class="n">c</span> <span class="mi">3</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filename</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the symbol table file. Its format is documented above.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="ids">
<h3>ids<a class="headerlink" href="#ids" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.SymbolTable.ids">
<span class="sig-prename descclassname"><span class="pre">SymbolTable.</span></span><span class="sig-name descname"><span class="pre">ids</span></span><a class="headerlink" href="#k2.SymbolTable.ids" title="Permalink to this definition"></a></dt>
<dd><p>Returns a list of integer IDs corresponding to the symbols.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="symbols">
<h3>symbols<a class="headerlink" href="#symbols" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.SymbolTable.symbols">
<span class="sig-prename descclassname"><span class="pre">SymbolTable.</span></span><span class="sig-name descname"><span class="pre">symbols</span></span><a class="headerlink" href="#k2.SymbolTable.symbols" title="Permalink to this definition"></a></dt>
<dd><p>Returns a list of symbols (e.g., strings) corresponding to
the integer IDs.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[~Symbol]</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="k2-ragged">
<h1>k2.ragged<a class="headerlink" href="#k2-ragged" title="Permalink to this headline"></a></h1>
<section id="id51">
<h2>cat<a class="headerlink" href="#id51" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ragged.cat">
<span class="sig-prename descclassname"><span class="pre">k2.ragged.</span></span><span class="sig-name descname"><span class="pre">cat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">srcs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedTensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.cat" title="Permalink to this definition"></a></dt>
<dd><p>Concatenate a list of ragged tensor over a specified axis.</p>
<p><strong>Example 1</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3, 2, 3]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              [],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [],</span>
<span class="go">              [9],</span>
<span class="go">              [0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [],</span>
<span class="go">              [-1],</span>
<span class="go">              [10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1, 3, 0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [-1],</span>
<span class="go">              [9, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 3, 0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [-1],</span>
<span class="go">              [9, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">RaggedTensor([[0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [],</span>
<span class="go">              [-1],</span>
<span class="go">              [10],</span>
<span class="go">              [1, 3],</span>
<span class="go">              [],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>srcs</strong> – A list (or a tuple) of ragged tensors to concatenate. They <strong>MUST</strong> all
have the same dtype and on the same device.</p></li>
<li><p><strong>axis</strong> – Only 0 and 1 are supported right now. If it is 1, then
<code class="docutils literal notranslate"><span class="pre">srcs[i].dim0</span></code> must all have the same value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a concatenated tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="create-ragged-shape2">
<h2>create_ragged_shape2<a class="headerlink" href="#create-ragged-shape2" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ragged.create_ragged_shape2">
<span class="sig-prename descclassname"><span class="pre">k2.ragged.</span></span><span class="sig-name descname"><span class="pre">create_ragged_shape2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">row_splits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">row_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cached_tot_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.create_ragged_shape2" title="Permalink to this definition"></a></dt>
<dd><p>Construct a RaggedShape from row_ids and/or row_splits vectors.  For
the overall concepts, please see comments in k2/csrc/utils.h.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[[x x] [x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_shape2</span><span class="p">(</span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="go">[ [ x x ] [ x ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>row_splits</strong> – Optional. A 1-D torch.Tensor with dtype torch.int32.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, you have to specify <code class="docutils literal notranslate"><span class="pre">row_ids</span></code>.</p></li>
<li><p><strong>row_ids</strong> – Optional. A 1-D torch.Tensor with dtype torch.int32.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, you have to specify <code class="docutils literal notranslate"><span class="pre">row_splits</span></code>.</p></li>
<li><p><strong>cached_tot_size</strong> – The number of elements (length of row_ids, even if row_ids
is not provided); would be identical to the last element of row_splits,
but can avoid a GPU to CPU transfer if known.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedShape</span></code>, with <code class="docutils literal notranslate"><span class="pre">ans.num_axes</span> <span class="pre">==</span> <span class="pre">2</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="create-ragged-tensor">
<h2>create_ragged_tensor<a class="headerlink" href="#create-ragged-tensor" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ragged.create_ragged_tensor">
<span class="sig-prename descclassname"><span class="pre">k2.ragged.</span></span><span class="sig-name descname"><span class="pre">create_ragged_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.ragged.create_ragged_tensor" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>create_ragged_tensor(data: list, dtype: object = None, device: object = ‘cpu’) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Create a ragged tensor with arbitrary number of axes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A ragged tensor has at least two axes.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The returned tensor is on CPU.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              []], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [2, 3]],</span>
<span class="go">              [[4],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              []], device=&#39;cuda:1&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – A list-of sublist(s) of integers or real numbers.
It can have arbitrary number of axes (at least two).</p></li>
<li><p><strong>dtype</strong> – Optional. If None, it infers the dtype from <code class="docutils literal notranslate"><span class="pre">data</span></code>
automatically, which is either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor.</p>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>create_ragged_tensor(data: list, dtype: object = None, device: str = ‘cpu’) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Create a ragged tensor with arbitrary number of axes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A ragged tensor has at least two axes.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The returned tensor is on CPU.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              []], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [2, 3]],</span>
<span class="go">              [[4],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              []], device=&#39;cuda:1&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – A list-of sublist(s) of integers or real numbers.
It can have arbitrary number of axes (at least two).</p></li>
<li><p><strong>dtype</strong> – Optional. If None, it infers the dtype from <code class="docutils literal notranslate"><span class="pre">data</span></code>
automatically, which is either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor.</p>
</dd>
</dl>
<ol class="arabic simple" start="3">
<li><p>create_ragged_tensor(s: str, dtype: object = None, device: object = ‘cpu’) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Create a ragged tensor from its string representation.</p>
<p>Fields are separated by space(s) <strong>or</strong> comma(s).</p>
<p>An example string for a 2-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>An example string for a 3-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]]</span> <span class="p">[[]]</span> <span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="s1">&#39;[ [1] [] [3 4] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [3, 4]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="s1">&#39;[ [[] [3]]  [[10]] ]&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">[ [ [ ] [ 3 ] ] [ [ 10 ] ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Number of spaces or commas in <code class="docutils literal notranslate"><span class="pre">s</span></code> does not affect the result.
Of course, numbers have to be separated by at least one space or comma.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> – A string representation of a ragged tensor.</p></li>
<li><p><strong>dtype</strong> – The desired dtype of the tensor. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it tries
to infer the correct dtype from <code class="docutils literal notranslate"><span class="pre">s</span></code>, which is assumed to be
either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are:
<code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor.</p>
</dd>
</dl>
<ol class="arabic simple" start="4">
<li><p>create_ragged_tensor(s: str, dtype: object = None, device: str = ‘cpu’) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Create a ragged tensor from its string representation.</p>
<p>Fields are separated by space(s) <strong>or</strong> comma(s).</p>
<p>An example string for a 2-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>An example string for a 3-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]]</span> <span class="p">[[]]</span> <span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="s1">&#39;[ [1] [] [3 4] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [3, 4]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="s1">&#39;[ [[] [3]]  [[10]] ]&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">[ [ [ ] [ 3 ] ] [ [ 10 ] ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Number of spaces or commas in <code class="docutils literal notranslate"><span class="pre">s</span></code> does not affect the result.
Of course, numbers have to be separated by at least one space or comma.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> – A string representation of a ragged tensor.</p></li>
<li><p><strong>dtype</strong> – The desired dtype of the tensor. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it tries
to infer the correct dtype from <code class="docutils literal notranslate"><span class="pre">s</span></code>, which is assumed to be
either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are:
<code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor.</p>
</dd>
</dl>
<ol class="arabic simple" start="5">
<li><p>create_ragged_tensor(tensor: torch.Tensor) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Create a ragged tensor from a torch tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It turns a regular tensor into a ragged tensor.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The input tensor has to have more than 1 dimension.
That is <code class="docutils literal notranslate"><span class="pre">tensor.ndim</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p>
<p>Also, if the input tensor is contiguous, <code class="docutils literal notranslate"><span class="pre">self</span></code>
will share the underlying memory with it. Otherwise,
memory of the input tensor is copied to create <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>Supported dtypes of the input tensor are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 1, 2],</span>
<span class="go">        [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 1, 2],</span>
<span class="go">              [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[10, 1, 2],</span>
<span class="go">              [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[10, -2,  2],</span>
<span class="go">        [ 3,  4,  5]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">)[:,</span> <span class="p">::</span><span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0,  4,  8],</span>
<span class="go">        [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 4, 8],</span>
<span class="go">              [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 4, 8],</span>
<span class="go">              [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[10,  4,  8],</span>
<span class="go">        [12, 16, 20]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 3</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[[ 0.,  1.,  2.,  3.],</span>
<span class="go">         [ 4.,  5.,  6.,  7.],</span>
<span class="go">         [ 8.,  9., 10., 11.]],</span>
<span class="go">        [[12., 13., 14., 15.],</span>
<span class="go">         [16., 17., 18., 19.],</span>
<span class="go">         [20., 21., 22., 23.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">create_ragged_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[0, 1, 2, 3],</span>
<span class="go">               [4, 5, 6, 7],</span>
<span class="go">               [8, 9, 10, 11]],</span>
<span class="go">              [[12, 13, 14, 15],</span>
<span class="go">               [16, 17, 18, 19],</span>
<span class="go">               [20, 21, 22, 23]]], dtype=torch.float32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – An N-D (N &gt; 1) tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id52">
<h2>index<a class="headerlink" href="#id52" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ragged.index">
<span class="sig-prename descclassname"><span class="pre">k2.ragged.</span></span><span class="sig-name descname"><span class="pre">index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.index" title="Permalink to this definition"></a></dt>
<dd><p>Use a ragged tensor to index a 1-d torch tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span>
<span class="go">tensor([ 0, 10, 20, 30, 40, 50], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
<span class="go">RaggedTensor([[10, 50, 30],</span>
<span class="go">              [0, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="go">RaggedTensor([[[10, 50, 30],</span>
<span class="go">               [0]],</span>
<span class="go">              [[0, 20],</span>
<span class="go">               [10, 30]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="go">RaggedTensor([[10, 0],</span>
<span class="go">              [0, 0],</span>
<span class="go">              [0]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
<span class="go">RaggedTensor([[10, -2],</span>
<span class="go">              [-2, 0],</span>
<span class="go">              [-2]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> – A 1-D torch tensor.</p></li>
<li><p><strong>indexes</strong> – A ragged tensor with dtype <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>.</p></li>
<li><p><strong>default_value</strong> – Used only when an entry in <code class="docutils literal notranslate"><span class="pre">indexes</span></code> is -1, in which case
it returns <code class="docutils literal notranslate"><span class="pre">default_value</span></code> as -1 is not a valid index.
If it is <code class="docutils literal notranslate"><span class="pre">None</span></code> and an entry in <code class="docutils literal notranslate"><span class="pre">indexes</span></code> is -1, 0 is returned.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor with the same dtype and device as <code class="docutils literal notranslate"><span class="pre">src</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="index-and-sum">
<h2>index_and_sum<a class="headerlink" href="#index-and-sum" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ragged.index_and_sum">
<span class="sig-prename descclassname"><span class="pre">k2.ragged.</span></span><span class="sig-name descname"><span class="pre">index_and_sum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.index_and_sum" title="Permalink to this definition"></a></dt>
<dd><p>Index a 1-D tensor with a ragged tensor of indexes, perform
a sum-per-sublist operation, and return the resulting 1-D tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span>
<span class="go">tensor([ 0., 10., 20., 30., 40., 50.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">index_and_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
<span class="go">tensor([90., 50.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">index_and_sum</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="go">tensor([30.,  0., 70.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> – A 1-D tensor.</p></li>
<li><p><strong>indexes</strong> – A ragged tensor with two axes. Its dtype MUST be <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>.
For instance, it can be the arc map returned from the function
<code class="docutils literal notranslate"><span class="pre">remove_epsilon</span></code>. If an index is -1, the resulting sublist
is 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a 1-D tensor with the same dtype and device as <code class="docutils literal notranslate"><span class="pre">src</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="random-ragged-shape">
<h2>random_ragged_shape<a class="headerlink" href="#random-ragged-shape" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ragged.random_ragged_shape">
<span class="sig-prename descclassname"><span class="pre">k2.ragged.</span></span><span class="sig-name descname"><span class="pre">random_ragged_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_row_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_num_axes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_axes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_num_elements</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_elements</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2000</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.random_ragged_shape" title="Permalink to this definition"></a></dt>
<dd><p>RandomRaggedShape</p>
</dd></dl>

</section>
<section id="id53">
<h2>regular_ragged_shape<a class="headerlink" href="#id53" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="k2.ragged.regular_ragged_shape">
<span class="sig-prename descclassname"><span class="pre">k2.ragged.</span></span><span class="sig-name descname"><span class="pre">regular_ragged_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.regular_ragged_shape" title="Permalink to this definition"></a></dt>
<dd><p>Create a ragged shape with 2 axes that has a regular structure.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="o">.</span><span class="n">regular_ragged_shape</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span>
<span class="go">[ [ x x x ] [ x x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">regular_ragged_shape</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span>
<span class="go">[ [ x x ] [ x x ] [ x x ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim0</strong> – Number of entries at axis 0.</p></li>
<li><p><strong>dim1</strong> – Number of entries in each sublist at axis 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged shape on CPU.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id54">
<h2>RaggedShape<a class="headerlink" href="#id54" title="Permalink to this headline"></a></h2>
<section id="id55">
<h3>__eq__<a class="headerlink" href="#id55" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.__eq__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__eq__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.__eq__" title="Permalink to this definition"></a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if two shapes are equal. Otherwise, return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two shapes have to be on the same device. Otherwise, it throws
an exception.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">==</span> <span class="n">shape2</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span> <span class="o">==</span> <span class="n">shape2</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The shape that we want to compare with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the two shapes are the same.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id56">
<h3>__getitem__<a class="headerlink" href="#id56" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.__getitem__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__getitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">i</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.__getitem__" title="Permalink to this definition"></a></dt>
<dd><p>Select the i-th sublist along axis 0.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It requires that this shape has at least 3 axes.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [x x]] [[x x x] [] [x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">[ [ x ] [ x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">[ [ x x x ] [ ] [ x x ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>i</strong> – The i-th sublist along axis 0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new ragged shape with one fewer axis.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id57">
<h3>__init__<a class="headerlink" href="#id57" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.__init__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Construct a ragged shape from a string.</p>
<p>An example string for a ragged shape with 2 axes is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="n">x</span> <span class="n">x</span><span class="p">]</span> <span class="p">[</span> <span class="p">]</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>An example string for a ragged shape with 3 axes is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[[</span><span class="n">x</span><span class="p">]</span> <span class="p">[]]</span> <span class="p">[[</span><span class="n">x</span><span class="p">]</span> <span class="p">[</span><span class="n">x</span> <span class="n">x</span><span class="p">]]</span> <span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [] [x x]] [[]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span>
<span class="go">[ [ [ x ] [ ] [ x x ] ] [ [ ] ] ]</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id58">
<h3>__ne__<a class="headerlink" href="#id58" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.__ne__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__ne__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.__ne__" title="Permalink to this definition"></a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if two shapes are not equal. Otherwise, return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two shapes have to be on the same device. Otherwise, it throws
an exception.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">!=</span> <span class="n">shape2</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">!=</span> <span class="n">shape3</span>
<span class="go">False</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The shape that we want to compare with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the two shapes are not equal.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id59">
<h3>__repr__<a class="headerlink" href="#id59" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.__repr__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__repr__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.__repr__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x ] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id60">
<h3>__str__<a class="headerlink" href="#id60" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.__str__">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x ] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span>
<span class="go">[ [ x ] [ ] [ x x ] ]</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id61">
<h3>compose<a class="headerlink" href="#id61" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.compose">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">compose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.compose" title="Permalink to this definition"></a></dt>
<dd><p>Compose <code class="docutils literal notranslate"><span class="pre">self</span></code> with a given shape.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">other</span></code> and <code class="docutils literal notranslate"><span class="pre">self</span></code> MUST be on the same device.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>In order to compose <code class="docutils literal notranslate"><span class="pre">self</span></code> with <code class="docutils literal notranslate"><span class="pre">other</span></code>, it has to
satisfy <code class="docutils literal notranslate"><span class="pre">self.tot_size(self.num_axes</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">==</span> <span class="pre">other.dim0</span></code></p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x x] [x x] [] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="n">shape2</span><span class="p">)</span>
<span class="go">[ [ [ x x x ] [ x x ] ] [ [ ] ] ]</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x x x] []] [[x] [x x x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x x x] [] [] [x x] [x] [] [x x x x] [] [x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span><span class="o">.</span><span class="n">compose</span><span class="p">(</span><span class="n">shape2</span><span class="p">)</span>
<span class="go">[ [ [ [ x ] [ x x x ] ] [ [ ] [ ] [ x x ] ] [ ] ] [ [ [ x ] ] [ [ ] [ x x x x ] [ ] [ x x ] ] ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="n">shape1</span><span class="o">.</span><span class="n">num_axes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">10</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The other shape that is to be composed with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a composed ragged shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id62">
<h3>get_layer<a class="headerlink" href="#id62" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.get_layer">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">get_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.get_layer" title="Permalink to this definition"></a></dt>
<dd><p>Returns a <cite>sub-shape</cite> of <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x] []] [[] [x x x] [x]] [[]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">[ [ x x x ] [ x x x ] [ x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">[ [ x x ] [ x ] [ ] [ ] [ x x x ] [ x ] [ ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>layer</strong> – Layer that is desired, from <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">..</span> <span class="pre">src.num_axes</span> <span class="pre">-</span> <span class="pre">2</span></code> (inclusive).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This returned shape will have <code class="docutils literal notranslate"><span class="pre">num_axes</span> <span class="pre">==</span> <span class="pre">2</span></code>, the minimal case of
a <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedShape</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id63">
<h3>index<a class="headerlink" href="#id63" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.index">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_value_indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedShape</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.index" title="Permalink to this definition"></a></dt>
<dd><p>Indexing operation on a ragged shape, returns <code class="docutils literal notranslate"><span class="pre">self[indexes]</span></code>, where elements
of <code class="docutils literal notranslate"><span class="pre">indexes</span></code> are interpreted as indexes into axis <code class="docutils literal notranslate"><span class="pre">axis</span></code> of self``.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">indexes</span></code> is a 1-D tensor and <code class="docutils literal notranslate"><span class="pre">indexes.dtype</span> <span class="pre">==</span> <span class="pre">torch.int32</span></code>.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [x] [x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span>
<span class="go">[ [ 0 10 ] [ 20 ] [ 30 40 50 ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub_shape</span><span class="p">,</span> <span class="n">value_indexes</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">indexes</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub_shape</span>
<span class="go">[ [ x x ] [ x x x ] [ x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_indexes</span>
<span class="go">tensor([0, 1, 3, 4, 5, 2], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">value_indexes</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([ 0., 10., 30., 40., 50., 20.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub_shape2</span><span class="p">,</span> <span class="n">value_indexes2</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">indexes</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub_shape2</span>
<span class="go">[ [ x x ] [ ] [ x ] [ x x ] [ x x x ] [ ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_indexes2</span>
<span class="go">tensor([0, 1, 2, 0, 1, 3, 4, 5], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x]] [[] [x x x] [x]] [[x] [] [] [x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">indexes</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
<span class="go">([ [ [ x x ] [ x ] ] [ [ x x x ] ] [ [ x ] [ ] [ x x ] ] ], tensor([0, 1, 2, 3, 4, 5, 7, 8, 9], dtype=torch.int32))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – The axis to be indexed. Must satisfy <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">axis</span> <span class="pre">&lt;</span> <span class="pre">self.num_axes</span></code>.</p></li>
<li><p><strong>indexes</strong> – Array of indexes, which will be interpreted as indexes into axis <code class="docutils literal notranslate"><span class="pre">axis</span></code>
of <code class="docutils literal notranslate"><span class="pre">self</span></code>, i.e. with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">indexes[i]</span> <span class="pre">&lt;</span> <span class="pre">self.tot_size(axis)</span></code>.
Note that if <code class="docutils literal notranslate"><span class="pre">axis</span></code> is 0, then -1 is also a valid entry in <code class="docutils literal notranslate"><span class="pre">index</span></code>,
in which case, an empty list is returned.</p></li>
<li><p><strong>need_value_indexes</strong> – <p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, it will return a torch.Tensor containing the indexes into
<code class="docutils literal notranslate"><span class="pre">ragged_tensor.data</span></code> that <code class="docutils literal notranslate"><span class="pre">ans.data</span></code> has, as in
<code class="docutils literal notranslate"><span class="pre">ans.data</span> <span class="pre">=</span> <span class="pre">ragged_tensor.data[value_indexes]</span></code>, where <code class="docutils literal notranslate"><span class="pre">ragged_tensor</span></code>
uses <code class="docutils literal notranslate"><span class="pre">self</span></code> as its shape.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It is currently not allowed to change the order on axes less than
<code class="docutils literal notranslate"><span class="pre">axis</span></code>, i.e. if <code class="docutils literal notranslate"><span class="pre">axis</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>, we require:
<code class="docutils literal notranslate"><span class="pre">IsMonotonic(self.row_ids(axis)[indexes])</span></code>.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return an indexed ragged shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id64">
<h3>max_size<a class="headerlink" href="#id64" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.max_size">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">max_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.max_size" title="Permalink to this definition"></a></dt>
<dd><p>Return the maximum number of elements of any sublist at the given axis.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [] [x] [x x] [x x x] [x x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">max_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x] [] [] []] [[x]] [[x x x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">max_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">max_size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">4</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> – <p>Compute the max size of this axis.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">axis</span></code> has to be greater than 0.</p>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return the maximum number of elements of sublists at the given <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id65">
<h3>numel<a class="headerlink" href="#id65" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.numel">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">numel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.numel" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of elements in this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x x x x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x x] [x] [] [] []] [[x]] [[x x x x]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x x] [x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape3</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">4</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Return the number of elements in this shape.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>It’s the number of <code class="docutils literal notranslate"><span class="pre">x</span></code>’s.</p>
</div>
</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id66">
<h3>regular_ragged_shape<a class="headerlink" href="#id66" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.regular_ragged_shape">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">regular_ragged_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.regular_ragged_shape" title="Permalink to this definition"></a></dt>
<dd><p>Create a ragged shape with 2 axes that has a regular structure.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="o">.</span><span class="n">regular_ragged_shape</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span>
<span class="go">[ [ x x x ] [ x x x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">regular_ragged_shape</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span>
<span class="go">[ [ x x ] [ x x ] [ x x ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim0</strong> – Number of entries at axis 0.</p></li>
<li><p><strong>dim1</strong> – Number of entries in each sublist at axis 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged shape on CPU.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id67">
<h3>remove_axis<a class="headerlink" href="#id67" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.remove_axis">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">remove_axis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.remove_axis" title="Permalink to this definition"></a></dt>
<dd><p>Remove a certain axis.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">self.num_axes</span></code> MUST be greater than 2.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [] [x x]] [[x x x] [x x x x]] [[] [] []]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">[ [ x ] [ ] [ x x ] [ x x x ] [ x x x x ] [ ] [ ] [ ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">[ [ x x x ] [ x x x x x x x ] [ ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> – The axis to be removed.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged shape with one fewer axis.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id68">
<h3>row_ids<a class="headerlink" href="#id68" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.row_ids">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">row_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.row_ids" title="Permalink to this definition"></a></dt>
<dd><p>Return the row ids of a certain <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [] [x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 0, 2, 2, 2], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [] [x x]] [[x x x] [x] [x x x x] [] []] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([0, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – The axis whose row ids is to be returned.</p></li>
<li><p><strong>Hint</strong> – <code class="docutils literal notranslate"><span class="pre">axis</span> <span class="pre">&gt;=</span> <span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return the row ids of the given <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id69">
<h3>row_splits<a class="headerlink" href="#id69" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.row_splits">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">row_splits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.row_splits" title="Permalink to this definition"></a></dt>
<dd><p>Return the row splits of a certain <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [] [x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 2, 2, 5], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] [] [x x]] [[x x x] [x] [x x x x] [] []] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 3, 8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([ 0,  1,  1,  3,  6,  7, 11, 11, 11], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – The axis whose row splits is to be returned.</p></li>
<li><p><strong>Hint</strong> – <code class="docutils literal notranslate"><span class="pre">axis</span> <span class="pre">&gt;=</span> <span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return the row splits of the given <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id70">
<h3>to<a class="headerlink" href="#id70" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.to">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedShape</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.to" title="Permalink to this definition"></a></dt>
<dd><p>Move this shape to the specified device.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If the shape is already on the specified device, the returned shape
shares the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[[x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span>
<span class="go">[ [ x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span>
<span class="go">[ [ x ] ]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – An instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code>. It can be either a CPU device or
a CUDA device.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a shape on the given device.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id71">
<h3>tot_size<a class="headerlink" href="#id71" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.tot_size">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">tot_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.tot_size" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of elements at a certain``axis``.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [x x] [x x x] []]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x]] [[x x]] [[x x x]] [[]] [[]] [[]] [[]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x]] [[x x]] [[x x x]] [[]] [[]] [[]] [[] []] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">6</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> – Return the number of elements for this <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return the number of elements at <code class="docutils literal notranslate"><span class="pre">axis</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id72">
<h3>tot_sizes<a class="headerlink" href="#id72" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.tot_sizes">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">tot_sizes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedShape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span></span></span><a class="headerlink" href="#k2.ragged.RaggedShape.tot_sizes" title="Permalink to this definition"></a></dt>
<dd><p>Return total sizes of every axis in a tuple.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [ ] [x x x x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">tot_sizes</span><span class="p">()</span>
<span class="go">(3, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] []] [[x x x x]]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">tot_sizes</span><span class="p">()</span>
<span class="go">(2, 3, 5)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return a tuple containing the total sizes of each axis.
<code class="docutils literal notranslate"><span class="pre">ans[i]</span></code> is the total size of axis <code class="docutils literal notranslate"><span class="pre">i</span></code> (for <code class="docutils literal notranslate"><span class="pre">i</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>).
For <code class="docutils literal notranslate"><span class="pre">i=0</span></code>, it is the <code class="docutils literal notranslate"><span class="pre">dim0</span></code> of this shape.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id73">
<h3>device<a class="headerlink" href="#id73" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.device">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#k2.ragged.RaggedShape.device" title="Permalink to this definition"></a></dt>
<dd><p>Return the device of this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[[]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id74">
<h3>dim0<a class="headerlink" href="#id74" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.dim0">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">dim0</span></span><a class="headerlink" href="#k2.ragged.RaggedShape.dim0" title="Permalink to this definition"></a></dt>
<dd><p>Return number of sublists at axis 0.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x] [] [x x x x x]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[x] []] [[]] [[x] [x x] [x x x]] [[]]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">4</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id75">
<h3>num_axes<a class="headerlink" href="#id75" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedShape.num_axes">
<span class="sig-prename descclassname"><span class="pre">RaggedShape.</span></span><span class="sig-name descname"><span class="pre">num_axes</span></span><a class="headerlink" href="#k2.ragged.RaggedShape.num_axes" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of axes of this shape.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[[] []]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [[]] [[]]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape2</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<section id="id76">
<h2>RaggedTensor<a class="headerlink" href="#id76" title="Permalink to this headline"></a></h2>
<section id="id77">
<h3>__eq__<a class="headerlink" href="#id77" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.__eq__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__eq__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.__eq__" title="Permalink to this definition"></a></dt>
<dd><p>Compare two ragged tensors.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two tensors MUST have the same dtype. Otherwise,
it throws an exception.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">==</span>  <span class="n">b</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">try</span><span class="p">:</span>
<span class="gp">... </span>  <span class="n">c</span> <span class="o">==</span> <span class="n">b</span>
<span class="gp">... </span><span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;raised exception&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The tensor to be compared.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the two tensors are equal.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id78">
<h3>__getitem__<a class="headerlink" href="#id78" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.__getitem__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__getitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.ragged.RaggedTensor.__getitem__" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>__getitem__(self: _k2.ragged.RaggedTensor, i: int) -&gt; object</p></li>
</ol>
<p>Select the i-th sublist along axis 0.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Support for autograd is to be implemented.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1 3] [] [9]]  [[8]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1, 3],</span>
<span class="go">               [],</span>
<span class="go">               [9]],</span>
<span class="go">              [[8]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">RaggedTensor([[8]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [1 3] [9] [8] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              [9],</span>
<span class="go">              [8]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">tensor([1, 3], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">tensor([9], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>i</strong> – The i-th sublist along axis 0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new ragged tensor with one fewer axis. If <cite>num_axes == 2</cite>, the
return value will be a 1D tensor.</p>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>__getitem__(self: _k2.ragged.RaggedTensor, key: slice) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Slices sublists along axis 0 with the given range. Only support slicing step
equals to 1.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Support for autograd is to be implemented.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1 3] [] [9]]  [[8]] [[10 11]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1, 3],</span>
<span class="go">               [],</span>
<span class="go">               [9]],</span>
<span class="go">              [[8]],</span>
<span class="go">              [[10, 11]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="go">RaggedTensor([[[1, 3],</span>
<span class="go">               [],</span>
<span class="go">               [9]],</span>
<span class="go">              [[8]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="go">RaggedTensor([[[8]]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – Slice containing integer constants.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new ragged tensor with the same axes as original ragged tensor, but
only contains the sublists within the range.</p>
</dd>
</dl>
<ol class="arabic simple" start="3">
<li><p>__getitem__(self: _k2.ragged.RaggedTensor, key: torch.Tensor) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Slice a ragged tensor along axis 0 using a 1-D torch.int32 tensor.</p>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span> <span class="p">[</span><span class="mi">300</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">tensor([1, 2, 0], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="go">RaggedTensor([[300],</span>
<span class="go">              [-10, 0, -1],</span>
<span class="go">              [10, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">tensor([0, 1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="go">RaggedTensor([[10, 20],</span>
<span class="go">              [300]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="go">tensor([2, 3], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
<span class="go">RaggedTensor([[-10, 0, -1],</span>
<span class="go">              [-2, 4, 5]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [2, 3],</span>
<span class="go">               [0]],</span>
<span class="go">              [[10, 20]],</span>
<span class="go">              [[],</span>
<span class="go">               [2]],</span>
<span class="go">              [[1],</span>
<span class="go">               [2, 3],</span>
<span class="go">               [0]]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – A 1-D torch.int32 tensor containing the indexes to select along
axis 0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a new ragged tensor with the same number of axes as <code class="docutils literal notranslate"><span class="pre">self</span></code> but
only contains the specified sublists.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id79">
<h3>__getstate__<a class="headerlink" href="#id79" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.__getstate__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__getstate__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">k2.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.__getstate__" title="Permalink to this definition"></a></dt>
<dd><p>Requires a tensor with 2 axes or 3 axes. Other number
of axes are not implemented yet.</p>
<p>This method is to support <code class="docutils literal notranslate"><span class="pre">pickle</span></code>, e.g., used by <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code>.
You are not expected to call it by yourself.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>If this tensor has 2 axes, return a tuple containing
(self.row_splits(1), “row_ids1”, self.values).
If this tensor has 3 axes, return a tuple containing
(self.row_splits(1), “row_ids1”, self.row_splits(1),
“row_ids2”, self.values)</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>“row_ids1” and “row_ids2” in the returned value is for
backward compatibility.</p>
</div>
</dd></dl>

</section>
<section id="id80">
<h3>__init__<a class="headerlink" href="#id80" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.__init__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.ragged.RaggedTensor.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>__init__(self: _k2.ragged.RaggedTensor, data: list, dtype: object = None, device: object = ‘cpu’) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor with arbitrary number of axes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A ragged tensor has at least two axes.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              []], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [2, 3]],</span>
<span class="go">              [[4],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[]]</span> <span class="p">])</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [],</span>
<span class="go">              [[]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[]]</span> <span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [],</span>
<span class="go">              [[]]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – A list-of sublist(s) of integers or real numbers.
It can have arbitrary number of axes (at least two).</p></li>
<li><p><strong>dtype</strong> – Optional. If None, it infers the dtype from <code class="docutils literal notranslate"><span class="pre">data</span></code>
automatically, which is either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>__init__(self: _k2.ragged.RaggedTensor, data: list, dtype: object = None, device: str = ‘cpu’) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor with arbitrary number of axes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A ragged tensor has at least two axes.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              []], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [2, 3]],</span>
<span class="go">              [[4],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_splits</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[]]</span> <span class="p">])</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [],</span>
<span class="go">              [[]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[],</span> <span class="p">[[]]</span> <span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [],</span>
<span class="go">              [[]]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – A list-of sublist(s) of integers or real numbers.
It can have arbitrary number of axes (at least two).</p></li>
<li><p><strong>dtype</strong> – Optional. If None, it infers the dtype from <code class="docutils literal notranslate"><span class="pre">data</span></code>
automatically, which is either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="3">
<li><p>__init__(self: _k2.ragged.RaggedTensor, s: str, dtype: object = None, device: object = ‘cpu’) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor from its string representation.</p>
<p>Fields are separated by space(s) <strong>or</strong> comma(s).</p>
<p>An example string for a 2-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>An example string for a 3-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]]</span> <span class="p">[[]]</span> <span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [1] [] [3 4] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [3, 4]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[] [3]]  [[10]] ]&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[],</span>
<span class="go">               [3]],</span>
<span class="go">              [[10]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1]], device=&#39;cuda:0&#39;, dtype=torch.float32)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Number of spaces or commas in <code class="docutils literal notranslate"><span class="pre">s</span></code> does not affect the result.
Of course, numbers have to be separated by at least one space or comma.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> – A string representation of a ragged tensor.</p></li>
<li><p><strong>dtype</strong> – The desired dtype of the tensor. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it tries
to infer the correct dtype from <code class="docutils literal notranslate"><span class="pre">s</span></code>, which is assumed to be
either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are:
<code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="4">
<li><p>__init__(self: _k2.ragged.RaggedTensor, s: str, dtype: object = None, device: str = ‘cpu’) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor from its string representation.</p>
<p>Fields are separated by space(s) <strong>or</strong> comma(s).</p>
<p>An example string for a 2-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>An example string for a 3-axis ragged tensor is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]]</span> <span class="p">[[]]</span> <span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [1] [] [3 4] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [3, 4]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[] [3]]  [[10]] ]&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[],</span>
<span class="go">               [3]],</span>
<span class="go">              [[10]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1.]]&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1]], device=&#39;cuda:0&#39;, dtype=torch.float32)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Number of spaces or commas in <code class="docutils literal notranslate"><span class="pre">s</span></code> does not affect the result.
Of course, numbers have to be separated by at least one space or comma.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>s</strong> – A string representation of a ragged tensor.</p></li>
<li><p><strong>dtype</strong> – The desired dtype of the tensor. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it tries
to infer the correct dtype from <code class="docutils literal notranslate"><span class="pre">s</span></code>, which is assumed to be
either <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>. Supported dtypes are:
<code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p></li>
<li><p><strong>device</strong> – It can be either an instance of <code class="docutils literal notranslate"><span class="pre">torch.device</span></code> or
a string representing a torch device. Example
values are: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cpu&quot;)</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.device(&quot;cuda&quot;,</span> <span class="pre">0)</span></code>.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="5">
<li><p>__init__(self: _k2.ragged.RaggedTensor, shape: _k2.ragged.RaggedShape, value: torch.Tensor) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor from a shape and a value.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedShape</span><span class="p">(</span><span class="s1">&#39;[ [x x] [] [x x x] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ragged</span>
<span class="go">RaggedTensor([[10, 0],</span>
<span class="go">              [],</span>
<span class="go">              [20, 30, 40]], dtype=torch.float32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> – The shape of the tensor.</p></li>
<li><p><strong>value</strong> – The value of the tensor.</p></li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="6">
<li><p>__init__(self: _k2.ragged.RaggedTensor, tensor: torch.Tensor) -&gt; None</p></li>
</ol>
<p>Create a ragged tensor from a torch tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It turns a regular tensor into a ragged tensor.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The input tensor has to have more than 1 dimension.
That is <code class="docutils literal notranslate"><span class="pre">tensor.ndim</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>.</p>
<p>Also, if the input tensor is contiguous, <code class="docutils literal notranslate"><span class="pre">self</span></code>
will share the underlying memory with it. Otherwise,
memory of the input tensor is copied to create <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>Supported dtypes of the input tensor are: <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>,
<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 1, 2],</span>
<span class="go">        [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 1, 2],</span>
<span class="go">              [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[10, 1, 2],</span>
<span class="go">              [3, 4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[10, -2,  2],</span>
<span class="go">        [ 3,  4,  5]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">)[:,</span> <span class="p">::</span><span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0,  4,  8],</span>
<span class="go">        [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 4, 8],</span>
<span class="go">              [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 4, 8],</span>
<span class="go">              [12, 16, 20]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[10,  4,  8],</span>
<span class="go">        [12, 16, 20]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 3</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[[ 0.,  1.,  2.,  3.],</span>
<span class="go">         [ 4.,  5.,  6.,  7.],</span>
<span class="go">         [ 8.,  9., 10., 11.]],</span>
<span class="go">        [[12., 13., 14., 15.],</span>
<span class="go">         [16., 17., 18., 19.],</span>
<span class="go">         [20., 21., 22., 23.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[0, 1, 2, 3],</span>
<span class="go">               [4, 5, 6, 7],</span>
<span class="go">               [8, 9, 10, 11]],</span>
<span class="go">              [[12, 13, 14, 15],</span>
<span class="go">               [16, 17, 18, 19],</span>
<span class="go">               [20, 21, 22, 23]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 2]], device=&#39;cuda:0&#39;, dtype=torch.float32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – An N-D (N &gt; 1) tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id81">
<h3>__ne__<a class="headerlink" href="#id81" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.__ne__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__ne__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.__ne__" title="Permalink to this definition"></a></dt>
<dd><p>Compare two ragged tensors.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The two tensors MUST have the same dtype. Otherwise,
it throws an exception.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">!=</span> <span class="n">a</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">!=</span> <span class="n">a</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> – The tensor to be compared.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the two tensors are NOT equal.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id82">
<h3>__repr__<a class="headerlink" href="#id82" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.__repr__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__repr__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.__repr__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [2, 3],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&#39;RaggedTensor([[1],\n              [2, 3],\n              []], dtype=torch.int32)&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 2]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id83">
<h3>__setstate__<a class="headerlink" href="#id83" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.__setstate__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__setstate__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">k2.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.__setstate__" title="Permalink to this definition"></a></dt>
<dd><p>Set the content of this class from <code class="docutils literal notranslate"><span class="pre">arg0</span></code>.</p>
<p>This method is to support <code class="docutils literal notranslate"><span class="pre">pickle</span></code>, e.g., used by torch.load().
You are not expected to call it by yourself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>arg0</strong> – It is the return value from the method <code class="docutils literal notranslate"><span class="pre">__getstate__</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id84">
<h3>__str__<a class="headerlink" href="#id84" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.__str__">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.__str__" title="Permalink to this definition"></a></dt>
<dd><p>Return a string representation of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [2, 3],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&#39;RaggedTensor([[1],\n              [2, 3],\n              []], dtype=torch.int32)&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 2]], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id85">
<h3>add<a class="headerlink" href="#id85" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.add">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.add" title="Permalink to this definition"></a></dt>
<dd><p>Add value scaled by alpha to source ragged tensor over the last axis.</p>
<p>It implements:</p>
<blockquote>
<div><p>dest[…][i][j] = src[…][i][j] + alpha * value[i]</p>
</div></blockquote>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[2, 4],</span>
<span class="go">              [3],</span>
<span class="go">              [5, 11]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[0, 2],</span>
<span class="go">              [-1],</span>
<span class="go">              [-1, 5]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> – The value to be added to the <code class="docutils literal notranslate"><span class="pre">self</span></code>, whose dimension MUST
equal the number of sublists along the last dimension of <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p></li>
<li><p><strong>alpha</strong> – The number used to scaled value before adding to <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a new RaggedTensor, sharing the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id86">
<h3>arange<a class="headerlink" href="#id86" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.arange">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">arange</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">begin</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.arange" title="Permalink to this definition"></a></dt>
<dd><p>Return a sub-range of <code class="docutils literal notranslate"><span class="pre">self</span></code> containing indexes <code class="docutils literal notranslate"><span class="pre">begin</span></code>
through <code class="docutils literal notranslate"><span class="pre">end</span> <span class="pre">-</span> <span class="pre">1</span></code> along axis <code class="docutils literal notranslate"><span class="pre">axis</span></code> of <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">axis</span></code> argument may be confusing; its behavior is equivalent to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
  <span class="bp">self</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The returned tensor shares the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</div>
<p><strong>Example 1</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [],</span>
<span class="go">               [2]],</span>
<span class="go">              [[],</span>
<span class="go">               [4, 5],</span>
<span class="go">               []],</span>
<span class="go">              [[],</span>
<span class="go">               [1]],</span>
<span class="go">              [[]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[],</span>
<span class="go">               [4, 5],</span>
<span class="go">               []],</span>
<span class="go">              [[],</span>
<span class="go">               [1]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[[],</span>
<span class="go">               [4, 5],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [2],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">RaggedTensor([[2],</span>
<span class="go">              [],</span>
<span class="go">              [4, 5]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[]]],</span> <span class="p">[[[],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[[],</span>
<span class="go">                [1],</span>
<span class="go">                [2, 3]],</span>
<span class="go">               [[5, 8],</span>
<span class="go">                [],</span>
<span class="go">                [9]]],</span>
<span class="go">              [[[10],</span>
<span class="go">                [0],</span>
<span class="go">                []]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[[5, 8],</span>
<span class="go">               [],</span>
<span class="go">               [9]],</span>
<span class="go">              [[10],</span>
<span class="go">               [0],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[],</span>
<span class="go">              [1],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [5, 8],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
</pre></div>
</div>
<p><strong>Example 3</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[0],</span>
<span class="go">              [1],</span>
<span class="go">              [2],</span>
<span class="go">              [],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[0],</span>
<span class="go">              [-1],</span>
<span class="go">              [2],</span>
<span class="go">              [],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> – The axis from which <code class="docutils literal notranslate"><span class="pre">begin</span></code> and <code class="docutils literal notranslate"><span class="pre">end</span></code> correspond to.</p></li>
<li><p><strong>begin</strong> – The beginning of the range (inclusive).</p></li>
<li><p><strong>end</strong> – The end of the range (exclusive).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="id87">
<h3>argmax<a class="headerlink" href="#id87" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.argmax">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">argmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.argmax" title="Permalink to this definition"></a></dt>
<dd><p>Return a tensor containing maximum value indexes within each sub-list along the
last axis of <code class="docutils literal notranslate"><span class="pre">self</span></code>, i.e. the max taken over the last axis, The index is -1
if the sub-list was empty or all values in the sub-list are less
than <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="go">tensor([ 0, -1, -1, -1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([ 0, -1, -1, -1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="go">tensor([ 3, -1,  7], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">tensor([ 3, -1,  7], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span>
<span class="go">(tensor(5, dtype=torch.int32), tensor(8, dtype=torch.int32))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="go">tensor([-1, -1,  7], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([ 3, -1,  7], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([ 3, -1,  7], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – A base value to compare. If values in a sublist are all less
than this value, then the <code class="docutils literal notranslate"><span class="pre">argmax</span></code> of this sublist is -1.
If a sublist is empty, the <code class="docutils literal notranslate"><span class="pre">argmax</span></code> of it is also -1.
If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the lowest value of <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code> is used.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a 1-D <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> tensor. It is on the same device
as <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id88">
<h3>cat<a class="headerlink" href="#id88" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.cat">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">cat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">srcs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedTensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.cat" title="Permalink to this definition"></a></dt>
<dd><p>Concatenate a list of ragged tensor over a specified axis.</p>
<p><strong>Example 1</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3, 2, 3]], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 3],</span>
<span class="go">              [],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [],</span>
<span class="go">              [9],</span>
<span class="go">              [0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [],</span>
<span class="go">              [-1],</span>
<span class="go">              [10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[1, 3, 0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [-1],</span>
<span class="go">              [9, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 3, 0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [-1],</span>
<span class="go">              [9, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2r</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">RaggedTensor([[0],</span>
<span class="go">              [1, 8],</span>
<span class="go">              [],</span>
<span class="go">              [-1],</span>
<span class="go">              [10],</span>
<span class="go">              [1, 3],</span>
<span class="go">              [],</span>
<span class="go">              [5, 8],</span>
<span class="go">              [],</span>
<span class="go">              [9]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>srcs</strong> – A list (or a tuple) of ragged tensors to concatenate. They <strong>MUST</strong> all
have the same dtype and on the same device.</p></li>
<li><p><strong>axis</strong> – Only 0 and 1 are supported right now. If it is 1, then
<code class="docutils literal notranslate"><span class="pre">srcs[i].dim0</span></code> must all have the same value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a concatenated tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id89">
<h3>clone<a class="headerlink" href="#id89" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.clone">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">clone</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.clone" title="Permalink to this definition"></a></dt>
<dd><p>Return a copy of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[10, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[-1, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[10, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[10, 2],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id90">
<h3>index<a class="headerlink" href="#id90" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.index">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.ragged.RaggedTensor.index" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>index(self: _k2.ragged.RaggedTensor, indexes: _k2.ragged.RaggedTensor) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Index a ragged tensor with a ragged tensor.</p>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mf">13.5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indexes</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span>
<span class="go">RaggedTensor([[[10, 11],</span>
<span class="go">               [12, 13.5]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="go">RaggedTensor([[[10, 11]],</span>
<span class="go">              [[12, 13.5]],</span>
<span class="go">              [[10, 11],</span>
<span class="go">               [10, 11]]], dtype=torch.float32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="go">RaggedTensor([[[[[1, 0],</span>
<span class="go">                 [],</span>
<span class="go">                 [2]],</span>
<span class="go">                [[1, 2],</span>
<span class="go">                 [-1]]],</span>
<span class="go">               [[[],</span>
<span class="go">                 [3],</span>
<span class="go">                 [0, 0, 1]]]],</span>
<span class="go">              [[[[1, 0],</span>
<span class="go">                 [],</span>
<span class="go">                 [2]]]]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>indexes</strong> – <p>Its values must satisfy <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">values[i]</span> <span class="pre">&lt;</span> <span class="pre">self.dim0</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Its dtype has to be <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>.</p>
</div>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return indexed tensor.</p>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>index(self: _k2.ragged.RaggedTensor, indexes: torch.Tensor, axis: int, need_value_indexes: bool = False) -&gt; Tuple[_k2.ragged.RaggedTensor, Optional[torch.Tensor]]</p></li>
</ol>
<p>Indexing operation on ragged tensor, returns <code class="docutils literal notranslate"><span class="pre">self[indexes]</span></code>, where
the elements of <code class="docutils literal notranslate"><span class="pre">indexes</span></code> are interpreted as indexes into axis <code class="docutils literal notranslate"><span class="pre">axis</span></code> of
<code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">indexes</span></code> is a 1-D tensor and <code class="docutils literal notranslate"><span class="pre">indexes.dtype</span> <span class="pre">==</span> <span class="pre">torch.int32</span></code>.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.25</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">value_indexes</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[0, 1, 2],</span>
<span class="go">              [0, 2, 3],</span>
<span class="go">              [],</span>
<span class="go">              [3, -1.25]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_indexes</span>
<span class="go">tensor([3, 4, 5, 0, 1, 2, 6, 7], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">value_indexes</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([ 0.0000,  1.0000,  2.0000,  0.0000,  2.0000,  3.0000,  3.0000, -1.2500])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[0, 1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [0, 2, 3]], dtype=torch.float32), tensor([3, 4, 5, 0, 1, 2], dtype=torch.int32))</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">row_ids</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="n">i</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([0, 0, 0, 1, 1, 1, 1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">value_indexes</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">need_value_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[1, 3],</span>
<span class="go">               [2],</span>
<span class="go">               []],</span>
<span class="go">              [[2],</span>
<span class="go">               [5, 8],</span>
<span class="go">               [-1],</span>
<span class="go">               []]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_indexes</span>
<span class="go">tensor([0, 1, 2, 6, 3, 4, 5], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">value_indexes</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([ 1,  3,  2,  2,  5,  8, -1], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indexes</strong> – <p>Array of indexes, which will be interpreted as indexes into axis <code class="docutils literal notranslate"><span class="pre">axis</span></code>
of <code class="docutils literal notranslate"><span class="pre">self</span></code>, i.e. with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">indexes[i]</span> <span class="pre">&lt;</span> <span class="pre">self.tot_size(axis)</span></code>.
Note that if <code class="docutils literal notranslate"><span class="pre">axis</span></code> is 0, then -1 is also a valid entry in <code class="docutils literal notranslate"><span class="pre">index</span></code>,
-1 as an index, which will result in an empty list (as if it were the index
into a position in <code class="docutils literal notranslate"><span class="pre">self</span></code> that had an empty list at that point).</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It is currently not allowed to change the order on axes less than
<code class="docutils literal notranslate"><span class="pre">axis</span></code>, i.e. if <code class="docutils literal notranslate"><span class="pre">axis</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>, we require:
<code class="docutils literal notranslate"><span class="pre">IsMonotonic(self.shape.row_ids(axis)[indexes])</span></code>.</p>
</div>
</p></li>
<li><p><strong>axis</strong> – The axis to be indexed. Must satisfy <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">axis</span> <span class="pre">&lt;</span> <span class="pre">self.num_axes</span></code>.</p></li>
<li><p><strong>need_value_indexes</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, it will return a torch.Tensor containing the indexes into
<code class="docutils literal notranslate"><span class="pre">self.values</span></code> that <code class="docutils literal notranslate"><span class="pre">ans.values</span></code> has, as in
<code class="docutils literal notranslate"><span class="pre">ans.values</span> <span class="pre">=</span> <span class="pre">self.values[value_indexes]</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>A ragged tensor, sharing the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> if <code class="docutils literal notranslate"><span class="pre">need_value_indexes</span></code> is False; a 1-D torch.tensor of
dtype <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> containing the indexes into <code class="docutils literal notranslate"><span class="pre">self.values</span></code> that
<code class="docutils literal notranslate"><span class="pre">ans.values</span></code> has.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Return a tuple containing</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id91">
<h3>logsumexp<a class="headerlink" href="#id91" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.logsumexp">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">logsumexp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-</span> <span class="pre">inf</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.logsumexp" title="Permalink to this definition"></a></dt>
<dd><p>Compute the logsumexp of sublists over the last axis of this tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is similar to torch.logsumexp except it accepts a ragged tensor.
See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logsumexp.html">https://pytorch.org/docs/stable/generated/torch.logsumexp.html</a>
for definition of logsumexp.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a sublist is empty, the logsumexp for it is the provided
<code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation only supports float type input,
i.e., with dtype being torch.float32 or torch.float64.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[-0.25, -0.25, -0.25, -0.25],</span>
<span class="go">              [],</span>
<span class="go">              [-0.5, -0.5]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([1.1363,   -inf, 0.1931], grad_fn=&lt;LogSumExpFunction&gt;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor(-inf, grad_fn=&lt;SumBackward0&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.5000, 0.5000])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># if a is a 3-d ragged tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[[-0.25, -0.25, -0.25, -0.25]],</span>
<span class="go">              [[],</span>
<span class="go">               [-0.5, -0.5]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([1.1363,   -inf, 0.1931], grad_fn=&lt;LogSumExpFunction&gt;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor(-inf, grad_fn=&lt;SumBackward0&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.5000, 0.5000])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – If a sublist is empty, its logsumexp is this value.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a 1-D tensor with the same dtype of this tensor
containing the computed logsumexp.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id93">
<h3>max<a class="headerlink" href="#id93" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.max">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.max" title="Permalink to this definition"></a></dt>
<dd><p>Return a tensor containing the maximum of each sub-list along the last
axis of <code class="docutils literal notranslate"><span class="pre">self</span></code>. The max is taken over the last axis or <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>,
whichever was larger.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="go">tensor([          3,           5, -2147483648, -2147483648,           9,</span>
<span class="go">        -2147483648,           8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=-</span><span class="mi">10</span><span class="p">)</span>
<span class="go">tensor([  3,   5, -10, -10,   9, -10,   8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="go">tensor([7, 7, 7, 7, 9, 7, 8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([ 3.,  5., -3., -3.,  9., -3.,  8.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([ 3,  5, -2, -2,  9, -2,  8], device=&#39;cuda:0&#39;, dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – The base value to compare. If values in a sublist are all less
than this value, then the max of this sublist is <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.
If a sublist is empty, its max is also <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return 1-D tensor containing the max value of each sublist.
It shares the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id94">
<h3>min<a class="headerlink" href="#id94" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.min">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.min" title="Permalink to this definition"></a></dt>
<dd><p>Return a tensor containing the minimum of each sub-list along the last
axis of <code class="docutils literal notranslate"><span class="pre">self</span></code>. The min is taken over the last axis or <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>,
whichever was smaller.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="go">tensor([ 0.0000e+00, -1.0000e+00,  3.4028e+38,  3.4028e+38,  1.0000e+00,</span>
<span class="go">         3.4028e+38,  2.0000e+00])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
<span class="go">tensor([ 0., -1., inf, inf,  1., inf,  2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="go">tensor([  0.,  -1., 100., 100.,   1., 100.,   2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="go">tensor([ 0, -1, 20, 20,  1, 20,  2], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
<span class="go">tensor([ 0., -1., 15., 15.,  1., 15.,  2.], device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – The base value to compare. If values in a sublist are all larger
than this value, then the minimum of this sublist is <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.
If a sublist is empty, its minimum is also <code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return 1-D tensor containing the minimum of each sublist.
It shares the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id95">
<h3>normalize<a class="headerlink" href="#id95" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.normalize">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">normalize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_log</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.normalize" title="Permalink to this definition"></a></dt>
<dd><p>Normalize a ragged tensor over the last axis.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">use_log</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the normalization per sublist is done as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Compute the log sum per sublist</p></li>
</ol>
<p>2. Subtract the log sum computed above from the sublist and return
it</p>
</div></blockquote>
<p>If <code class="docutils literal notranslate"><span class="pre">use_log</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the normalization per sublist is done as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Compute the sum per sublist</p></li>
<li><p>Divide the sublist by the above sum and return the resulting sublist</p></li>
</ol>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a sublist contains 3 elements <code class="docutils literal notranslate"><span class="pre">[a,</span> <span class="pre">b,</span> <span class="pre">c]</span></code>, then the log sum
is defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
</pre></div>
</div>
<p>The resulting sublist looks like below if <code class="docutils literal notranslate"><span class="pre">use_log</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">a</span> <span class="o">-</span> <span class="n">s</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">s</span><span class="p">,</span> <span class="n">c</span> <span class="o">-</span> <span class="n">s</span><span class="p">]</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">use_log</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the resulting sublist looks like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">a</span><span class="o">/</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">),</span> <span class="n">b</span><span class="o">/</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">),</span> <span class="n">c</span><span class="o">/</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">use_log</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">RaggedTensor([[0.25, 0.75],</span>
<span class="go">              [],</span>
<span class="go">              [1],</span>
<span class="go">              [0.2, 0.8]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">use_log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[-0.798139, -0.598139],</span>
<span class="go">              [],</span>
<span class="go">              [0],</span>
<span class="go">              [-1.03749, -0.437488]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">use_log</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">RaggedTensor([[[0.25, 0.75],</span>
<span class="go">               []],</span>
<span class="go">              [[1],</span>
<span class="go">               [0.2, 0.8]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">use_log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[[-0.798139, -0.598139],</span>
<span class="go">               []],</span>
<span class="go">              [[0],</span>
<span class="go">               [-1.03749, -0.437488]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
<span class="go">tensor([-0.7981, -0.5981])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_log</strong> – It indicates which kind of normalization to be applied.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a 1-D tensor, sharing the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id96">
<h3>numel<a class="headerlink" href="#id96" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.numel">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">numel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.numel" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return number of elements in this tensor. It equals to
<code class="docutils literal notranslate"><span class="pre">self.values.numel()</span></code>.</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1] [] []]  [[2 3]]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[[1] [] [3 4 5 6]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="go">5</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id97">
<h3>pad<a class="headerlink" href="#id97" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.pad">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">pad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.pad" title="Permalink to this definition"></a></dt>
<dd><p>Pad a ragged tensor with 2-axes to a 2-D torch tensor.</p>
<p>For example, if <code class="docutils literal notranslate"><span class="pre">self</span></code> has the following values:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span><span class="p">]</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span><span class="p">]</span> <span class="p">]</span>
</pre></div>
</div>
<p>Then it returns a 2-D tensor as follows if <code class="docutils literal notranslate"><span class="pre">padding_value</span></code> is 0 and
mode is <code class="docutils literal notranslate"><span class="pre">constant</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
</pre></div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It requires that <code class="docutils literal notranslate"><span class="pre">self.num_axes</span> <span class="pre">==</span> <span class="pre">2</span></code>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1, -1, -1, -1, -1],</span>
<span class="go">        [-1, -1, -1, -1, -1],</span>
<span class="go">        [ 2,  3, -1, -1, -1],</span>
<span class="go">        [ 5,  8,  9,  8,  2]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;replicate&#39;</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1,  1,  1,  1,  1],</span>
<span class="go">        [-1, -1, -1, -1, -1],</span>
<span class="go">        [ 2,  3,  3,  3,  3],</span>
<span class="go">        [ 5,  8,  9,  8,  2]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> – Valid values are: <code class="docutils literal notranslate"><span class="pre">constant</span></code>, <code class="docutils literal notranslate"><span class="pre">replicate</span></code>. If it is
<code class="docutils literal notranslate"><span class="pre">constant</span></code>, the given <code class="docutils literal notranslate"><span class="pre">padding_value</span></code> is used for filling.
If it is <code class="docutils literal notranslate"><span class="pre">replicate</span></code>, the last entry in a list is used for filling.
If a list is empty, then the given <cite>padding_value</cite> is also used for filling.</p></li>
<li><p><strong>padding_value</strong> – The filling value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 2-D torch tensor, sharing the same dtype and device with <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id98">
<h3>remove_axis<a class="headerlink" href="#id98" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.remove_axis">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">remove_axis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.remove_axis" title="Permalink to this definition"></a></dt>
<dd><p>Remove an axis; if it is not the first or last axis, this is done by appending
lists (effectively the axis is combined with the following axis).  If it is the
last axis it is just removed and the number of elements may be changed.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The tensor has to have more than two axes.</p>
</div>
<p><strong>Example 1</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [],</span>
<span class="go">               [0, -1]],</span>
<span class="go">              [[],</span>
<span class="go">               [2, 3],</span>
<span class="go">               []],</span>
<span class="go">              [[0]],</span>
<span class="go">              [[]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1],</span>
<span class="go">              [],</span>
<span class="go">              [0, -1],</span>
<span class="go">              [],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [],</span>
<span class="go">              [0],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 0, -1],</span>
<span class="go">              [2, 3],</span>
<span class="go">              [0],</span>
<span class="go">              []], dtype=torch.int32)</span>
</pre></div>
</div>
<p><strong>Example 2</strong>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[]]],</span> <span class="p">[[[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[[1],</span>
<span class="go">                [],</span>
<span class="go">                [2]]],</span>
<span class="go">              [[[3, 4],</span>
<span class="go">                [],</span>
<span class="go">                [5, 6],</span>
<span class="go">                []]],</span>
<span class="go">              [[[],</span>
<span class="go">                [0]]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [],</span>
<span class="go">               [2]],</span>
<span class="go">              [[3, 4],</span>
<span class="go">               [],</span>
<span class="go">               [5, 6],</span>
<span class="go">               []],</span>
<span class="go">              [[],</span>
<span class="go">               [0]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[[1],</span>
<span class="go">               [],</span>
<span class="go">               [2]],</span>
<span class="go">              [[3, 4],</span>
<span class="go">               [],</span>
<span class="go">               [5, 6],</span>
<span class="go">               []],</span>
<span class="go">              [[],</span>
<span class="go">               [0]]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[[1, 2]],</span>
<span class="go">              [[3, 4, 5, 6]],</span>
<span class="go">              [[0]]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> – The axis to move.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor with one fewer axes.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id99">
<h3>remove_values_eq<a class="headerlink" href="#id99" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.remove_values_eq">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">remove_values_eq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.remove_values_eq" title="Permalink to this definition"></a></dt>
<dd><p>Returns a ragged tensor after removing all ‘values’ that equal a provided
target.  Leaves all layers of the shape except for the last one unaffected.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2, 3, 0, 3, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3, 2, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_eq</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[1, 2, 0, 2],</span>
<span class="go">              [],</span>
<span class="go">              [2],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_eq</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[1, 3, 0, 3],</span>
<span class="go">              [],</span>
<span class="go">              [3, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> – The target value to delete.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor whose values don’t contain the <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id100">
<h3>remove_values_leq<a class="headerlink" href="#id100" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.remove_values_leq">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">remove_values_leq</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.remove_values_leq" title="Permalink to this definition"></a></dt>
<dd><p>Returns a ragged tensor after removing all ‘values’ that are
equal to or less than a provided cutoff.
Leaves all layers of the shape except for the last one unaffected.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2, 3, 0, 3, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3, 2, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_leq</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">RaggedTensor([[],</span>
<span class="go">              [],</span>
<span class="go">              [],</span>
<span class="go">              []], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_leq</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">RaggedTensor([[3, 3],</span>
<span class="go">              [],</span>
<span class="go">              [3, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">remove_values_leq</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">RaggedTensor([[2, 3, 3, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3, 2, 3],</span>
<span class="go">              [3]], dtype=torch.int32)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cutoff</strong> – Values less than or equal to this <code class="docutils literal notranslate"><span class="pre">cutoff</span></code> are deleted.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a ragged tensor whose values are all above <code class="docutils literal notranslate"><span class="pre">cutoff</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id101">
<h3><a href="#id133"><span class="problematic" id="id134">requires_grad_</span></a><a class="headerlink" href="#id101" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.requires_grad_">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_k2.ragged.RaggedTensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.requires_grad_" title="Permalink to this definition"></a></dt>
<dd><p>Change if autograd should record operations on this tensor: Set
this tensor’s <a class="reference internal" href="#k2.ragged.RaggedTensor.requires_grad" title="k2.ragged.RaggedTensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> attribute <strong>in-place</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this tensor is not a float tensor, PyTorch will throw a
RuntimeError exception.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This method ends with an underscore, meaning it changes this tensor
<strong>in-place</strong>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[1]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> – If autograd should record operations on this tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return this tensor.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id102">
<h3><a href="#id135"><span class="problematic" id="id136">sort_</span></a><a class="headerlink" href="#id102" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.sort_">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">sort_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">descending</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_new2old_indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.sort_" title="Permalink to this definition"></a></dt>
<dd><p>Sort a ragged tensor over the last axis <strong>in-place</strong>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><code class="docutils literal notranslate"><span class="pre">sort_</span></code> ends with an underscore, meaning this operation
changes <code class="docutils literal notranslate"><span class="pre">self</span></code> <strong>in-place</strong>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_clone</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sort_</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([1, 0, 2, 4, 5, 3, 7, 6, 8], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[3, 1, 0],</span>
<span class="go">              [5, 3, 2],</span>
<span class="go">              [],</span>
<span class="go">              [3, 1, 0]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_clone</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">b</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([3., 1., 0., 5., 3., 2., 3., 1., 0.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_clone</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sort_</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor([2, 1, 0, 5, 4, 3, 8, 7, 6], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[0, 1, 3],</span>
<span class="go">              [2, 3, 5],</span>
<span class="go">              [],</span>
<span class="go">              [0, 1, 3]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_clone</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
<span class="go">tensor([0., 1., 3., 2., 3., 5., 0., 1., 3.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>descending</strong> – <code class="docutils literal notranslate"><span class="pre">True</span></code> to sort in <strong>descending</strong> order.
<code class="docutils literal notranslate"><span class="pre">False</span></code> to sort in <strong>ascending</strong> order.</p></li>
<li><p><strong>need_new2old_indexes</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, also returns a 1-D tensor, containing the indexes mapping
from the sorted elements to the unsorted elements. We can use
<code class="docutils literal notranslate"><span class="pre">self.clone().values[returned_tensor]</span></code> to get a sorted tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If <code class="docutils literal notranslate"><span class="pre">need_new2old_indexes</span></code> is False, returns None. Otherwise, returns
a 1-D tensor of dtype <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id103">
<h3>sum<a class="headerlink" href="#id103" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.sum">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">sum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.sum" title="Permalink to this definition"></a></dt>
<dd><p>Compute the sum of sublists over the last axis of this tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a sublist is empty, the sum for it is the provided
<code class="docutils literal notranslate"><span class="pre">initial_value</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation supports autograd if this tensor is a float tensor,
i.e., with dtype being torch.float32 or torch.float64.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1 2] [] [5]]  [[10]] ]&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[[1, 2],</span>
<span class="go">               [],</span>
<span class="go">               [5]],</span>
<span class="go">              [[10]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0., 0., 2., 3.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([ 3.,  0.,  5., 10.], grad_fn=&lt;SumFunction&gt;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span>
<span class="go">tensor(40., grad_fn=&lt;SumBackward0&gt;)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initial_value</strong> – This value is added to the sum of each sublist. So when
a sublist is empty, its sum is this value.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a 1-D tensor with the same dtype of this tensor
containing the computed sum.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id104">
<h3>to<a class="headerlink" href="#id104" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.to">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#k2.ragged.RaggedTensor.to" title="Permalink to this definition"></a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic simple">
<li><p>to(self: _k2.ragged.RaggedTensor, device: object) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Transfer this tensor to a given device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">self</span></code> is already on the specified device, return a
ragged tensor sharing the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.
Otherwise, a new tensor is returned.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – The target device to move this tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a tensor on the given device.</p>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>to(self: _k2.ragged.RaggedTensor, device: str) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Transfer this tensor to a given device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">self</span></code> is already on the specified device, return a
ragged tensor sharing the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.
Otherwise, a new tensor is returned.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=1)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – The target device to move this tensor.
Note: The device is represented as a string.
Valid strings are: “cpu”, “cuda:0”, “cuda:1”, etc.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a tensor on the given device.</p>
</dd>
</dl>
<ol class="arabic simple" start="3">
<li><p>to(self: _k2.ragged.RaggedTensor, dtype: torch::dtype) -&gt; _k2.ragged.RaggedTensor</p></li>
</ol>
<p>Convert this tensor to a specific dtype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">self</span></code> is already of the specified <cite>dtype</cite>, return
a ragged tensor sharing the underlying memory with <code class="docutils literal notranslate"><span class="pre">self</span></code>.
Otherwise, a new tensor is returned.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
</pre></div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Currently, only support dtypes <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, and
<code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>. We can support other types if needed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dtype</strong> – The <cite>dtype</cite> this tensor should be converted to.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Return a tensor of the given <cite>dtype</cite>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id105">
<h3>to_str_simple<a class="headerlink" href="#id105" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.to_str_simple">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">to_str_simple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.to_str_simple" title="Permalink to this definition"></a></dt>
<dd><p>Convert a ragged tensor to a string representation, which
is more compact than <code class="docutils literal notranslate"><span class="pre">self.__str__</span></code>.</p>
<p>An example output is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">RaggedTensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[[1, 2, 3],</span>
<span class="go">               [],</span>
<span class="go">               [0]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3, 10.5]]], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">&#39;RaggedTensor([[[1, 2, 3],\n               [],\n               [0]],\n              [[2],\n               [3, 10.5]]], dtype=torch.float32)&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to_str_simple</span><span class="p">()</span>
<span class="go">&#39;RaggedTensor([[[1, 2, 3], [], [0]], [[2], [3, 10.5]]], dtype=torch.float32)&#39;</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id106">
<h3>tolist<a class="headerlink" href="#id106" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.tolist">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">tolist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.tolist" title="Permalink to this definition"></a></dt>
<dd><p>Turn a ragged tensor into a list of lists [of lists..].</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>You can pass the returned list to the constructor of <code class="xref py py-class docutils literal notranslate"><span class="pre">RaggedTensor</span></code>.</p>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[]],</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[[], [1, 2], [3], []], [[5, 6, 7]], [[], [0, 2, 3], [], []]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mf">3.25</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[1.0], [2.0], [], [3.25, 2.5]]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of list of lists [of lists …] containing the same elements
and structure as <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id107">
<h3>tot_size<a class="headerlink" href="#id107" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.tot_size">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">tot_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.tot_size" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of elements of an given axis. If axis is 0, it’s
equivalent to the property <code class="docutils literal notranslate"><span class="pre">dim0</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [1 2 3] [] [5 8 ] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[1 2 3] [] [5 8]] [[] [1 5 9 10 -1] [] [] []] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">tot_size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">10</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id108">
<h3>unique<a class="headerlink" href="#id108" title="Permalink to this headline"></a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.unique">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">unique</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_k2.ragged.RaggedTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_num_repeats</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_new2old_indexes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedTensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">_k2.ragged.RaggedTensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.unique" title="Permalink to this definition"></a></dt>
<dd><p>If <code class="docutils literal notranslate"><span class="pre">self</span></code> has two axes, this will return the unique sub-lists
(in a possibly different order, but without repeats).
If <code class="docutils literal notranslate"><span class="pre">self</span></code> has 3 axes, it will do the above but separately for each
index on axis 0; if more than 3 axes, the earliest axes will be ignored.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It does not completely guarantee that all unique sequences will be
present in the output, as it relies on hashing and ignores collisions.
If several sequences have the same hash, only one of them is kept, even
if the actual content in the sequence is different.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Even if there are no repeated sequences, the output may be different
from <code class="docutils literal notranslate"><span class="pre">self</span></code>. That is, <cite>new2old_indexes</cite> may NOT be an identity map even
if nothing was removed.</p>
</div>
<p><strong>Example 1</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3],</span>
<span class="go">              [3, 1]], dtype=torch.int32), None, None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_num_repeats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3],</span>
<span class="go">              [3, 1]], dtype=torch.int32), RaggedTensor([[2, 1, 1, 2]], dtype=torch.int32), tensor([2, 5, 1, 0], dtype=torch.int32))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_num_repeats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3],</span>
<span class="go">              [3, 1]], dtype=torch.int32), RaggedTensor([[2, 1, 1, 2]], dtype=torch.int32), None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3],</span>
<span class="go">              [3, 1]], dtype=torch.int32), None, tensor([2, 5, 1, 0], dtype=torch.int32))</span>
</pre></div>
</div>
<p><strong>Example 2</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="go">(RaggedTensor([[[1, 2],</span>
<span class="go">               [2, 1]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3],</span>
<span class="go">               [0, 1]],</span>
<span class="go">              [[],</span>
<span class="go">               [3],</span>
<span class="go">               [2, 3]]], dtype=torch.int32), None, None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_num_repeats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[[1, 2],</span>
<span class="go">               [2, 1]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3],</span>
<span class="go">               [0, 1]],</span>
<span class="go">              [[],</span>
<span class="go">               [3],</span>
<span class="go">               [2, 3]]], dtype=torch.int32), RaggedTensor([[3, 1],</span>
<span class="go">              [2, 1, 1],</span>
<span class="go">              [2, 1, 1]], dtype=torch.int32), tensor([ 0,  1,  5,  4,  6,  8, 11,  9], dtype=torch.int32))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_num_repeats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[[1, 2],</span>
<span class="go">               [2, 1]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3],</span>
<span class="go">               [0, 1]],</span>
<span class="go">              [[],</span>
<span class="go">               [3],</span>
<span class="go">               [2, 3]]], dtype=torch.int32), RaggedTensor([[3, 1],</span>
<span class="go">              [2, 1, 1],</span>
<span class="go">              [2, 1, 1]], dtype=torch.int32), None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">need_new2old_indexes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[[1, 2],</span>
<span class="go">               [2, 1]],</span>
<span class="go">              [[2],</span>
<span class="go">               [3],</span>
<span class="go">               [0, 1]],</span>
<span class="go">              [[],</span>
<span class="go">               [3],</span>
<span class="go">               [2, 3]]], dtype=torch.int32), None, tensor([ 0,  1,  5,  4,  6,  8, 11,  9], dtype=torch.int32))</span>
</pre></div>
</div>
<p><strong>Example 3</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">(RaggedTensor([[1],</span>
<span class="go">              [2],</span>
<span class="go">              [3]], dtype=torch.int32), RaggedTensor([[1, 1, 1]], dtype=torch.int32), tensor([0, 2, 1], dtype=torch.int32))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>need_num_repeats</strong> – If True, it also returns the number of repeats of each sequence.</p></li>
<li><p><strong>need_new2old_indexes</strong> – <p>If true, it returns an extra 1-D tensor <cite>new2old_indexes</cite>.
If <cite>src</cite> has 2 axes, this tensor contains <cite>src_idx0</cite>;
if <cite>src</cite> has 3 axes, this tensor contains <cite>src_idx01</cite>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>For repeated sublists, only one of them is kept.
The choice of which one to keep is <strong>deterministic</strong> and
is an implementation detail.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>ans: A ragged tensor with the same number of axes as <code class="docutils literal notranslate"><span class="pre">self</span></code> and possibly
fewer elements due to removing repeated sequences on the last axis
(and with the last-but-one indexes possibly in a different order).</p></li>
<li><p>num_repeats: A tensor containing number of repeats of each returned
sequence if <code class="docutils literal notranslate"><span class="pre">need_num_repeats</span></code> is True; it is <code class="docutils literal notranslate"><span class="pre">None</span></code> otherwise.
If it is not <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">num_repeats.num_axes</span></code> is always 2.
If <code class="docutils literal notranslate"><span class="pre">ans.num_axes</span></code> is 2, then <code class="docutils literal notranslate"><span class="pre">num_repeats.dim0</span> <span class="pre">==</span> <span class="pre">1</span></code> and
<code class="docutils literal notranslate"><span class="pre">num_repeats.numel()</span> <span class="pre">==</span> <span class="pre">ans.dim0</span></code>.
If <code class="docutils literal notranslate"><span class="pre">ans.num_axes</span></code> is 3, then <code class="docutils literal notranslate"><span class="pre">num_repeats.dim0</span> <span class="pre">==</span> <span class="pre">ans.dim0</span></code> and
<code class="docutils literal notranslate"><span class="pre">num_repeats.numel()</span> <span class="pre">==</span> <span class="pre">ans.tot_size(1)</span></code>.</p></li>
<li><p>new2old_indexes: A 1-D tensor whose i-th element specifies the
input sublist that the i-th output sublist corresponds to.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Returns a tuple containing</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id109">
<h3>device<a class="headerlink" href="#id109" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.device">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.device" title="Permalink to this definition"></a></dt>
<dd><p>Return the device of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id110">
<h3>dim0<a class="headerlink" href="#id110" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.dim0">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">dim0</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.dim0" title="Permalink to this definition"></a></dt>
<dd><p>Return number of sublists at axis 0.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[]] [[] []]]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dim0</span>
<span class="go">2</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id111">
<h3>dtype<a class="headerlink" href="#id111" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.dtype">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">dtype</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.dtype" title="Permalink to this definition"></a></dt>
<dd><p>Return the dtype of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.int32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float64</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id112">
<h3>grad<a class="headerlink" href="#id112" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.grad">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">grad</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.grad" title="Permalink to this definition"></a></dt>
<dd><p>This attribute is <code class="docutils literal notranslate"><span class="pre">None</span></code> by default. PyTorch will set it
during <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p>
<p>The attribute will contain the gradients computed and future
calls to <code class="docutils literal notranslate"><span class="pre">backward()</span></code> will accumulate (add) gradients into it.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [3],</span>
<span class="go">              [5, 6],</span>
<span class="go">              []], dtype=torch.float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([ 3.,  3., 11.,  0.], grad_fn=&lt;SumFunction&gt;&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0., 0., 1., 2., 2.])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id113">
<h3>is_cuda<a class="headerlink" href="#id113" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.is_cuda">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">is_cuda</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.is_cuda" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the tensor is stored on the GPU, <code class="docutils literal notranslate"><span class="pre">False</span></code>
otherwise.</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_cuda</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">is_cuda</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id114">
<h3>num_axes<a class="headerlink" href="#id114" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.num_axes">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">num_axes</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.num_axes" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of axes of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [] [] [] [] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">(</span><span class="s1">&#39;[ [[] []] [[]] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">k24</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="s1">&#39;[ [ [[] [1]] [[3 4] []] ]  [ [[1]] [[2] [3 4]] ] ]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">num_axes</span>
<span class="go">4</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Return number of axes of this tensor, which is at least 2.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id115">
<h3>requires_grad<a class="headerlink" href="#id115" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.requires_grad">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.requires_grad" title="Permalink to this definition"></a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if gradients need to be computed for this tensor.
Return <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id116">
<h3>shape<a class="headerlink" href="#id116" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.shape">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">shape</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.shape" title="Permalink to this definition"></a></dt>
<dd><p>Return the shape of this tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">shape</span>
<span class="go">[ [ x x ] [ ] [ x ] ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">&lt;class &#39;_k2.ragged.RaggedShape&#39;&gt;</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="id117">
<h3>values<a class="headerlink" href="#id117" title="Permalink to this headline"></a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="k2.ragged.RaggedTensor.values">
<span class="sig-prename descclassname"><span class="pre">RaggedTensor.</span></span><span class="sig-name descname"><span class="pre">values</span></span><a class="headerlink" href="#k2.ragged.RaggedTensor.values" title="Permalink to this definition"></a></dt>
<dd><p>Return the underlying memory as a 1-D tensor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">k2.ragged</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">k2r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">k2r</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span>
<span class="go">tensor([ 1,  2,  5,  8,  9, 10], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [8, -1, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [5],</span>
<span class="go">              [],</span>
<span class="go">              [-3, -1, 10]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">RaggedTensor([[1, 2],</span>
<span class="go">              [],</span>
<span class="go">              [-2],</span>
<span class="go">              [],</span>
<span class="go">              [-3, -1, 10]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="version.html" class="btn btn-neutral float-left" title="version" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2022, k2 development team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>